Welcome back to DATA 442, lecture 15!  
#SLIDE
As usual, we'll start off with a brief recap.  Last lecture we introduced the diea of generative models, which are a class of model that seek to create new data points by drawing from the distirbution of your inputs.  Or, a cat generating machine.  You pick.
#SLIDE
We discussed three different types - two that use explicit density functions, and one in which we implicitly sample.
#SLIDE
The PixelRNN and CNN were examples of solving explicit density functions, using different approaches to capturing the probability at each pixel.
#SLIDE
We then introduced Variational Autoencoders (VAE), which leverage an encoding/decoding process to generate new samples from our inputs.
#SLIDE
And, finally, we discussed the Generative Adversarial Network (GAN) approach, in which two nets - a generative and discriminating net - compete with eachother to either create fake images, or - in the case of the discriminator - identify the difference between fake and real images.
#SLIDE
Today we're going to talk about one of my favorite topics - visualization of neural networks.  There is a lot of discussion you'll hear about how neural networks are "black boxes", and we can't identify what's going on within them.  This is, put simply, silly.  While it's hard to identify the underlying patterns that are occuring in convolutional - or, neural - networks, it is possible to get very good ideas of why predictions are being made by visualizing just what a network is doing at any given layer or computation.  This is also a fun lecture, because I get to show you a whole bunch of pictures that really let us go under the hood of networks you've been implementing in your lab! As a teaser and to kick us off, you're looking at a project by one of our Ph.D. students in this slide, in which we are trying to distinguish between high and low quality roads from satellite images.  The method (SHAP) you see here helps unpack what features in the image are voting for or against a given classification.  If you take a look at the upper-left hand image, under "High Quality", you'll see - as an example to kick us off - the well painted road centerlines are highlighted in red pixels.  This means that, when the model sees clearly painted road lines, it is voting in favor of high quality.  Pretty cool, huh?
#SLIDE
Throughout this course we've talked about a lot of different neural network architectures for different purposes, like this one - this is a representation of VGG16, showing the activation surfaces as convolutions occur throughout the network.  However,
#SLIDE
We haven't talked much (at all) about what the activation surfaces themselves are actually identifying.  As we parameterize our network, what types of patterns are they actually finding that are correlated? What exactly is the network identifying that gives it all of this explanatory power for our predictions?  To date, we've really just seen convolutions as filters - i.e., we put some image in, we convolve our filter, and some vector comes out.  The question is, what meaning can be attributed to these vectors throughout the net?  This matters a lot, as if we can build an intuition for what types of layers are working, what type of features they're detecting, we can start to make more intelligent decisions about model architectures.
#SLIDE
One of the first things you can do is visualize the actual filter weights themselves - especially in the first layer.  To take AlexNet for ann example, these filters are representative of 3x11x11 shapes, and these filters get slid over the image as they convolve over the image itself.  In AlexNet you would have 64.  IN the first layer, this is the direct inner product between the weights in the filters and the image.  So, by visualizing the filters as images themselves, we can visualize by an 11x11x3 (in AlexNet), with all 64 images being represented.  These would be the weights of the convolutional filters on th left - you can start to see what filters are meaningful for a given project.  What you're looking at here are the weights solved for in the context of ImageNet.  So, you can see the types of things the filters are detecting - vertical lines, horizontal lines, opposing colors, diagonals, and other absrtact types of shapes.  Remember all the way back to the first lecture in this class - you'll recall that the inspiration behind convolutional neural networks largely came from the hierarchical nature of the human visual system, and how they interconnect, starting with veyr basic neurons that build up to achieve pattern recognition.  The same idea is present here - the convolutional neural network is identifying very basic patterns, which it can then combine into more complex patterns at later stages in the network.  
#SLIDE
One really cool thing is that, almost no matter what network you're using, filter design, or even input imagery, you end up with a pretty similar set of filter weights for the first layer - i.e., the baseline features things a network interprets (straight lines, horizontal lines, highly differnet colors, etc) are relatively similar, and seem to be similar to the types of patterns the human eye is desigend to pick up on.  Here you see four different architectures and the type of features detected in the first layer for each case.  It would be quite difficult to tell which was which on the basis of this - all of our architectures seem to settle on the same fundamental building blocks to extract data from imagery.
#SLIDE
Another type of visualization we can learn from is the last layer - i.e., the multi-dimensional affine layers that are used to calculate the final class scores (i.e, for imageNet it would be 1000 class scores; for CIFAR-10 it would be 10).  These are the feature vectors that represent the sum total of information that the CNN reduces our input down to. So, we can run our network across all of the layers, and then record these vectors, and then try to visualize how they change based on the input.  
#SLIDE
Let's make this a little more explicit - when we put an image into our network (say, VGG16), we are going to have some amount of data at each layer.  IN the case of VGG16, one of those layers is a densenet, which takes as an input the 25,000 or so activation filters in VGG, and feeds them into a fully connected layer with 4,096 outputs.  Those 4,096 outputs represent a point in 4,096 dimensional space; that point is our representation of the image.
#SLIDE
We can do that for any number of images, passing them throught the network, and identifying where in our multidimensional space that image ends up.
#SLIDE
Once we've done that for enough images, we can then cluster them together - i.e., try to figure out what images are represented in clusters throughout our space.  Here you have an example of this, where on the left is an image that is selected as an input, and to the right are the 6 closest neighbors to that image that was fed into the network.  You can see that the feature space allows us to capture all sorts of variation - remember back to our original discussions about..
#SLIDE
two headed horses.  If we were looking at these images in pixel space - i.e., just the pixel values in the image - we would struggle to identify cases where the image was semantically correct, but orientations or occlusion changed.
#SLIDE
There are many examples you can pick out here where feature space is clearly not susceptible to the same issues that pixel space was.  Our input elephant is looking right - but the example elephants nearby in feature space are looking in all sorts of directions.  Similarly the aircraft carrier is heavily occluded in some images, and facing different aspects throughout. This is a good way to understand if the feature space you're identifying is helpful in identifying the differences you're trying to find.
#SLIDE
You can also apply dimensionality reduction to this space - i.e., instead of having our 4,096 values in multidimensional space represent each image, we can reduce those down to the 2 dimensions that represent the data most completely.  If you've taken an earlier DATA course (i.e., DATA 310), you probably learned concepts like PCA, t-SNE, kernel PCA and other dimensionality reduction techniques; you could theoretically apply any of these.  This gives you a direct way to visualize where different classes are in the multidimensional space being output by your CNN, and thus a good understanding of what classes may be getting confused (or not) with one another.
#SLIDE
Here is a quick example of this technique being applied to a CNN designed to classify the MNIST numbers (0 to 9) - you can see the dispersion of all cases, but if you look closely you'll see confusion between (for example) 0 and 9 in the upper-right. 
#SLIDE
Another cool thing you can do is to actually place the images themselves in this 2D space.  So, here is an example of a display of images that would be considered similar to eachother in 2D space.  I encourage you to go to the website at the bottom of the screen to take a look at these images in all of their glory, as..
#SLIDE
some really interesting and neat patterns begin to emerge as you do this with more and more images.  For example, here you can see the images in the upper-right tend to have blue.  Lots of blue.  So, a filter must be picking up on that color, or patterns associated with blue, becuase they are all in a similar multi-dimensional space.  If you poke around the images, you'll see that that blue smudge is clearly "boats"; the green at the lower-left is flowers, and so on.
#SLIDE
We can also get very specific with regard to unpacking individual pieces of our networks.  Envision for example selecting a given activation in a given layer - i.e., we want to know for the 1st filter of the 4th convolutional layer, what is causing that filter to trigger?  This is a patch activation approach.  First, we run thousands of images through our network, and identify the images that caused the maximum activations for the filter we're interested in (i.e., the largest return values).  We can then look at the activation surface itself, and identify where the largest values are. Because we know that each convolution is representative of a specific area in the space of the original image, we can essentially work our way backwards to identify what that patch is, and then visualize it.  
#SLIDE
Another approach is an occlusion experiment - the basic idea is that we want to identify what part of the image actually results in the network making a decision.  So, what we do is we take our input image - here, a picture of a cat and a dog -  and then we'll start to block out parts of the image and replace it with a mean pixel value.  We then run this through the network and record the probability for each class in each case.  We then slide this occlusion window across the image and repeat the process, and draw the heat map that records - for each lcoation we occluded - the probability an image would be classified as a cat or a dog.  The idea is that if we block out something that is key for the networks interpretation in the network, then the network should change a lot - i.e., the classification decision would change.  On the right you'll see an example of this, where different pixel groups oon the image of the cat and the dog have been systematically occluded.  The top represents the "Cat" classification case.  When the pixels of the cat are occluded, we see that the probability goes down that it is a Cat - as represented in blue.  You can see from the intensity of blue that things like that cat's tail mattered just a little, but things like it's neck and, apparently, underbelly were quite important. 
#SLIDE
Another approach is to create a saliency map - i.e., trying to understand which pixels in the image are important for the classification.  The approach is to compute the gradient of the class scores themselves with respect to the image pixels.  So, remember back to our backpropogation examples - we were always trying to solve for the gradient of our weights functions, and left X (our input image data) alone.  Instead, this is essentially trying to identify if you changed a pixel value by 1, by how much would the resultant score estimate for a given class change?  This gradient-based approach is visualized on this slide, where you can see the importance of different pixel values for each of the classifications.
#SLIDE
This same idea can be applied to any output - i.e., in the saliency case, we're computing a gradient based on the class score, but we can compute the gradient for any value in the network.  I.e., let's say I want to know what causes filter 3 of my third layer to go up - I could calculate the gradient of filter 3 in my third layer on the basis of my X pixel inputs.  Zeiler and Gerfus (in 2014) showed that you could do this effeciently using what is called a "guided" backpropogation, in which only positive gradients are backpropogated through ReLU networks.  The neat thing about this is that you can actually understand every single feature that a network is examining, irrespective of the depth (or lack thereof) of the operation that you're interested in exploring.  You can see an example of this in the second column on the slide now - i.e., you can see in the top image you can make out the cat with a fair amount of detail, suggesting that the features being looked for for the "Cat" class are things like the stripes, and maybe the shapes of the eyes and nose.  IN contrast to that, the Dog case clearly shows the top of the dogs head, and things like the shape of the nose and ears.
#SLIDE
Yet another approach is gradient ascent, in which we generate an image that will result in a given output (i.e., class score) reaching it's maximum value. Just like we can optimize the weights in our network, if we freeze the network we can essentially "optimize" the pixels in an input image, with the goal of the optimizer being to maximize (hence gradient ascent) some value in the network.  What you're looking at here are 9 examples drawn from work by Simonyan and Zisserman in 2015, drawn from ImageNet and an approximately 19 layer net.  These were the generated images that would result in the maximum class score for each class.  
#SLIDE
A fun example of how this works can be seen on the slide now, which is the process of a net optimizing the input image to maximize the likelihood of predicting "Spider".  Straight out of lord of the rings, or your favorite halloween movie, you can see legs everywhere, along with patterns that clearly seem to represent webs.
#SLIDE
With some minor image augmentation, such as gaussian blurring and clipping, and better specificaiton of regularizing hyperparameters, the images that can be generated this way can be pretty incredible.  Here, you're looking at the final score for ImageNet based on an 8-layer neural network.  I highly encourage you to take a look at the website at the bottom of the screen, where you can interact with the tools built to diagnose networks, as well as learn more about them and see more examples, using gradient ascent approaches.
#SLIDE
We're not quite done yet, but because we're going to be switching gears a bit I do want to provide a summary of where we are so far. We've covered a range of different approaches to understandign CNNs, including dimensionality reduction of our output vectors, identifying the patches of images that cause individual filters to activate; intentionally occluding part of the image to determine the areas that are most important for a given classification; using backpropogationn to establish the importance of one pixel changing, and finally gradient ascent to determine the image that would maximize a given class score.
#SLIDE
Alright!  With the fun stuff out of the way, now we're going to change gears a bit and discuss deep reinforcement learning.  The fundamental idea of reinforcement learning is..

{YAY}
