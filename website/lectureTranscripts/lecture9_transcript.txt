#In this lecture, we're going to be continuing our discussion about convolutional neural networks, going into a bit more depth on how neural networks are built.
#SLIDE
Last week we covered a number of the building blocks - we discussed what tensors are, filters, and activation layers.  We chatted about strides and pooling approaches to convolution, and how the convolutional elements of a network are then propogated into the "Fully connected" portion of the network which outputs the final score estiamtes for each class.  You'll recall there are four hyperparameters we specifically discussed must be chosen for convolutions - the number of filters, filter dimensions, stride, and zero padding.
#SLIDE
Ok - let's head back to our CIFAR-10 example.  You'll recall that we first technically implement our image as 3 matrices, i.e., a tensor.  We thenn pass some number of filters over that tensor, which will result in one activation surface for each filter.
#SLIDE
The entire process looks something like this.  You start with your input image tensor, and define some number of filters.  Then, based on your strides and zero padding, you convolve that filter over the tensor to create one activation surface per filter.  In this example, the activation surface is slightly smaller than the input tensor - representing the number of valid convolutions possible with a 5x5x3 filter given the 32x32x3 tensor.  Finally, we aggregate all of the output activation surfaces into a new tensor, with a depth equal to the number of filters.  Of note here, the terms "activation surface", "activation map", and "activation layer" are used somewhat interchangeably to refer to these surfaces; you may also hear the term "activation tensor" to refer to the collated group of all activation surfaces.
#SLIDE
Once we have our activation layers, we can then use them as inputs into other models instead of the image itself, to generate our final scores.  While this is a very simplified linear model, as we have discussed activation layer information is most commonly passed forward into another neural network. 
#SLIDE
When we're building our activation surfaces, the filters that we convolve across the tensor are made up of a set of dimensions and, also weights.  If we blow up the filter, we would see something like this - a stack of matrices with numeric weights.  These weights are used when convolving, and the dot product between the weights and the image tensor values are taking as we convolve to generate the activation surfaces (you'll recall we do a deeper dive on this in our first lecture on convolutional nets).  
#SLIDE
A few lectures ago, we also discussed the idea of optimization - i.e., identifying the best weights to minimize a loss function.  We also chatted about backpropogation and the chain rule as approaches to identifying optimal gradients.  Specific flavors of optimization we've discussed have been Gradient Descent, Stochasist Gradient Descent, and Mini-batch Stochastic Gradient Descent.  We also chatted about a few techniques that simply aren't used in practice - i.e., randomly guessing weights; full searches of all values.
#SLIDE
Recall for example Mini-batch SGD.  In this approach, we have a 6-step process we follow to optimze our network.  First we sample our data, taking some number of observations equal to a hyperparameter batch size.  We then take that data and run a forward propogation, and calculate the loss (i.e., how bad the current set of weights are).  That loss is then used to backpropogate across the network to calculate the gradients of weights with respect to the loss, and we update the weights using that gradient.  We then repeat this process until we reach some hyperparameter-defined threshold - the number of loops, the change in loss over the last few iterations, or other related metrics.
#SLIDE
There are a number of steps that you must go through to build and optimize a network - starting with network architecture, then optimization, and finally evaluation.  We're going to start at the top - choices you need to make regarding the overall architecture of your network - i.e., how to design your computational graph, and work our way through the entire process.
#SLIDE
First, let's start out with some fundamentals.  What you're looking at now is the basic computational graph we've been using throughout this course - we have two inputs (a pixel value in red and a weight in teal), and one computational node (multiplication).  
#SLIDE
A common practice in neural networks is to build a computational graph that looks like this - multiplications between weights and values fed into an additive function.
#SLIDE
This added sum is then passed into some activation function (in this example, ReLu), which then passes the signal on.
#SLIDE
A more common way to visualize networks incorporates the weights into the outputs of each node.  For example, here we have the same two data inputs - pixel 1 (p1) and pixel 2 (p2).  We still have two weights - w1 and w2 - but now they are visualized along the path between nodes.  
#SLIDE
Within this receiving, computation node you can visualize two different processes - the process used to aggregate data coming in, and then the activation function used to pass a signal on.  The aggregation algorithm in neural networks is nearly always assumed to be the sum of the weighted inputs - i.e., in this example, w1*p1 + w2*p2.  Frequently this aggregation algorithm will not be visualized and is instead assumed.  
#SLIDE
The more commonly explored process is the activation function - i.e., the function applied to the value the aggregation function calculates.  There are many different activations, and we'll get into them in just a moment.  Here, the output of that activation function is notated as o_1, and is the value that would be sent to the next node.  Also, note w3 - just like the other two inputs, another weight is applied to any outputs from this node.
#SLIDE
This can also be written like this - i.e., the output of this computation is equal to some activation function f applied to the weighted sum of inputs to the node. 