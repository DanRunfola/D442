Welcome back to DATA 442!  Today we're going to start talking about some specific software for deep learning, and related architectures.
#SLIDE
To briefly recap from last lecture, we first covered a range of methods for how a learning rate can be selected, including step and exponential decay, and moved on to some optimization techniques that did not require step size, focusing on BFGS.
#SLIDE
We then discussed a range of techniques that can help improve the performance of our model on data it hasn't seen yet, including regularization, image augmentation, transfer learning and more.
#SLIDE
As you engage with assignment 2, one of the big things you'll notice is that our models are starting to get a lot more computationally demanding.  So, let's talk a little bit about just how some of those challenges can be overcome, and frameworks that can help.  This is a complex topic, and changes very quickly every year as new hardware, software, and related techniques for optimization are all moving at a rapid clip.
#SLIDE
Let's start with some very, very basic building blocks.  First and foremost, lets talk about the distinction between a CPU, GPU, and why they perform differently for image recognition.  Here, you can see an example of a CPU (upper-left), GPU (upper-right), and how these things sit in a traditional desktop computer at the bottom.  As you can see here, CPUs are fairly small - they tend to take up just a tiny fraction of the inside of any computer case, and in some cases (like this) don't even have dedicated cooling.  Contrasted to that, GPUs take up a ton of real estate - the card in this image is relatively small, and GPUs can easily take up 30 to 40% of available space on many motherboards.  They have their own fans most of the time, and also have dedicated power rails.  So, you can imagine that GPUs can - from a hardware perspective - simply put a lot more power (literally) towards solving a problem.
#SLIDE
So, what is this GPU, or Graphics Processing Unit?  Mostly, GPUs are associated with videogames - and, with good reason.  For most of the 1990s and early 2000s, the vast majority of the reason for the development of this type of processor was to facilitate our ability to display images on screens with higher and higher levels of resolution.  At the time, most people working in the space wouldn't have dreamed that the same underlying architecture used to render - say, the Zerg putting a Protoss army down - could also be used to dramatically improve our capability to fit machine learning models.  
#SLIDE
For the 1990s and 2000s, two major players in the GPU space developed cards: AMD and NVIDIA.  While the debate over which is better for gaming is an ongoing .. civil disucssion .. on Reddit, NVIDIA has put extermely large resources into making their cards better suited to deep learning, while AMD has lagged significantly behind as of this recording.  
#SLIDE
So, for most practical purposes, if you're looking to purchase a desktop or laptop (or server shard) to do deep learning, you're looking for the most cutting edge NVIDIA cards. This might change if AMD decides to catch up, but for now - if you're running an AMD card at home, you're going to struggle a bit to get it working with deep learning model architectures.
#SLIDE
So, why do we care so much about a GPU?  Let's do a comparison of why these different computer components can do such different things.  Looking at a CPU, in most consumer cases you will have just a few cores - i.e., the elements of a CPU that can follow a set of instructions.  In most laptops today, you'll likely have between 4 and 8 cores, which means that your computer can perform, at most, 4 to 8 different tasks at the same time (8 to 16 if you have specialized chips from Intel with hyperthreading).  This isn't a lot, but the extremely fast clock speed of CPUs means these tasks can be executed extremely quickly - for tasks in which there isn't much data movind around.  However, CPUs also (except a small cache) rely on physical memory that is generally located elsewhere on your motherboard (i.e., shared with the system), which can lead to bottlenecks when you need to move lots of data through processing instructions.
#SLIDE
Contrast that to a GPU.  A GPU is designed to update what colors should be displayed on a monitor - i.e., if your monitor has a resolution of 800 * 600, there are 480,000 pixel colors that need to be updated.  As you might guess, many of these updates are handled through some form of matrix multiplication - and, thus, matrix multiplication is something that GPUs are really, really good at.
#SLIDE
But, why? Think back to your matrix multiplication - in the first matrix in this example, we have 4 rows of information, with (for example) 5 bits of information in each row.  In our second matrix, we have 5 columns of information - again with 5 pieces of information in each column.  If we multiply these matrices together, we get the third matrix, which is a 4x5 matrix.  
#SLIDE
To get the value for each element of our 4x5 matrix, i.e., the one value in green here, we need to take the dot product between one of the rows in the first matrix and one of the columns in the second matrix.  
#SLIDE
Importantly, each of these dot products are independent - i.e., the value in our final matrix representend in green, organge, red and blue could all be calculated at the same time. So, you could easily imagine calculating every element of this output matrix in parallel.
#SLIDE
So, let's think back to our GPU superlatives.  In this example, it would be trivial to assign one calculation - i.e., one dot product - to it's own core, and simultaneously solve the entire matrix.  As these examples grow - i.e., to matrices of thousands of entries - you can imagine how important having lots of cores at your disposal would be.  Additionally, the two input matrices have data that has to be sent to the processing units of the GPU for them to computer the dot product in each case.  Because the memory that these matrices are stored in is local on the GPU, bottlenecks associated with data moving from shared memory (like a CPU) can be avoided.  As you'll recall from earlier lectures, this type of matrix operation is key to solving for many operations within neural networks (i.e., affine layer solutions).
#SLIDE
So, GPUs are better, but how do we interact with them?  This requires some mechanism to write code that runs on a GPU - and, right now, there are two different interfaces for this.  The first, and most common, and fastest, and what I generally recommend using as of today, is CUDA.  CUDA has been developed by Nvidia over the last decade or so, and is exceptionally performant for Nvidia graphics cards.  A lot of higher level APIs / primitives for deep learning have also been implemented by Nvidia, building on CUDA - these include, for example cuDNN, which implement a large number of the functions we've explored in this course (affine layers, convolutions, forward/back propogation for layers) directly in CUDA, which can lead to huge performance gains.  Contrasting to this is an implementaiton called OpenCL, which can run on any type of GPU hardware.  However, unlike CUDA, which NVIDIA has spent - roughly - a bajillion dollars improving, the amount of optimization in OpenCL is still lacking.  For some hard numbers about the relative effeciency of each case, you can refer to the article linked in the lecture notes and on this slide, but very broadly OpenCL can be between 16 adn 67% slower, depending on the application.  Perhaps more important in practice is that most software libraries have very strong support for CUDA, but general support for OpenCL is still much more limited.  Things are likely to change as more players enter the arena of creating cards explicitly for machine learning, but today Nvidia is really the core player.
#SLIDE
So, what does this all mean in practice?  In short: GPUs absolutely dominate CPUs when it comes to processing huge amounts of imagery through networks.  What you're looking at here was a study done by Microsoft that contrasted a variety of network architectures and implementations in their Azure cloud.  Keras is the interface that we are using to define layers, which is an interface into tensorflow (so, writing code directly in tensorflow is faster than Keras; however, Keras has a lot of helper functions that make life easier).  If you look at ResNet50, you'll see results from one of the most common network architectures used today.  If you were to implement a ResNet50 model on your own computer - say, when you're trying to solve a question on Assignment 2 - and you only had a CPU, you would expect to be able to forward and backward propogate about 10 images each second.  So, for a dataset like the UC Mercer dataset (2100 images), you would expect a runtime for ResNet50 of around 3.5 minutes before all images were seen (i.e., one epoch finished).  Contrast that to the 1 GPU case in orange - here, you would expect to see around 30 images processed a second, for a runtime of about one minute.  It's easy to see how quickly - for a large dataset - you would need to integrate GPUs in order to have a feasible chance of fitting your model before key landmarks, like a presentation for your boss, or the heat death of the universe. 
#SLIDE
On the hardware front, there are a few other considerations to be aware of.  First, you'll note that while your video card contains information on - for example- your model weights, the actual data that you want to apply the model to lives on your hard drive.  This is challenging for two reasons.  First, you need to be able to access information on that hard drive quickly; second, you need your CPU to be able to keep up with the requests to transfer data from the hard drive to the video card (as your CPU handles system operations - including data transfer!).  These types of issues are most commonly handled by ensuring the disk holding imagery is very fast - i.e., 10,000RPM (rotations per minute) or a solid-state disk drive.  Couple that with a multi-core processor capable of prefetching data off of the hard drive, which most modern chips can do, and you can avoid data input/output bottlenecks in most cases.
#SLIDE
Alright, enough of the hardware - let's start talking about the frameworks that most people (including us) use to interact with deep learning models.  This is a rapidly evolving landscape, and in this course we're primarily going to focus on Torch and Tensorflow.  There are a *ton* of frameworks out there - while they all orginated in academia, major industry players (Baido, Microsoft, Amazon) have all now started developing their own frameworks; however, most of these are much less broadly adopted.  
#SLIDE

