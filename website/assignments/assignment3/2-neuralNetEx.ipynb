{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd08e3a21d38ab9816cf2a4fb5b70910b2de32092d7fedca6365d5651d786256744",
   "display_name": "Python 3.8.5 64-bit ('data442': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "8e3a21d38ab9816cf2a4fb5b70910b2de32092d7fedca6365d5651d786256744"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 2100 images belonging to 21 classes.\n",
      "Epoch 1/15\n",
      "66/66 [==============================] - 6s 49ms/step - loss: 2.0855 - categorical_accuracy: 0.0576\n",
      "Epoch 2/15\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.4799 - categorical_accuracy: 0.0688\n",
      "Epoch 3/15\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 1.3304 - categorical_accuracy: 0.0886\n",
      "Epoch 4/15\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.2003 - categorical_accuracy: 0.0815\n",
      "Epoch 5/15\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.1435 - categorical_accuracy: 0.0693\n",
      "Epoch 6/15\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0932 - categorical_accuracy: 0.0748\n",
      "Epoch 7/15\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0453 - categorical_accuracy: 0.0720\n",
      "Epoch 8/15\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0269 - categorical_accuracy: 0.0705\n",
      "Epoch 9/15\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0181 - categorical_accuracy: 0.0905\n",
      "Epoch 10/15\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0176 - categorical_accuracy: 0.0801\n",
      "Epoch 11/15\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0163 - categorical_accuracy: 0.0811\n",
      "Epoch 12/15\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 1.0167 - categorical_accuracy: 0.0782\n",
      "Epoch 13/15\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 1.0156 - categorical_accuracy: 0.0794\n",
      "Epoch 14/15\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 1.0153 - categorical_accuracy: 0.0779\n",
      "Epoch 15/15\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 1.0172 - categorical_accuracy: 0.0842\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2180859df0>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "#For this assignment, we will continue to use the UC Mercer Images Dataset.\n",
    "#Please refer back to assignment 2 for more details; it is contained in the \"mercerImages\" folder.\n",
    "\n",
    "#Let's get a quick net running with the Mercer Images Dataset, just to kick us off:\n",
    "def exampleNet():\n",
    "    m = keras.models.Sequential()\n",
    "    m.add(keras.layers.Conv2D(filters=64,\n",
    "                              kernel_size=(4,4),\n",
    "                              activation=\"tanh\",\n",
    "                              input_shape=(256,256,3)))\n",
    "    m.add(keras.layers.GlobalAveragePooling2D())\n",
    "    m.add(keras.layers.Dense(units=21))\n",
    "    m.compile(optimizer=keras.optimizers.SGD(learning_rate=.001),\n",
    "                                            loss='categorical_hinge',\n",
    "                                            metrics=['categorical_accuracy'])\n",
    "    \n",
    "    return(m)\n",
    "\n",
    "dataGenerator = keras.preprocessing.image.ImageDataGenerator()\n",
    "train = dataGenerator.flow_from_directory(\"./mercerImages\", class_mode='categorical', batch_size=32)\n",
    "    \n",
    "exampleNet = exampleNet()\n",
    "exampleNet.fit(train, epochs=15)\n",
    "\n",
    "#Look at Network Structure:\n",
    "#print(exampleNet.summary())\n",
    "#keras.utils.plot_model(exampleNet, show_shapes=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 2100 images belonging to 21 classes.\n",
      "Epoch 1/15\n",
      "66/66 [==============================] - 5s 52ms/step - loss: 1.1486 - categorical_accuracy: 0.0439\n",
      "Epoch 2/15\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.1489 - categorical_accuracy: 0.0558\n",
      "Epoch 3/15\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.1454 - categorical_accuracy: 0.0596\n",
      "Epoch 4/15\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 1.1396 - categorical_accuracy: 0.0614\n",
      "Epoch 5/15\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.1400 - categorical_accuracy: 0.0772\n",
      "Epoch 6/15\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 1.1390 - categorical_accuracy: 0.0814\n",
      "Epoch 7/15\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.1371 - categorical_accuracy: 0.0858\n",
      "Epoch 8/15\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 1.1302 - categorical_accuracy: 0.0907\n",
      "Epoch 9/15\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.1291 - categorical_accuracy: 0.0917\n",
      "Epoch 10/15\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.1209 - categorical_accuracy: 0.0930\n",
      "Epoch 11/15\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 1.1180 - categorical_accuracy: 0.1198\n",
      "Epoch 12/15\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.1193 - categorical_accuracy: 0.1003\n",
      "Epoch 13/15\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 1.1096 - categorical_accuracy: 0.1239\n",
      "Epoch 14/15\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.1009 - categorical_accuracy: 0.1483\n",
      "Epoch 15/15\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.1098 - categorical_accuracy: 0.1287\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2271761af0>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "#Now let's look at some popular architectures for comparison\n",
    "#Here is (almost) AlexNet!\n",
    "#Note that in the original AlexNet implementation, normalization layers\n",
    "#used were not actually BatchNorm (rather, it was a specialized normalization layer)\n",
    "#designed to normalize across channels that is no longer used, and so there is no\n",
    "#keras implementation I can use to emulate it!  BatchNorm is as close as I can get.\n",
    "\n",
    "def AlexNet():\n",
    "    m = keras.models.Sequential()\n",
    "\n",
    "    #Note the 256,256,3 input (from our Mercer Images)\n",
    "    m.add(keras.layers.Conv2D(filters=96, input_shape=(256,256,3), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
    "    m.add(keras.layers.normalization.BatchNormalization())\n",
    "    m.add(keras.layers.Activation('relu'))\n",
    "    m.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "    #Convolve\n",
    "    m.add(keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
    "    m.add(keras.layers.BatchNormalization())\n",
    "    m.add(keras.layers.Activation('relu'))\n",
    "    \n",
    "    #Maxpool\n",
    "    m.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "    #Convolve\n",
    "    m.add(keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "    m.add(keras.layers.BatchNormalization())\n",
    "    m.add(keras.layers.Activation('relu'))\n",
    "\n",
    "    #Convolve\n",
    "    m.add(keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "    m.add(keras.layers.BatchNormalization())\n",
    "    m.add(keras.layers.Activation('relu'))\n",
    "\n",
    "    #Convolve\n",
    "    m.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "    m.add(keras.layers.BatchNormalization())\n",
    "    m.add(keras.layers.Activation('relu'))\n",
    "    m.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "    #Affine Layer for Estimates\n",
    "    m.add(keras.layers.Flatten())\n",
    "    m.add(keras.layers.Dense(4096, input_shape=(32,32,3,)))\n",
    "    m.add(keras.layers.BatchNormalization())\n",
    "    m.add(keras.layers.Activation('relu'))\n",
    "\n",
    "    #Dropout Regularization\n",
    "    m.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "    #Affine\n",
    "    m.add(keras.layers.Dense(4096))\n",
    "    m.add(keras.layers.BatchNormalization())\n",
    "    m.add(keras.layers.Activation('relu'))\n",
    "    \n",
    "    #Dropout Regularization\n",
    "    m.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "    #Affine\n",
    "    m.add(keras.layers.Dense(1000))\n",
    "    m.add(keras.layers.BatchNormalization())\n",
    "    m.add(keras.layers.Activation('relu'))\n",
    "\n",
    "    #DropoutRegularization\n",
    "    m.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "    #Final Affine (21 Scores, one for each class in our Mercer)\n",
    "    m.add(keras.layers.Dense(21))\n",
    "    m.add(keras.layers.BatchNormalization())\n",
    "    m.add(keras.layers.Activation('softmax'))\n",
    "\n",
    "    #Same optimizer for all of these examples, for a fair comparison.\n",
    "    m.compile(optimizer=keras.optimizers.SGD(learning_rate=.001),\n",
    "                                            loss='categorical_hinge',\n",
    "                                            metrics=['categorical_accuracy'])\n",
    "\n",
    "    return(m)\n",
    "\n",
    "dataGenerator = keras.preprocessing.image.ImageDataGenerator()\n",
    "train = dataGenerator.flow_from_directory(\"./mercerImages\", class_mode='categorical', batch_size=32)\n",
    "    \n",
    "AlexNet = AlexNet()\n",
    "AlexNet.fit(train, epochs=15)\n",
    "\n",
    "#Look at Network Structure:\n",
    "#print(AlexNet.summary())\n",
    "#keras.utils.plot_model(AlexNet, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 2100 images belonging to 21 classes.\n",
      "(256, 256, 3)\n",
      "Found 2100 images belonging to 21 classes.\n",
      "(224, 224, 3)\n",
      "Epoch 1/5\n",
      "66/66 [==============================] - 30s 349ms/step - loss: 1.0236 - categorical_accuracy: 0.0534\n",
      "Epoch 2/5\n",
      "66/66 [==============================] - 18s 275ms/step - loss: 1.0049 - categorical_accuracy: 0.0799\n",
      "Epoch 3/5\n",
      "66/66 [==============================] - 18s 273ms/step - loss: 1.0039 - categorical_accuracy: 0.1319\n",
      "Epoch 4/5\n",
      "66/66 [==============================] - 18s 273ms/step - loss: 1.0031 - categorical_accuracy: 0.1504\n",
      "Epoch 5/5\n",
      "66/66 [==============================] - 18s 272ms/step - loss: 1.0026 - categorical_accuracy: 0.1808\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f22717a3880>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#One of the great things about using model frameworks like Keras\n",
    "#and tensorflow is that many of these model frameworks are already\n",
    "#prebuilt for us, which makes our lives *much* easier, as we don't\n",
    "#have to code each layer manually.  Just like in the above AlexNet example,\n",
    "#here we're going to run a VGG(16):\n",
    "\n",
    "VGG16Net = keras.applications.VGG16(classes=21, weights=None)\n",
    "VGG16Net.compile(optimizer=keras.optimizers.SGD(learning_rate=.001),\n",
    "                                            loss='categorical_hinge',\n",
    "                                            metrics=['categorical_accuracy'])\n",
    "\n",
    "dataGenerator = keras.preprocessing.image.ImageDataGenerator()\n",
    "train = dataGenerator.flow_from_directory(\"./mercerImages\", class_mode='categorical', batch_size=32)\n",
    "\n",
    "\n",
    "#Note VGG16, in it's default case, required an input shape of 224,224,3.\n",
    "#Our data doesn't conform to that - it's shape is:\n",
    "print(train[0][0][0].shape) #(256,256,3)\n",
    "\n",
    "#The reason for this is that - behind the scenes - the dataGenerator.flow_from_directory\n",
    "#automatically is rescaling our images to 256x256x3, using a nearest interpolation.\n",
    "#So, to use mercer with VGG16, we need to explicitly tell the algorithm what size we want.\n",
    "#As a simple example, here we'll use a bicubic interpolation:\n",
    "train = dataGenerator.flow_from_directory(\"./mercerImages\", class_mode='categorical', batch_size=32, target_size=(224,224), interpolation=\"bicubic\")\n",
    "print(train[0][0][0].shape) #(224,224,3)\n",
    "\n",
    "VGG16Net.fit(train, epochs=5)\n",
    "#Look at Network Structure:\n",
    "#print(AlexNet.summary())\n",
    "#keras.utils.plot_model(AlexNet, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 2100 images belonging to 21 classes.\n",
      "Epoch 1/15\n",
      "66/66 [==============================] - 19s 282ms/step - loss: 1.0127 - categorical_accuracy: 0.0546\n",
      "Epoch 2/15\n",
      "66/66 [==============================] - 19s 289ms/step - loss: 1.0034 - categorical_accuracy: 0.0921\n",
      "Epoch 3/15\n",
      "66/66 [==============================] - 20s 297ms/step - loss: 1.0028 - categorical_accuracy: 0.1144\n",
      "Epoch 4/15\n",
      "66/66 [==============================] - 19s 291ms/step - loss: 1.0023 - categorical_accuracy: 0.1432\n",
      "Epoch 5/15\n",
      "66/66 [==============================] - 19s 291ms/step - loss: 1.0021 - categorical_accuracy: 0.1845\n",
      "Epoch 6/15\n",
      "66/66 [==============================] - 19s 289ms/step - loss: 1.0018 - categorical_accuracy: 0.2001\n",
      "Epoch 7/15\n",
      "66/66 [==============================] - 19s 291ms/step - loss: 1.0015 - categorical_accuracy: 0.2469\n",
      "Epoch 8/15\n",
      "66/66 [==============================] - 19s 290ms/step - loss: 1.0014 - categorical_accuracy: 0.2614\n",
      "Epoch 9/15\n",
      "66/66 [==============================] - 19s 286ms/step - loss: 1.0012 - categorical_accuracy: 0.2844\n",
      "Epoch 10/15\n",
      "66/66 [==============================] - 19s 286ms/step - loss: 1.0012 - categorical_accuracy: 0.2729\n",
      "Epoch 11/15\n",
      "66/66 [==============================] - 19s 287ms/step - loss: 1.0010 - categorical_accuracy: 0.2930\n",
      "Epoch 12/15\n",
      "66/66 [==============================] - 19s 288ms/step - loss: 1.0009 - categorical_accuracy: 0.3091\n",
      "Epoch 13/15\n",
      "66/66 [==============================] - 19s 285ms/step - loss: 1.0008 - categorical_accuracy: 0.3288\n",
      "Epoch 14/15\n",
      "66/66 [==============================] - 19s 285ms/step - loss: 1.0008 - categorical_accuracy: 0.3320\n",
      "Epoch 15/15\n",
      "66/66 [==============================] - 19s 284ms/step - loss: 1.0007 - categorical_accuracy: 0.3443\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f227097f7f0>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#You can use any network in Keras's library this way:\n",
    "#https://keras.io/api/applications/\n",
    "#For example, here is a ResNet50 implementation:\n",
    "\n",
    "ResNet50 = keras.applications.VGG16(classes=21, weights=None)\n",
    "ResNet50.compile(optimizer=keras.optimizers.SGD(learning_rate=.001),\n",
    "                                            loss='categorical_hinge',\n",
    "                                            metrics=['categorical_accuracy'])\n",
    "dataGenerator = keras.preprocessing.image.ImageDataGenerator()\n",
    "train = dataGenerator.flow_from_directory(\"./mercerImages\", class_mode='categorical', batch_size=32, target_size=(224,224), interpolation=\"bicubic\")\n",
    "ResNet50.fit(train, epochs=15)"
   ]
  }
 ]
}