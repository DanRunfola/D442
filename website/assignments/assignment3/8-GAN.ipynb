{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd08e3a21d38ab9816cf2a4fb5b70910b2de32092d7fedca6365d5651d786256744",
   "display_name": "Python 3.8.5 64-bit ('data442': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "8e3a21d38ab9816cf2a4fb5b70910b2de32092d7fedca6365d5651d786256744"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 819 images belonging to 1 classes.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"250.618594pt\" version=\"1.1\" viewBox=\"0 0 251.565 250.618594\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-13T06:57:20.488557</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 250.618594 \nL 251.565 250.618594 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 226.740469 \nL 244.365 226.740469 \nL 244.365 9.300469 \nL 26.925 9.300469 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p3edb0ea7ab)\">\n    <image height=\"218\" id=\"image34fc0042c0\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-8.740469\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mfbde921837\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.62375\" xlink:href=\"#mfbde921837\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(25.4425 241.338906)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.59875\" xlink:href=\"#mfbde921837\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g transform=\"translate(56.23625 241.338906)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.57375\" xlink:href=\"#mfbde921837\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <g transform=\"translate(90.21125 241.338906)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.54875\" xlink:href=\"#mfbde921837\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 30 -->\n      <g transform=\"translate(124.18625 241.338906)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.52375\" xlink:href=\"#mfbde921837\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 40 -->\n      <g transform=\"translate(158.16125 241.338906)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"198.49875\" xlink:href=\"#mfbde921837\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 50 -->\n      <g transform=\"translate(192.13625 241.338906)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.47375\" xlink:href=\"#mfbde921837\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 60 -->\n      <g transform=\"translate(226.11125 241.338906)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m66f9f91453\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m66f9f91453\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m66f9f91453\" y=\"44.974219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 48.773437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m66f9f91453\" y=\"78.949219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 82.748437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m66f9f91453\" y=\"112.924219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 30 -->\n      <g transform=\"translate(7.2 116.723437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m66f9f91453\" y=\"146.899219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 40 -->\n      <g transform=\"translate(7.2 150.698437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m66f9f91453\" y=\"180.874219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 50 -->\n      <g transform=\"translate(7.2 184.673437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m66f9f91453\" y=\"214.849219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 60 -->\n      <g transform=\"translate(7.2 218.648437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 226.740469 \nL 26.925 9.300469 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 226.740469 \nL 244.365 9.300469 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 226.740469 \nL 244.365 226.740469 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 9.300469 \nL 244.365 9.300469 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p3edb0ea7ab\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"9.300469\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "#In this notebook, you'll learn how to implement a Generative Adversarial Network (GAN).\n",
    "#The goal of a GAN is to create one network - a \"Generator\" - which seeks to create pictures\n",
    "#of something (in our case - pokemon); the second network is a \"Discriminator\", which examines\n",
    "#our generated pictures and tries to determine if they are fake or real.\n",
    "#The generator network learns over multiple iterations to \"fool\" the discriminator - \n",
    "#hopefully creating some powerful pokemon!\n",
    "#Note GANs are data hungry, so I'm combining gen1 and gen2 pokemon.  Blasphemy, I know.\n",
    "\n",
    "#(Also Note - Pokemon are a very popular starting point for GAN.  For all sorts of implementations, \n",
    "#search for \"PokeGAN\").\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "#We're going to implement our GAN as a class, mostly so as to keep our sanity!\n",
    "\n",
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.dataFolder = \"./pocketMonsters\"\n",
    "        self.targetShape = (64, 64, 3)\n",
    "        self.batchSize = 64\n",
    "        self.epochs = 100\n",
    "\n",
    "    #Note in our data generator, we're not doing any image augmentation.\n",
    "    #You could, but you would want to limit yourself to generators that\n",
    "    #retain the look, feel, heart and soul of pocketmonsters.\n",
    "    def loadData(self):\n",
    "        dataset_generator = keras.preprocessing.image.ImageDataGenerator().flow_from_directory(\n",
    "            self.dataFolder, target_size=(self.targetShape[0], self.targetShape[1]),\n",
    "            batch_size=self.batchSize,\n",
    "            class_mode=None)\n",
    "\n",
    "        return dataset_generator\n",
    "\n",
    "pokeGAN = GAN()\n",
    "\n",
    "train = pokeGAN.loadData()                                    \n",
    "\n",
    "plt.imshow(next(train)[10].astype('uint8'))\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_4 (Conv2D)            (None, 32, 32, 64)        4864      \n_________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 64)        0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 16, 16, 128)       204928    \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 16, 16, 128)       512       \n_________________________________________________________________\nleaky_re_lu_5 (LeakyReLU)    (None, 16, 16, 128)       0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 8, 8, 256)         819456    \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 8, 8, 256)         1024      \n_________________________________________________________________\nleaky_re_lu_6 (LeakyReLU)    (None, 8, 8, 256)         0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 4, 4, 512)         3277312   \n_________________________________________________________________\nbatch_normalization_5 (Batch (None, 4, 4, 512)         2048      \n_________________________________________________________________\nleaky_re_lu_7 (LeakyReLU)    (None, 4, 4, 512)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 8192)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 8193      \n_________________________________________________________________\nactivation_1 (Activation)    (None, 1)                 0         \n=================================================================\nTotal params: 4,318,337\nTrainable params: 4,316,545\nNon-trainable params: 1,792\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.dataFolder = \"./pocketMonsters\"\n",
    "        self.targetShape = (64, 64, 3)\n",
    "        self.batchSize = 64\n",
    "        self.epochs = 100\n",
    "\n",
    "    def loadData(self):\n",
    "        dataset_generator = ImageDataGenerator().flow_from_directory(\n",
    "            self.dataFolder, target_size=(self.targetShape[0], self.targetShape[1]),\n",
    "            batch_size=self.batchSize,\n",
    "            class_mode=None)\n",
    "\n",
    "        return dataset_generator\n",
    "\n",
    "    #Next up: our descrimanator!  The basic idea of the GAN is to have a model\n",
    "    #that checks if an image is real, and train another model (the generator)\n",
    "    #on that model.  Here, we're creating the descriminator - i.e., the model\n",
    "    #that checks if something is real or not.\n",
    "    #For now we're just going to define it here - in a few cells, we'll take a look at how\n",
    "    #we train it.\n",
    "    def loadDescriminator(self):\n",
    "        discriminator = keras.models.Sequential()\n",
    "        discriminator.add(keras.layers.Conv2D(filters=64, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform',\n",
    "                                    input_shape=self.targetShape))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Conv2D(filters=128, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform'))\n",
    "        discriminator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Conv2D(filters=256, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform'))\n",
    "        discriminator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Conv2D(filters=512, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform'))\n",
    "        discriminator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Flatten())\n",
    "        discriminator.add(keras.layers.Dense(1))\n",
    "        discriminator.add(keras.layers.Activation('sigmoid'))\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "        discriminator.compile(loss='binary_crossentropy',\n",
    "                                optimizer=optimizer,\n",
    "                                metrics=None)\n",
    "\n",
    "        return discriminator\n",
    "\n",
    "pokeGAN = GAN()\n",
    "\n",
    "model = pokeGAN.loadDescriminator() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_2 (Dense)              (None, 1, 1, 8192)        827392    \n_________________________________________________________________\nreshape (Reshape)            (None, 4, 4, 512)         0         \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 4, 4, 512)         2048      \n_________________________________________________________________\nactivation_2 (Activation)    (None, 4, 4, 512)         0         \n_________________________________________________________________\nconv2d_transpose (Conv2DTran (None, 8, 8, 256)         3277056   \n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 8, 8, 256)         1024      \n_________________________________________________________________\nactivation_3 (Activation)    (None, 8, 8, 256)         0         \n_________________________________________________________________\nconv2d_transpose_1 (Conv2DTr (None, 16, 16, 128)       819328    \n_________________________________________________________________\nbatch_normalization_8 (Batch (None, 16, 16, 128)       512       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 16, 16, 128)       0         \n_________________________________________________________________\nconv2d_transpose_2 (Conv2DTr (None, 32, 32, 64)        204864    \n_________________________________________________________________\nbatch_normalization_9 (Batch (None, 32, 32, 64)        256       \n_________________________________________________________________\nactivation_5 (Activation)    (None, 32, 32, 64)        0         \n_________________________________________________________________\nconv2d_transpose_3 (Conv2DTr (None, 64, 64, 3)         4803      \n_________________________________________________________________\nactivation_6 (Activation)    (None, 64, 64, 3)         0         \n=================================================================\nTotal params: 5,137,283\nTrainable params: 5,135,363\nNon-trainable params: 1,920\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.dataFolder = \"./pocketMonsters\"\n",
    "        self.targetShape = (64, 64, 3)\n",
    "        self.batchSize = 64\n",
    "        self.epochs = 100\n",
    "\n",
    "    #And, here is our generator.  This is a model that attempts\n",
    "    #to create images that will fool the discriminator.\n",
    "    #Again, we'll use this in a few cells to show how it works.\n",
    "    #Of note is the input_shape for the first dense layer -\n",
    "    #this is going to represent an input of random noise\n",
    "    #that we're using to initialize the generator.\n",
    "    def loadGenerator(self):\n",
    "        generator = keras.models.Sequential()\n",
    "        generator.add(keras.layers.Dense(units=4 * 4 * 512,\n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            input_shape=(1, 1, 100)))\n",
    "        generator.add(keras.layers.Reshape(target_shape=(4, 4, 512)))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=256, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=128, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=64, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=3, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.Activation('tanh'))\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=0.00015, beta_1=0.5)\n",
    "        generator.compile(loss='binary_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=None)\n",
    "\n",
    "        return generator\n",
    "\n",
    "pokeGAN = GAN()\n",
    "\n",
    "model = pokeGAN.loadGenerator() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.dataFolder = \"./pocketMonsters\"\n",
    "        self.targetShape = (64, 64, 3)\n",
    "        self.batchSize = 64\n",
    "        self.epochs = 100\n",
    "\n",
    "    #Next up is a small helper function so we can see how the images evolve\n",
    "    #across epochs:\n",
    "    def saveImages(self, genImage, epochNum, batchNum):\n",
    "            #We're generating 64 pokemon each iteration:\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            grid = gridspec.GridSpec(8, 8)\n",
    "            grid.update(wspace=0.1, hspace=0.1)\n",
    "\n",
    "            for i in range(64):\n",
    "                ax1 = plt.subplot(grid[i])\n",
    "                ax1.set_aspect('equal')\n",
    "                image = generated_images[i, :, :, :]\n",
    "\n",
    "                #Scale colors between 1 and 255\n",
    "                image += 1\n",
    "                image *= 255\n",
    "                fig = plt.imshow(image.astype(np.uint8))\n",
    "                plt.axis('off')\n",
    "                fig.axes.get_xaxis().set_visible(False)\n",
    "                fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            save_name = 'myPokemon/epoch' + str(epoch_no + 1) + 'Batch' + str(batch_no + 1) + '.png'\n",
    "            if not os.path.exists('myPokemon'):\n",
    "                os.mkdir('myPokemon')\n",
    "            plt.savefig(save_name, bbox_inches='tight', pad_inches=0)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.dataFolder = \"./pocketMonsters\"\n",
    "        self.targetShape = (64, 64, 3)\n",
    "        self.batchSize = 64\n",
    "        self.epochs = 100\n",
    "\n",
    "    #Now we need to define how the generator and dicriminator\n",
    "    #will interact, and how we'll optimize them.  This is \n",
    "    #the GAN itself.\n",
    "    def loadGAN(self, generator, discriminator):\n",
    "        m = keras.models.Sequential()\n",
    "        discriminator.trainable = False\n",
    "        m.add(generator)\n",
    "        m.add(discriminator)\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=0.00015, beta_1=0.5)\n",
    "        m.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "                    metrics=None)\n",
    "        return m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "h 6 of 13 | Generator Loss: 0.858 | Discriminator Loss: 0.681 | Time: 0.35052013397216797 seconds.\n",
      "Epoch9836 Batch 7 of 13 | Generator Loss: 0.815 | Discriminator Loss: 0.655 | Time: 0.3612055778503418 seconds.\n",
      "Epoch9836 Batch 8 of 13 | Generator Loss: 0.832 | Discriminator Loss: 0.664 | Time: 0.4641416072845459 seconds.\n",
      "Epoch9836 Batch 9 of 13 | Generator Loss: 0.835 | Discriminator Loss: 0.651 | Time: 0.4189176559448242 seconds.\n",
      "Epoch9836 Batch 10 of 13 | Generator Loss: 0.857 | Discriminator Loss: 0.646 | Time: 0.3972482681274414 seconds.\n",
      "Epoch9836 Batch 11 of 13 | Generator Loss: 0.868 | Discriminator Loss: 0.645 | Time: 0.5020625591278076 seconds.\n",
      "Epoch9836 Batch 12 of 13 | Generator Loss: 0.836 | Discriminator Loss: 0.651 | Time: 0.464125394821167 seconds.\n",
      "Epoch9836 Batch 13 of 13 | Generator Loss: 0.819 | Discriminator Loss: 0.652 | Time: 0.44248151779174805 seconds.\n",
      "Epoch: 9838 of 100000\n",
      "Epoch9837 Batch 1 of 13 | Generator Loss: 0.825 | Discriminator Loss: 0.642 | Time: 0.4889254570007324 seconds.\n",
      "Epoch9837 Batch 2 of 13 | Generator Loss: 0.819 | Discriminator Loss: 0.676 | Time: 0.513235330581665 seconds.\n",
      "Epoch9837 Batch 3 of 13 | Generator Loss: 0.81 | Discriminator Loss: 0.693 | Time: 0.5259859561920166 seconds.\n",
      "Epoch9837 Batch 4 of 13 | Generator Loss: 0.802 | Discriminator Loss: 0.686 | Time: 0.5637733936309814 seconds.\n",
      "Epoch9837 Batch 5 of 13 | Generator Loss: 0.806 | Discriminator Loss: 0.661 | Time: 0.7103302478790283 seconds.\n",
      "Epoch9837 Batch 6 of 13 | Generator Loss: 0.811 | Discriminator Loss: 0.694 | Time: 0.5799219608306885 seconds.\n",
      "Epoch9837 Batch 7 of 13 | Generator Loss: 0.817 | Discriminator Loss: 0.639 | Time: 0.6120059490203857 seconds.\n",
      "Epoch9837 Batch 8 of 13 | Generator Loss: 0.81 | Discriminator Loss: 0.632 | Time: 0.48962903022766113 seconds.\n",
      "Epoch9837 Batch 9 of 13 | Generator Loss: 0.817 | Discriminator Loss: 0.648 | Time: 0.5144507884979248 seconds.\n",
      "Epoch9837 Batch 10 of 13 | Generator Loss: 0.835 | Discriminator Loss: 0.652 | Time: 0.48860740661621094 seconds.\n",
      "Epoch9837 Batch 11 of 13 | Generator Loss: 0.85 | Discriminator Loss: 0.674 | Time: 0.49269986152648926 seconds.\n",
      "Epoch9837 Batch 12 of 13 | Generator Loss: 0.834 | Discriminator Loss: 0.634 | Time: 0.49306583404541016 seconds.\n",
      "Epoch9837 Batch 13 of 13 | Generator Loss: 0.835 | Discriminator Loss: 0.629 | Time: 0.512603759765625 seconds.\n",
      "Epoch: 9839 of 100000\n",
      "Epoch9838 Batch 1 of 13 | Generator Loss: 0.826 | Discriminator Loss: 0.668 | Time: 0.6334261894226074 seconds.\n",
      "Epoch9838 Batch 2 of 13 | Generator Loss: 0.823 | Discriminator Loss: 0.666 | Time: 0.7336976528167725 seconds.\n",
      "Epoch9838 Batch 3 of 13 | Generator Loss: 0.848 | Discriminator Loss: 0.636 | Time: 0.4895188808441162 seconds.\n",
      "Epoch9838 Batch 4 of 13 | Generator Loss: 0.834 | Discriminator Loss: 0.633 | Time: 0.5024454593658447 seconds.\n",
      "Epoch9838 Batch 5 of 13 | Generator Loss: 0.825 | Discriminator Loss: 0.651 | Time: 0.5110881328582764 seconds.\n",
      "Epoch9838 Batch 6 of 13 | Generator Loss: 0.808 | Discriminator Loss: 0.638 | Time: 0.5916380882263184 seconds.\n",
      "Epoch9838 Batch 7 of 13 | Generator Loss: 0.811 | Discriminator Loss: 0.623 | Time: 0.551671028137207 seconds.\n",
      "Epoch9838 Batch 8 of 13 | Generator Loss: 0.797 | Discriminator Loss: 0.663 | Time: 0.5599427223205566 seconds.\n",
      "Epoch9838 Batch 9 of 13 | Generator Loss: 0.786 | Discriminator Loss: 0.668 | Time: 0.630481481552124 seconds.\n",
      "Epoch9838 Batch 10 of 13 | Generator Loss: 0.791 | Discriminator Loss: 0.684 | Time: 0.6620945930480957 seconds.\n",
      "Epoch9838 Batch 11 of 13 | Generator Loss: 0.782 | Discriminator Loss: 0.688 | Time: 0.5729587078094482 seconds.\n",
      "Epoch9838 Batch 12 of 13 | Generator Loss: 0.791 | Discriminator Loss: 0.659 | Time: 0.6097352504730225 seconds.\n",
      "Epoch9838 Batch 13 of 13 | Generator Loss: 0.784 | Discriminator Loss: 0.653 | Time: 0.4903745651245117 seconds.\n",
      "Epoch: 9840 of 100000\n",
      "Epoch9839 Batch 1 of 13 | Generator Loss: 0.799 | Discriminator Loss: 0.628 | Time: 0.6151559352874756 seconds.\n",
      "Epoch9839 Batch 2 of 13 | Generator Loss: 0.791 | Discriminator Loss: 0.632 | Time: 0.5840137004852295 seconds.\n",
      "Epoch9839 Batch 3 of 13 | Generator Loss: 0.791 | Discriminator Loss: 0.678 | Time: 0.636000394821167 seconds.\n",
      "Epoch9839 Batch 4 of 13 | Generator Loss: 0.815 | Discriminator Loss: 0.656 | Time: 0.9107294082641602 seconds.\n",
      "Epoch9839 Batch 5 of 13 | Generator Loss: 0.797 | Discriminator Loss: 0.652 | Time: 0.7516787052154541 seconds.\n",
      "Epoch9839 Batch 6 of 13 | Generator Loss: 0.785 | Discriminator Loss: 0.718 | Time: 0.676642656326294 seconds.\n",
      "Epoch9839 Batch 7 of 13 | Generator Loss: 0.779 | Discriminator Loss: 0.649 | Time: 0.525904655456543 seconds.\n",
      "Epoch9839 Batch 8 of 13 | Generator Loss: 0.776 | Discriminator Loss: 0.676 | Time: 0.5626041889190674 seconds.\n",
      "Epoch9839 Batch 9 of 13 | Generator Loss: 0.792 | Discriminator Loss: 0.659 | Time: 0.6648705005645752 seconds.\n",
      "Epoch9839 Batch 10 of 13 | Generator Loss: 0.792 | Discriminator Loss: 0.642 | Time: 0.5722808837890625 seconds.\n",
      "Epoch9839 Batch 11 of 13 | Generator Loss: 0.807 | Discriminator Loss: 0.639 | Time: 0.6012744903564453 seconds.\n",
      "Epoch9839 Batch 12 of 13 | Generator Loss: 0.805 | Discriminator Loss: 0.661 | Time: 0.8749468326568604 seconds.\n",
      "Epoch9839 Batch 13 of 13 | Generator Loss: 0.815 | Discriminator Loss: 0.633 | Time: 0.9177227020263672 seconds.\n",
      "Epoch: 9841 of 100000\n",
      "Epoch9840 Batch 1 of 13 | Generator Loss: 0.817 | Discriminator Loss: 0.679 | Time: 5.976102590560913 seconds.\n",
      "Epoch9840 Batch 2 of 13 | Generator Loss: 0.789 | Discriminator Loss: 0.681 | Time: 6.034676790237427 seconds.\n",
      "Epoch9840 Batch 3 of 13 | Generator Loss: 0.789 | Discriminator Loss: 0.639 | Time: 7.145927667617798 seconds.\n",
      "Epoch9840 Batch 4 of 13 | Generator Loss: 0.796 | Discriminator Loss: 0.681 | Time: 7.5824644565582275 seconds.\n",
      "Epoch9840 Batch 5 of 13 | Generator Loss: 0.789 | Discriminator Loss: 0.687 | Time: 1.009922981262207 seconds.\n",
      "Epoch9840 Batch 6 of 13 | Generator Loss: 0.786 | Discriminator Loss: 0.642 | Time: 0.3343026638031006 seconds.\n",
      "Epoch9840 Batch 7 of 13 | Generator Loss: 0.807 | Discriminator Loss: 0.632 | Time: 0.3043830394744873 seconds.\n",
      "Epoch9840 Batch 8 of 13 | Generator Loss: 0.818 | Discriminator Loss: 0.639 | Time: 0.2886693477630615 seconds.\n",
      "Epoch9840 Batch 9 of 13 | Generator Loss: 0.807 | Discriminator Loss: 0.622 | Time: 0.28818655014038086 seconds.\n",
      "Epoch9840 Batch 10 of 13 | Generator Loss: 0.8 | Discriminator Loss: 0.685 | Time: 0.28897857666015625 seconds.\n",
      "Epoch9840 Batch 11 of 13 | Generator Loss: 0.782 | Discriminator Loss: 0.683 | Time: 0.2753465175628662 seconds.\n",
      "Epoch9840 Batch 12 of 13 | Generator Loss: 0.784 | Discriminator Loss: 0.67 | Time: 0.28914642333984375 seconds.\n",
      "Epoch9840 Batch 13 of 13 | Generator Loss: 0.811 | Discriminator Loss: 0.633 | Time: 0.2757236957550049 seconds.\n",
      "Epoch: 9842 of 100000\n",
      "Epoch9841 Batch 1 of 13 | Generator Loss: 0.816 | Discriminator Loss: 0.629 | Time: 0.37755560874938965 seconds.\n",
      "Epoch9841 Batch 2 of 13 | Generator Loss: 0.802 | Discriminator Loss: 0.628 | Time: 4.3147125244140625 seconds.\n",
      "Epoch9841 Batch 3 of 13 | Generator Loss: 0.818 | Discriminator Loss: 0.666 | Time: 6.261698246002197 seconds.\n",
      "Epoch9841 Batch 4 of 13 | Generator Loss: 0.79 | Discriminator Loss: 0.653 | Time: 5.657508611679077 seconds.\n",
      "Epoch9841 Batch 5 of 13 | Generator Loss: 0.802 | Discriminator Loss: 0.646 | Time: 5.39933967590332 seconds.\n",
      "Epoch9841 Batch 6 of 13 | Generator Loss: 0.797 | Discriminator Loss: 0.625 | Time: 5.312047243118286 seconds.\n",
      "Epoch9841 Batch 7 of 13 | Generator Loss: 0.793 | Discriminator Loss: 0.645 | Time: 5.811422109603882 seconds.\n",
      "Epoch9841 Batch 8 of 13 | Generator Loss: 0.814 | Discriminator Loss: 0.667 | Time: 6.87577486038208 seconds.\n",
      "Epoch9841 Batch 9 of 13 | Generator Loss: 0.825 | Discriminator Loss: 0.658 | Time: 6.590518951416016 seconds.\n",
      "Epoch9841 Batch 10 of 13 | Generator Loss: 0.815 | Discriminator Loss: 0.605 | Time: 5.29412055015564 seconds.\n",
      "Epoch9841 Batch 11 of 13 | Generator Loss: 0.79 | Discriminator Loss: 0.684 | Time: 5.385955095291138 seconds.\n",
      "Epoch9841 Batch 12 of 13 | Generator Loss: 0.797 | Discriminator Loss: 0.674 | Time: 6.6053314208984375 seconds.\n",
      "Epoch9841 Batch 13 of 13 | Generator Loss: 0.785 | Discriminator Loss: 0.64 | Time: 6.735771417617798 seconds.\n",
      "Epoch: 9843 of 100000\n",
      "Epoch9842 Batch 1 of 13 | Generator Loss: 0.789 | Discriminator Loss: 0.638 | Time: 5.797217607498169 seconds.\n",
      "Epoch9842 Batch 2 of 13 | Generator Loss: 0.797 | Discriminator Loss: 0.648 | Time: 6.524506330490112 seconds.\n",
      "Epoch9842 Batch 3 of 13 | Generator Loss: 0.798 | Discriminator Loss: 0.638 | Time: 3.5405654907226562 seconds.\n",
      "Epoch9842 Batch 4 of 13 | Generator Loss: 0.817 | Discriminator Loss: 0.653 | Time: 1.9586803913116455 seconds.\n",
      "Epoch9842 Batch 5 of 13 | Generator Loss: 0.815 | Discriminator Loss: 0.651 | Time: 1.8006956577301025 seconds.\n",
      "Epoch9842 Batch 6 of 13 | Generator Loss: 0.825 | Discriminator Loss: 0.644 | Time: 5.237107038497925 seconds.\n",
      "Epoch9842 Batch 7 of 13 | Generator Loss: 0.819 | Discriminator Loss: 0.653 | Time: 6.0852649211883545 seconds.\n",
      "Epoch9842 Batch 8 of 13 | Generator Loss: 0.835 | Discriminator Loss: 0.633 | Time: 7.213598251342773 seconds.\n",
      "Epoch9842 Batch 9 of 13 | Generator Loss: 0.837 | Discriminator Loss: 0.647 | Time: 6.798418998718262 seconds.\n",
      "Epoch9842 Batch 10 of 13 | Generator Loss: 0.845 | Discriminator Loss: 0.662 | Time: 5.592902183532715 seconds.\n",
      "Epoch9842 Batch 11 of 13 | Generator Loss: 0.847 | Discriminator Loss: 0.644 | Time: 5.275289535522461 seconds.\n",
      "Epoch9842 Batch 12 of 13 | Generator Loss: 0.838 | Discriminator Loss: 0.638 | Time: 5.799644231796265 seconds.\n",
      "Epoch9842 Batch 13 of 13 | Generator Loss: 0.831 | Discriminator Loss: 0.694 | Time: 6.170175313949585 seconds.\n",
      "Epoch: 9844 of 100000\n",
      "Epoch9843 Batch 1 of 13 | Generator Loss: 0.833 | Discriminator Loss: 0.682 | Time: 7.054607391357422 seconds.\n",
      "Epoch9843 Batch 2 of 13 | Generator Loss: 0.837 | Discriminator Loss: 0.671 | Time: 6.049366474151611 seconds.\n",
      "Epoch9843 Batch 3 of 13 | Generator Loss: 0.811 | Discriminator Loss: 0.643 | Time: 6.8050055503845215 seconds.\n",
      "Epoch9843 Batch 4 of 13 | Generator Loss: 0.8 | Discriminator Loss: 0.679 | Time: 6.0138702392578125 seconds.\n",
      "Epoch9843 Batch 5 of 13 | Generator Loss: 0.804 | Discriminator Loss: 0.65 | Time: 6.431443929672241 seconds.\n",
      "Epoch9843 Batch 6 of 13 | Generator Loss: 0.786 | Discriminator Loss: 0.656 | Time: 5.392463445663452 seconds.\n",
      "Epoch9843 Batch 7 of 13 | Generator Loss: 0.776 | Discriminator Loss: 0.633 | Time: 5.522721767425537 seconds.\n",
      "Epoch9843 Batch 8 of 13 | Generator Loss: 0.773 | Discriminator Loss: 0.645 | Time: 6.385551929473877 seconds.\n",
      "Epoch9843 Batch 9 of 13 | Generator Loss: 0.76 | Discriminator Loss: 0.665 | Time: 6.20589542388916 seconds.\n",
      "Epoch9843 Batch 10 of 13 | Generator Loss: 0.761 | Discriminator Loss: 0.643 | Time: 6.801899194717407 seconds.\n",
      "Epoch9843 Batch 11 of 13 | Generator Loss: 0.761 | Discriminator Loss: 0.667 | Time: 6.00448203086853 seconds.\n",
      "Epoch9843 Batch 12 of 13 | Generator Loss: 0.783 | Discriminator Loss: 0.662 | Time: 5.840970516204834 seconds.\n",
      "Epoch9843 Batch 13 of 13 | Generator Loss: 0.783 | Discriminator Loss: 0.612 | Time: 5.49925684928894 seconds.\n",
      "Epoch: 9845 of 100000\n",
      "Epoch9844 Batch 1 of 13 | Generator Loss: 0.797 | Discriminator Loss: 0.664 | Time: 7.523505926132202 seconds.\n",
      "Epoch9844 Batch 2 of 13 | Generator Loss: 0.789 | Discriminator Loss: 0.64 | Time: 7.311619520187378 seconds.\n",
      "Epoch9844 Batch 3 of 13 | Generator Loss: 0.803 | Discriminator Loss: 0.658 | Time: 7.520711421966553 seconds.\n",
      "Epoch9844 Batch 4 of 13 | Generator Loss: 0.818 | Discriminator Loss: 0.642 | Time: 6.020132064819336 seconds.\n",
      "Epoch9844 Batch 5 of 13 | Generator Loss: 0.815 | Discriminator Loss: 0.635 | Time: 6.774652004241943 seconds.\n",
      "Epoch9844 Batch 6 of 13 | Generator Loss: 0.804 | Discriminator Loss: 0.658 | Time: 6.781165599822998 seconds.\n",
      "Epoch9844 Batch 7 of 13 | Generator Loss: 0.805 | Discriminator Loss: 0.681 | Time: 5.85615611076355 seconds.\n",
      "Epoch9844 Batch 8 of 13 | Generator Loss: 0.808 | Discriminator Loss: 0.64 | Time: 7.170773029327393 seconds.\n",
      "Epoch9844 Batch 9 of 13 | Generator Loss: 0.8 | Discriminator Loss: 0.667 | Time: 7.726067781448364 seconds.\n",
      "Epoch9844 Batch 10 of 13 | Generator Loss: 0.801 | Discriminator Loss: 0.655 | Time: 8.055636405944824 seconds.\n",
      "Epoch9844 Batch 11 of 13 | Generator Loss: 0.818 | Discriminator Loss: 0.643 | Time: 6.951444387435913 seconds.\n",
      "Epoch9844 Batch 12 of 13 | Generator Loss: 0.801 | Discriminator Loss: 0.651 | Time: 7.490434408187866 seconds.\n",
      "Epoch9844 Batch 13 of 13 | Generator Loss: 0.792 | Discriminator Loss: 0.632 | Time: 7.103440284729004 seconds.\n",
      "Epoch: 9846 of 100000\n",
      "Epoch9845 Batch 1 of 13 | Generator Loss: 0.814 | Discriminator Loss: 0.669 | Time: 5.494231462478638 seconds.\n",
      "Epoch9845 Batch 2 of 13 | Generator Loss: 0.806 | Discriminator Loss: 0.621 | Time: 6.35232400894165 seconds.\n",
      "Epoch9845 Batch 3 of 13 | Generator Loss: 0.803 | Discriminator Loss: 0.617 | Time: 6.260699510574341 seconds.\n",
      "Epoch9845 Batch 4 of 13 | Generator Loss: 0.84 | Discriminator Loss: 0.662 | Time: 5.515352010726929 seconds.\n",
      "Epoch9845 Batch 5 of 13 | Generator Loss: 0.829 | Discriminator Loss: 0.684 | Time: 5.746883153915405 seconds.\n",
      "Epoch9845 Batch 6 of 13 | Generator Loss: 0.814 | Discriminator Loss: 0.598 | Time: 5.4083781242370605 seconds.\n",
      "Epoch9845 Batch 7 of 13 | Generator Loss: 0.841 | Discriminator Loss: 0.67 | Time: 5.90208625793457 seconds.\n",
      "Epoch9845 Batch 8 of 13 | Generator Loss: 0.839 | Discriminator Loss: 0.66 | Time: 5.964869737625122 seconds.\n",
      "Epoch9845 Batch 9 of 13 | Generator Loss: 0.849 | Discriminator Loss: 0.623 | Time: 6.2388458251953125 seconds.\n",
      "Epoch9845 Batch 10 of 13 | Generator Loss: 0.85 | Discriminator Loss: 0.662 | Time: 7.683246374130249 seconds.\n",
      "Epoch9845 Batch 11 of 13 | Generator Loss: 0.826 | Discriminator Loss: 0.619 | Time: 6.086272478103638 seconds.\n",
      "Epoch9845 Batch 12 of 13 | Generator Loss: 0.85 | Discriminator Loss: 0.662 | Time: 6.76626181602478 seconds.\n",
      "Epoch9845 Batch 13 of 13 | Generator Loss: 0.82 | Discriminator Loss: 0.659 | Time: 6.2626953125 seconds.\n",
      "Epoch: 9847 of 100000\n",
      "Epoch9846 Batch 1 of 13 | Generator Loss: 0.813 | Discriminator Loss: 0.603 | Time: 7.093059062957764 seconds.\n",
      "Epoch9846 Batch 2 of 13 | Generator Loss: 0.828 | Discriminator Loss: 0.638 | Time: 6.457620620727539 seconds.\n",
      "Epoch9846 Batch 3 of 13 | Generator Loss: 0.819 | Discriminator Loss: 0.662 | Time: 5.7608137130737305 seconds.\n",
      "Epoch9846 Batch 4 of 13 | Generator Loss: 0.82 | Discriminator Loss: 0.674 | Time: 6.881105661392212 seconds.\n",
      "Epoch9846 Batch 5 of 13 | Generator Loss: 0.827 | Discriminator Loss: 0.684 | Time: 5.791363000869751 seconds.\n",
      "Epoch9846 Batch 6 of 13 | Generator Loss: 0.82 | Discriminator Loss: 0.66 | Time: 6.7622504234313965 seconds.\n",
      "Epoch9846 Batch 7 of 13 | Generator Loss: 0.843 | Discriminator Loss: 0.664 | Time: 7.229997158050537 seconds.\n",
      "Epoch9846 Batch 8 of 13 | Generator Loss: 0.824 | Discriminator Loss: 0.649 | Time: 7.589610576629639 seconds.\n",
      "Epoch9846 Batch 9 of 13 | Generator Loss: 0.811 | Discriminator Loss: 0.656 | Time: 7.503175735473633 seconds.\n",
      "Epoch9846 Batch 10 of 13 | Generator Loss: 0.799 | Discriminator Loss: 0.641 | Time: 7.328770160675049 seconds.\n",
      "Epoch9846 Batch 11 of 13 | Generator Loss: 0.811 | Discriminator Loss: 0.644 | Time: 5.558716535568237 seconds.\n",
      "Epoch9846 Batch 12 of 13 | Generator Loss: 0.803 | Discriminator Loss: 0.623 | Time: 6.938521385192871 seconds.\n",
      "Epoch9846 Batch 13 of 13 | Generator Loss: 0.828 | Discriminator Loss: 0.646 | Time: 7.678069353103638 seconds.\n",
      "Epoch: 9848 of 100000\n",
      "Epoch9847 Batch 1 of 13 | Generator Loss: 0.822 | Discriminator Loss: 0.645 | Time: 7.083629846572876 seconds.\n",
      "Epoch9847 Batch 2 of 13 | Generator Loss: 0.815 | Discriminator Loss: 0.641 | Time: 7.652790069580078 seconds.\n",
      "Epoch9847 Batch 3 of 13 | Generator Loss: 0.803 | Discriminator Loss: 0.655 | Time: 7.1224284172058105 seconds.\n",
      "Epoch9847 Batch 4 of 13 | Generator Loss: 0.817 | Discriminator Loss: 0.699 | Time: 6.181297779083252 seconds.\n",
      "Epoch9847 Batch 5 of 13 | Generator Loss: 0.796 | Discriminator Loss: 0.635 | Time: 7.056435585021973 seconds.\n",
      "Epoch9847 Batch 6 of 13 | Generator Loss: 0.798 | Discriminator Loss: 0.638 | Time: 5.490486145019531 seconds.\n",
      "Epoch9847 Batch 7 of 13 | Generator Loss: 0.784 | Discriminator Loss: 0.608 | Time: 6.470547914505005 seconds.\n",
      "Epoch9847 Batch 8 of 13 | Generator Loss: 0.8 | Discriminator Loss: 0.656 | Time: 6.337998390197754 seconds.\n",
      "Epoch9847 Batch 9 of 13 | Generator Loss: 0.78 | Discriminator Loss: 0.692 | Time: 6.372269630432129 seconds.\n",
      "Epoch9847 Batch 10 of 13 | Generator Loss: 0.793 | Discriminator Loss: 0.651 | Time: 7.105699300765991 seconds.\n",
      "Epoch9847 Batch 11 of 13 | Generator Loss: 0.788 | Discriminator Loss: 0.631 | Time: 6.124449968338013 seconds.\n",
      "Epoch9847 Batch 12 of 13 | Generator Loss: 0.818 | Discriminator Loss: 0.667 | Time: 6.572107553482056 seconds.\n",
      "Epoch9847 Batch 13 of 13 | Generator Loss: 0.803 | Discriminator Loss: 0.617 | Time: 6.98637580871582 seconds.\n",
      "Epoch: 9849 of 100000\n",
      "Epoch9848 Batch 1 of 13 | Generator Loss: 0.817 | Discriminator Loss: 0.633 | Time: 7.1857476234436035 seconds.\n",
      "Epoch9848 Batch 2 of 13 | Generator Loss: 0.811 | Discriminator Loss: 0.67 | Time: 6.517195224761963 seconds.\n",
      "Epoch9848 Batch 3 of 13 | Generator Loss: 0.817 | Discriminator Loss: 0.65 | Time: 6.768418073654175 seconds.\n",
      "Epoch9848 Batch 4 of 13 | Generator Loss: 0.787 | Discriminator Loss: 0.629 | Time: 7.393104076385498 seconds.\n",
      "Epoch9848 Batch 5 of 13 | Generator Loss: 0.761 | Discriminator Loss: 0.673 | Time: 6.327902555465698 seconds.\n",
      "Epoch9848 Batch 6 of 13 | Generator Loss: 0.773 | Discriminator Loss: 0.61 | Time: 7.06417989730835 seconds.\n",
      "Epoch9848 Batch 7 of 13 | Generator Loss: 0.777 | Discriminator Loss: 0.653 | Time: 7.489701986312866 seconds.\n",
      "Epoch9848 Batch 8 of 13 | Generator Loss: 0.787 | Discriminator Loss: 0.67 | Time: 6.4739274978637695 seconds.\n",
      "Epoch9848 Batch 9 of 13 | Generator Loss: 0.78 | Discriminator Loss: 0.661 | Time: 7.660132169723511 seconds.\n",
      "Epoch9848 Batch 10 of 13 | Generator Loss: 0.792 | Discriminator Loss: 0.644 | Time: 7.377826929092407 seconds.\n",
      "Epoch9848 Batch 11 of 13 | Generator Loss: 0.804 | Discriminator Loss: 0.641 | Time: 7.628448963165283 seconds.\n",
      "Epoch9848 Batch 12 of 13 | Generator Loss: 0.816 | Discriminator Loss: 0.606 | Time: 7.01147723197937 seconds.\n",
      "Epoch9848 Batch 13 of 13 | Generator Loss: 0.802 | Discriminator Loss: 0.643 | Time: 6.299202919006348 seconds.\n",
      "Epoch: 9850 of 100000\n",
      "Epoch9849 Batch 1 of 13 | Generator Loss: 0.808 | Discriminator Loss: 0.616 | Time: 7.118422269821167 seconds.\n",
      "Epoch9849 Batch 2 of 13 | Generator Loss: 0.784 | Discriminator Loss: 0.665 | Time: 6.972932577133179 seconds.\n",
      "Epoch9849 Batch 3 of 13 | Generator Loss: 0.785 | Discriminator Loss: 0.639 | Time: 7.308854103088379 seconds.\n",
      "Epoch9849 Batch 4 of 13 | Generator Loss: 0.775 | Discriminator Loss: 0.637 | Time: 4.385000944137573 seconds.\n",
      "Epoch9849 Batch 5 of 13 | Generator Loss: 0.77 | Discriminator Loss: 0.671 | Time: 2.0683679580688477 seconds.\n",
      "Epoch9849 Batch 6 of 13 | Generator Loss: 0.787 | Discriminator Loss: 0.627 | Time: 2.822528839111328 seconds.\n",
      "Epoch9849 Batch 7 of 13 | Generator Loss: 0.776 | Discriminator Loss: 0.667 | Time: 5.8308327198028564 seconds.\n",
      "Epoch9849 Batch 8 of 13 | Generator Loss: 0.789 | Discriminator Loss: 0.636 | Time: 7.4366631507873535 seconds.\n",
      "Epoch9849 Batch 9 of 13 | Generator Loss: 0.793 | Discriminator Loss: 0.704 | Time: 6.8012940883636475 seconds.\n",
      "Epoch9849 Batch 10 of 13 | Generator Loss: 0.783 | Discriminator Loss: 0.646 | Time: 5.991499900817871 seconds.\n",
      "Epoch9849 Batch 11 of 13 | Generator Loss: 0.793 | Discriminator Loss: 0.627 | Time: 6.214945077896118 seconds.\n",
      "Epoch9849 Batch 12 of 13 | Generator Loss: 0.768 | Discriminator Loss: 0.596 | Time: 7.899802207946777 seconds.\n",
      "Epoch9849 Batch 13 of 13 | Generator Loss: 0.743 | Discriminator Loss: 0.685 | Time: 7.072815895080566 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Finally, we add everything together into a mega-Class.\n",
    "#The real heavy lifting is in the final method here - our training method!\n",
    "#Running this cell will build our GAN.\n",
    "\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import keras\n",
    "import math\n",
    "import IPython\n",
    "\n",
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.dataFolder = \"./pocketMonsters\"\n",
    "        self.targetShape = (64, 64, 3)\n",
    "        self.batchSize = 64\n",
    "        self.epochs = 100000\n",
    "\n",
    "    #Note in our data generator, we're not doing any image augmentation.\n",
    "    #You could, but you would want to limit yourself to generators that\n",
    "    #retain the look, feel, heart and soul of pocketmonsters.\n",
    "    def loadData(self):\n",
    "        dataset_generator = keras.preprocessing.image.ImageDataGenerator().flow_from_directory(\n",
    "            self.dataFolder, target_size=(self.targetShape[0], self.targetShape[1]),\n",
    "            batch_size=self.batchSize,\n",
    "            class_mode=None)\n",
    "\n",
    "        return dataset_generator\n",
    "\n",
    "    def loadDiscriminator(self):\n",
    "        discriminator = keras.models.Sequential()\n",
    "        discriminator.add(keras.layers.Conv2D(filters=64, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform',\n",
    "                                    input_shape=self.targetShape))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Conv2D(filters=128, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform'))\n",
    "        discriminator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Conv2D(filters=256, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform'))\n",
    "        discriminator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Conv2D(filters=512, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform'))\n",
    "        discriminator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Flatten())\n",
    "        discriminator.add(keras.layers.Dense(1))\n",
    "        discriminator.add(keras.layers.Activation('sigmoid'))\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "        discriminator.compile(loss='binary_crossentropy',\n",
    "                                optimizer=optimizer,\n",
    "                                metrics=None)\n",
    "\n",
    "        return discriminator\n",
    "\n",
    "    def loadGenerator(self):\n",
    "        generator = keras.models.Sequential()\n",
    "        generator.add(keras.layers.Dense(units=4 * 4 * 512,\n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            input_shape=(1, 1, 100)))\n",
    "        generator.add(keras.layers.Reshape(target_shape=(4, 4, 512)))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=256, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=128, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=64, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=3, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.Activation('tanh'))\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=0.00015, beta_1=0.5)\n",
    "        generator.compile(loss='binary_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=None)\n",
    "\n",
    "        return generator\n",
    "\n",
    "    def saveImages(self, genImage, epochNum):\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            grid = gridspec.GridSpec(8, 8)\n",
    "            grid.update(wspace=0.1, hspace=0.1)\n",
    "\n",
    "            for i in range(64):\n",
    "                ax1 = plt.subplot(grid[i])\n",
    "                ax1.set_aspect('equal')\n",
    "                image = genImage[i, :, :, :]\n",
    "\n",
    "                image += 1\n",
    "                image *= 127.5\n",
    "                fig = plt.imshow(image.astype(np.uint8))\n",
    "                plt.axis('off')\n",
    "                fig.axes.get_xaxis().set_visible(False)\n",
    "                fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "            #Small changes to this function for our run: we'll only save\n",
    "            #once every five epochs for speed.\n",
    "            save_name = 'myPokemon/epoch' + str(epochNum + 1) + '.png'\n",
    "            if not os.path.exists('myPokemon'):\n",
    "                os.mkdir('myPokemon')\n",
    "            plt.savefig(save_name, bbox_inches='tight', pad_inches=0)\n",
    "            #We're commenting out this show for now, to let our script run faster.\n",
    "            #We'll just save to a file.\n",
    "            #plt.show()\n",
    "            plt.close()\n",
    "\n",
    "    def loadGAN(self, generator, discriminator):\n",
    "        m = keras.models.Sequential()\n",
    "        discriminator.trainable = False\n",
    "        m.add(generator)\n",
    "        m.add(discriminator)\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=0.00015, beta_1=0.5)\n",
    "        m.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "                    metrics=None)\n",
    "        return m\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        #Alright!  Now for the big payoff.\n",
    "        #First, we load our three models - the generator,\n",
    "        #discrimanator, and the GAN that strings the together.\n",
    "        generator = self.loadGenerator()\n",
    "        discriminator = self.loadDiscriminator()\n",
    "        gan = self.loadGAN(generator, discriminator)\n",
    "\n",
    "        # Load our pokemon data\n",
    "        dataGenerator = self.loadData()\n",
    "        \n",
    "        #Calculate the number of batches per epoch required\n",
    "        numBatches = math.ceil(dataGenerator.samples/self.batchSize)\n",
    "\n",
    "        # Save our losses\n",
    "        adversarialLoss = np.empty(shape=1)\n",
    "        discriminatorLoss = np.empty(shape=1)\n",
    "        batches = np.empty(shape=1)\n",
    "\n",
    "        #Let's us show the outputs in Jupyter\n",
    "        plt.ion()\n",
    "\n",
    "        currentBatch = 0\n",
    "\n",
    "        #Training loop starts here!\n",
    "        for epoch in range(self.epochs):\n",
    "            print(\"Epoch: \" + str(epoch + 1) + \" of \" + str(self.epochs))\n",
    "            for batchNum in range(numBatches):\n",
    "                startTime = time.time()\n",
    "\n",
    "                #First we load in this batch of real images:\n",
    "                realPokemonImages = dataGenerator.next()\n",
    "\n",
    "                #Here, we normalize to the color scale of the inpt pokemon \n",
    "                #(127.5), and then rescale to between -1 and 1.\n",
    "                #\n",
    "                realPokemonImages /= 127.5\n",
    "                realPokemonImages -= 1\n",
    "\n",
    "                #And calculate how many images we got:\n",
    "                curBatchSize = realPokemonImages.shape[0]\n",
    "\n",
    "                #Here, we generate noise to input into our generator (remember the input of 100 on our generators Dense input!)\n",
    "                noise = np.random.normal(0, 1, size=(curBatchSize,) + (1, 1, 100))\n",
    "\n",
    "                #Make our new pokemon\n",
    "                fakePokemonImages = generator.predict(noise)\n",
    "\n",
    "                #We're going to update our discriminator, but don't want to tell it\n",
    "                #exactly what is fake and what is real (as then it would train on our)\n",
    "                #fake images, and we would never be able to fool it.\n",
    "                #So, when we update we update with a \"noisy\" version of our Y, \n",
    "                #in which we randomly label a few true cases as fake,\n",
    "                #and vice-versa.\n",
    "                realPokemon_y = (np.ones(curBatchSize) - np.random.random_sample(curBatchSize) * 0.2)\n",
    "                fakePokemon_y = np.random.random_sample(curBatchSize) * 0.2\n",
    "\n",
    "                #This is where we update the Discriminator.\n",
    "                #We don't allow it to be trained in the adverserial part of the GAN,\n",
    "                #so we must manually do it here.\n",
    "                discriminator.trainable = True\n",
    "\n",
    "                #Here we train with both batches, and then save our loss and turn off\n",
    "                #the discriminator training\n",
    "                discLoss = discriminator.train_on_batch(realPokemonImages, realPokemon_y)\n",
    "                discLoss += discriminator.train_on_batch(fakePokemonImages, fakePokemon_y)\n",
    "\n",
    "                discriminatorLoss = np.append(discriminatorLoss, discLoss)\n",
    "                discriminator.trainable = False\n",
    "\n",
    "\n",
    "                #Now we are going to generate our pokemon!\n",
    "                #Note here we're going to generate 64 \"imaginary\"\n",
    "                #pokemon (i.e., those not living in Williamsburg)\n",
    "                #This number is independent of your batch size - i.e.,\n",
    "                #earlier in the training loop, we created 32 examples\n",
    "                #to train our discriminator - this was so we had the same\n",
    "                #number of real and fake cases.\n",
    "                noise = np.random.normal(0, 1,size=(64,) + (1, 1, 100))\n",
    "\n",
    "                #As before, we are going to assign a fraction of our fake cases to \"true\" cases\n",
    "                #to see how well we can fool the discriminator\n",
    "                fakePokemon_y = (np.ones(64) - np.random.random_sample(64) * 0.2)\n",
    "\n",
    "                #Make the actual images\n",
    "                fakePokemonImages = generator.predict(noise)\n",
    "\n",
    "                gLoss = gan.train_on_batch(noise, fakePokemon_y)\n",
    "                adversarialLoss = np.append(adversarialLoss, gLoss)\n",
    "                batches = np.append(batches, currentBatch)                    \n",
    "                \n",
    "                timeElapsed = time.time() - startTime\n",
    "                print(\"Epoch\" + str(epoch) + \" Batch \" + str(batchNum + 1) + \" of \" + str(numBatches) + \" | Generator Loss: \" + str(round(gLoss, 3)) + \" | Discriminator Loss: \" + str(round(discLoss, 3)) + \" | Time: \" + str(timeElapsed) + \" seconds.\")\n",
    "\n",
    "\n",
    "                currentBatch += 1\n",
    "\n",
    "            # Regularly save model weights and images\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                discriminator.trainable = True\n",
    "                if not os.path.exists('models'):\n",
    "                    os.mkdir('models')\n",
    "                generator.save('models/genEpoch' + str(epoch) + '.hdf5')\n",
    "                discriminator.save('models/discEpoch' + str(epoch) + '.hdf5')\n",
    "                self.saveImages(fakePokemonImages, epoch)\n",
    "\n",
    "            # Update our loss graph\n",
    "            plt.figure(1)\n",
    "            plt.plot(batches, adversarialLoss, color='green',\n",
    "                     label='Generator Loss')\n",
    "            plt.plot(batches, discriminatorLoss, color='blue',\n",
    "                     label='Discriminator Loss')\n",
    "            plt.title(\"GAN Training\")\n",
    "            plt.xlabel(\"Batch Iteration\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            if epoch == 0:\n",
    "                plt.legend()\n",
    "            plt.savefig('trainingLossPlot.png')\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "makePokemon = GAN()\n",
    "makePokemon.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}