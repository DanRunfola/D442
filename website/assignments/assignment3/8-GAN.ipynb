{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd08e3a21d38ab9816cf2a4fb5b70910b2de32092d7fedca6365d5651d786256744",
   "display_name": "Python 3.8.5 64-bit ('data442': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "8e3a21d38ab9816cf2a4fb5b70910b2de32092d7fedca6365d5651d786256744"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this notebook, you'll learn how to implement a Generative Adversarial Network (GAN).\n",
    "#The fundamental idea behind a GAN is to have two networks - a 'generator' which creates images,\n",
    "#and a 'discriminator' that tries to detect if those images are fake or real.\n",
    "#We're going to use MNIST as a quick example of a GAN here, which is a database of handwritten\n",
    "#numbers from 0 to 9.  We want to teach our algorithm to draw numbers.\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "(trainX, trainy), (testX, testy) = keras.datasets.mnist.load_data()\n",
    "\n",
    "#Show a few random cases:\n",
    "c = 1\n",
    "for i in random.sample(range(0, len(trainX)), 25):\n",
    "    plt.subplot(5, 5, c)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(trainX[i], cmap='gray_r')\n",
    "    c = c + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alright!  Now we can start building our GAN.\n",
    "#The first thing we want to do is build a discriminator - i.e., a network that\n",
    "#can tell handwritten numbers from \"fake\" numbers we generate.\n",
    "#This can be conceptualized as a simple CNN that is trying to predict\n",
    "#a binary fake/real output.\n",
    "#I'll define a simple one here.\n",
    "import keras\n",
    "\n",
    "\n",
    "#Note this architecture is a fairly common example that has been\n",
    "#shown to work well with MNIST.  Different problems with different levels\n",
    "#of complexity require more complex discriminating models.\n",
    "#Note that Adam is very commonly used for GANs, as a slightly better spin\n",
    "#on SGD with momentum (go back to the lecture for more details).\n",
    "def discriminatingModel():\n",
    "    m = keras.models.Sequential()\n",
    "    m.add(keras.layers.Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=(28,28,1)))\n",
    "    m.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "    m.add(keras.layers.Dropout(0.4))\n",
    "    m.add(keras.layers.Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "    m.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "    m.add(keras.layers.Dropout(0.4))\n",
    "    m.add(keras.layers.Flatten())\n",
    "    m.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "    m.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return(m)\n",
    "\n",
    "discModel = discriminatingModel()\n",
    "discModel.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we're going to build our generator - \n",
    "#the model we use to create our numbers.\n",
    "\n",
    "#The first step of this is to create a \"space\" \n",
    "#which the model samples from to generate the numbers.\n",
    "#Very simply, you can imagine a giant sphere with x,y and z\n",
    "#coordinates.  If you pick a random x, y and z, an image will\n",
    "#come out - in our case, a handwritten number.\n",
    "#As you move around the sphere, you'll get different numbers.\n",
    "#We can define that space however we want - it's normally \n",
    "#a ~100 dimension space, where you have x,y,z and 97 other dimensions\n",
    "#that define each point (a hypersphere), but the best dimensions\n",
    "#of this space are still a topic of inquiry.\n",
    "\n",
    "def generateLatentSpace(nSamples, dim=100):\n",
    "\t#Create a location in our space to generate an image from,\n",
    "    #with a unique random location for each sample we want to generate.\n",
    "    sampledPoints = np.random.randn(dim * nSamples)\n",
    "    #Reshape for the net.\n",
    "    reshape = sampledPoints.reshape(nSamples, dim)\n",
    "    return reshape\n",
    "\n",
    "#The second step is to define the algorithm that will actually generate\n",
    "#our images.  This will start at essentially generating random noise,\n",
    "#and then learn over time how it can transform that noise into \n",
    "#meaningful data.\n",
    "def generatorModel(dim=100):\n",
    "    model = keras.models.Sequential()\n",
    "\t\n",
    "    #We are generating 28x28 images for MNIST, through upsampling.\n",
    "    #Here, we'll start with a 7x7x128 vector to represent the space,\n",
    "    #and sample it to 28x28x1 at the end with a convolution.\n",
    "\t#Note on the input side, we start with our vector space coordinates \n",
    "    #(the 100 point vector we create with generateLatentSpace for each case).\n",
    "    model.add(keras.layers.Dense(128 * 7 * 7, input_dim=dim))\n",
    "\t\n",
    "    #Here, we're doing our earlier network (the discriminator), \n",
    "    #but backwards!  We pass our dense network outputs (128*7*7)\n",
    "    #through a leaky reule, then reshape it into a 7x7x128 \n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(keras.layers.Reshape((7, 7, 128)))\n",
    "\t\n",
    "    #Now we'll do a transposed 2D Convolution.  This is essentially a \n",
    "    #convolution in reverse - we have a filter with weights that vary,\n",
    "    #and these are used to \"upsample\" each input pixel.  The weights get\n",
    "    #fit to create more resolvable images.\n",
    "    model.add(keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "\t#Same thing, but going up to 28,28.\n",
    "    model.add(keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(keras.layers.Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
    "    return model\n",
    "\n",
    "genModel = generatorModel()\n",
    "#Check out the output shapes here - these upscalings are happening\n",
    "#based on the filter sizes and kernels across layers.\n",
    "genModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alright!  Now we have a generator and discriminator, and we need\n",
    "#to tie them together.  Essentially, we're going to use this\n",
    "#to back-propogate loss to our generator. \n",
    "#Pay attention to the details below, as the GAN works\n",
    "#due to how backpropogation works in this component of the model.\n",
    "\n",
    "def GAN(genModel, discModel):\n",
    "\t#First, we turn off the ability to update the weights in the discriminating\n",
    "    #model.  We'll be training it in a seperate process; this process is just for the generator.\n",
    "    discModel.trainable = False\n",
    "\t\n",
    "    #Build our \"meta\" model that contains both steps.\n",
    "    m = keras.models.Sequential()\n",
    "\t#Add our generator model here. This will generate an image based on\n",
    "    #the input latent space (and weights).\n",
    "    m.add(genModel)\n",
    "\n",
    "\t#And, we add the discriminator here.  This will create an estimate (1 or 0)\n",
    "    #of if the image is fake or real.\n",
    "    m.add(discModel)\n",
    "    \n",
    "    #Note that - to train our generator - we will want to *tell* this model\n",
    "    #that our input images are real, even though they are all being generated by our genModel.\n",
    "    #This is so that the loss function works correctly - i.e., if the discriminator says it's real,\n",
    "    #then we're happy (because our generator fooled it).  So, when we go to train the generator,\n",
    "    #we'll need to label some of our fake cases as real cases in the Y we pass.\n",
    "    opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "    m.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return(m)\n",
    "\n",
    "ganModel = GAN(genModel, discModel)\n",
    "ganModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we're going to write three data loaders.\n",
    "#The first will load our real samples, and rescale them for input into our network.\n",
    "#(Remember, we need 28,28,1).  The second will generate samples from that data.\n",
    "#And, the third will generate fake samples using our generator.\n",
    "import numpy as np\n",
    "\n",
    "#Load our Real Data:\n",
    "def loadData():\n",
    "    (trainX, _), (_, _) = keras.datasets.mnist.load_data()\n",
    "    #Add a dimension to the data - we need 28,28,1\n",
    "    X = np.expand_dims(trainX, axis=-1)\n",
    "\n",
    "    #Next, we're going to convert from integers to floats.\n",
    "    #Ultimately we want data scaled between 0 and 1 for input into our net.\n",
    "    X = X.astype('float32')\n",
    "\n",
    "    #Now we scale - our image data is from 0 to 255 in it's raw form.\n",
    "    X = X / 255.0\n",
    "\n",
    "    return(X)\n",
    "\n",
    "#Generate Samples from our Data:\n",
    "def generateRealSamples(dta, nSamples):\n",
    "    #Choose random indices from our input data\n",
    "    rnd = np.random.randint(0, dta.shape[0], nSamples)\n",
    "    #Select them\n",
    "    X = dta[rnd]\n",
    "    #General class labels (1 for \"real\" images)\n",
    "    y = np.ones((nSamples, 1))\n",
    "\n",
    "    return(X,y)\n",
    "\n",
    "def generateFakeSamples(genModel, nSamples, dim=100):\n",
    "    #First, we sample our latent space to grab a 100-element (dim element)\n",
    "    #vector, representing a \"spot\" in that space.\n",
    "    inputData = generateLatentSpace(dim=dim, nSamples=nSamples)\n",
    "\n",
    "    #Now, we predict the output at that space\n",
    "    X = genModel.predict(inputData)\n",
    "\n",
    "    #Generate class labels (0 for fake)\n",
    "    y = np.zeros((nSamples, 1))\n",
    "\n",
    "    return(X,y)\n",
    "\n",
    "\n",
    "#We're also going to add a helper function here for visualization and output\n",
    "\n",
    "def saveResults(genModel, epoch, dim=100, size=5):\n",
    "    #Generate a few fake samples\n",
    "    fakeImages, _ = generateFakeSamples(genModel, dim=dim, nSamples=size*size)\n",
    "\n",
    "    #Visualize and save the results\n",
    "    for i in range (size*size):\n",
    "        plt.subplot(size, size, 1+i)\n",
    "        plt.axis('off')\n",
    "        #Inverse the colors to make it match\n",
    "        plt.imshow(fakeImages[i,:,:,0], cmap=\"gray_r\")\n",
    "    outFile = \"./mnistGen/epoch_\" + str(epoch) + \".png\"\n",
    "    plt.savefig(outFile)\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#Ok!  Now we have all the pieces we need - time to string them together in a training function.\n",
    "#This is the big function that will actually train our data.\n",
    "\n",
    "def train(genModel, discModel, ganModel, dta, dim=100, epochs=20, batchSize=1024):\n",
    "    #Calculate the number of batches required to complete an epoch\n",
    "    #(until all data is seen by the model)\n",
    "    totalBatches = int(dta.shape[0] / batchSize)\n",
    "\n",
    "    #Iterate over each epoch / batch\n",
    "    for i in range(epochs):\n",
    "        print(\"Starting Epoch \" + str(i))\n",
    "        for j in range(totalBatches):\n",
    "            print(\"       Batch \" + str(j))\n",
    "            #First, we generate a set of real samples\n",
    "            xReal, yReal = generateRealSamples(dta, batchSize)\n",
    "\n",
    "            #Second, we generate our fake examples.\n",
    "            xFake, yFake = generateFakeSamples(genModel, nSamples = batchSize, dim=100)\n",
    "\n",
    "            #We stack these together to train our discriminating model\n",
    "            X, y = np.vstack((xReal, xFake)), np.vstack((yReal, yFake))\n",
    "\n",
    "            #Use this to train our discriminator and update weights.\n",
    "            #This shoudl be intuitive - each step, our discriminator learns to \n",
    "            #tell the difference between fake and real data, and we tell it what\n",
    "            #is fake and real so it gets good at it!\n",
    "            discLoss, _ = discModel.train_on_batch(X,y)\n",
    "\n",
    "            #Now we need to update the generator, which is trickier.\n",
    "            #First we need our latent points we use to initialize\n",
    "            inputSample = generateLatentSpace(nSamples=batchSize, dim=100)\n",
    "\n",
    "            #Remember - we're going to tell the GAN trainer that our fake\n",
    "            #data is actually true data, so that when it labels them all as true,\n",
    "            #we know that we've fooled it (i.e., there would be no loss).\n",
    "            #If it labels them all as false, we get maximum loss, because we\n",
    "            #haven't fooled it.\n",
    "            inputY = np.ones((batchSize, 1))\n",
    "\n",
    "            #Actually train the generator via the GAN\n",
    "            #(Remember, the discriminating model is frozen here)\n",
    "            ganLoss = ganModel.train_on_batch(inputSample, inputY)\n",
    "\n",
    "        #Finally, every epoch we'll save the ouputs.\n",
    "        saveResults(genModel, i)\n",
    "\n",
    "#And, now we just call the model\n",
    "#Note you may need to do a %reset to clear past model states, if you run this cell more than once\n",
    "#on jupyter.\n",
    "#%reset\n",
    "train(genModel, discModel, ganModel, dta=loadData(), dim=100, epochs=100, batchSize=256)"
   ]
  }
 ]
}