{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd08e3a21d38ab9816cf2a4fb5b70910b2de32092d7fedca6365d5651d786256744",
   "display_name": "Python 3.8.5 64-bit ('data442': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "8e3a21d38ab9816cf2a4fb5b70910b2de32092d7fedca6365d5651d786256744"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 819 images belonging to 1 classes.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"250.618594pt\" version=\"1.1\" viewBox=\"0 0 251.565 250.618594\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-13T06:57:20.488557</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 250.618594 \nL 251.565 250.618594 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 226.740469 \nL 244.365 226.740469 \nL 244.365 9.300469 \nL 26.925 9.300469 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p3edb0ea7ab)\">\n    <image height=\"218\" id=\"image34fc0042c0\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-8.740469\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mfbde921837\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.62375\" xlink:href=\"#mfbde921837\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(25.4425 241.338906)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.59875\" xlink:href=\"#mfbde921837\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g transform=\"translate(56.23625 241.338906)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.57375\" xlink:href=\"#mfbde921837\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <g transform=\"translate(90.21125 241.338906)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.54875\" xlink:href=\"#mfbde921837\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 30 -->\n      <g transform=\"translate(124.18625 241.338906)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.52375\" xlink:href=\"#mfbde921837\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 40 -->\n      <g transform=\"translate(158.16125 241.338906)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"198.49875\" xlink:href=\"#mfbde921837\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 50 -->\n      <g transform=\"translate(192.13625 241.338906)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.47375\" xlink:href=\"#mfbde921837\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 60 -->\n      <g transform=\"translate(226.11125 241.338906)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m66f9f91453\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m66f9f91453\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m66f9f91453\" y=\"44.974219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 48.773437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m66f9f91453\" y=\"78.949219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 82.748437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m66f9f91453\" y=\"112.924219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 30 -->\n      <g transform=\"translate(7.2 116.723437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m66f9f91453\" y=\"146.899219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 40 -->\n      <g transform=\"translate(7.2 150.698437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m66f9f91453\" y=\"180.874219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 50 -->\n      <g transform=\"translate(7.2 184.673437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m66f9f91453\" y=\"214.849219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 60 -->\n      <g transform=\"translate(7.2 218.648437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 226.740469 \nL 26.925 9.300469 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 226.740469 \nL 244.365 9.300469 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 226.740469 \nL 244.365 226.740469 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 9.300469 \nL 244.365 9.300469 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p3edb0ea7ab\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"9.300469\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "#In this notebook, you'll learn how to implement a Generative Adversarial Network (GAN).\n",
    "#The goal of a GAN is to create one network - a \"Generator\" - which seeks to create pictures\n",
    "#of something (in our case - pokemon); the second network is a \"Discriminator\", which examines\n",
    "#our generated pictures and tries to determine if they are fake or real.\n",
    "#The generator network learns over multiple iterations to \"fool\" the discriminator - \n",
    "#hopefully creating some powerful pokemon!\n",
    "#Note GANs are data hungry, so I'm combining gen1 and gen2 pokemon.  Blasphemy, I know.\n",
    "\n",
    "#(Also Note - Pokemon are a very popular starting point for GAN.  For all sorts of implementations, \n",
    "#search for \"PokeGAN\").\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "#We're going to implement our GAN as a class, mostly so as to keep our sanity!\n",
    "\n",
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.dataFolder = \"./pocketMonsters\"\n",
    "        self.targetShape = (64, 64, 3)\n",
    "        self.batchSize = 64\n",
    "        self.epochs = 100\n",
    "\n",
    "    #Note in our data generator, we're not doing any image augmentation.\n",
    "    #You could, but you would want to limit yourself to generators that\n",
    "    #retain the look, feel, heart and soul of pocketmonsters.\n",
    "    def loadData(self):\n",
    "        dataset_generator = keras.preprocessing.image.ImageDataGenerator().flow_from_directory(\n",
    "            self.dataFolder, target_size=(self.targetShape[0], self.targetShape[1]),\n",
    "            batch_size=self.batchSize,\n",
    "            class_mode=None)\n",
    "\n",
    "        return dataset_generator\n",
    "\n",
    "pokeGAN = GAN()\n",
    "\n",
    "train = pokeGAN.loadData()                                    \n",
    "\n",
    "plt.imshow(next(train)[10].astype('uint8'))\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_4 (Conv2D)            (None, 32, 32, 64)        4864      \n_________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 64)        0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 16, 16, 128)       204928    \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 16, 16, 128)       512       \n_________________________________________________________________\nleaky_re_lu_5 (LeakyReLU)    (None, 16, 16, 128)       0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 8, 8, 256)         819456    \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 8, 8, 256)         1024      \n_________________________________________________________________\nleaky_re_lu_6 (LeakyReLU)    (None, 8, 8, 256)         0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 4, 4, 512)         3277312   \n_________________________________________________________________\nbatch_normalization_5 (Batch (None, 4, 4, 512)         2048      \n_________________________________________________________________\nleaky_re_lu_7 (LeakyReLU)    (None, 4, 4, 512)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 8192)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 8193      \n_________________________________________________________________\nactivation_1 (Activation)    (None, 1)                 0         \n=================================================================\nTotal params: 4,318,337\nTrainable params: 4,316,545\nNon-trainable params: 1,792\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.dataFolder = \"./pocketMonsters\"\n",
    "        self.targetShape = (64, 64, 3)\n",
    "        self.batchSize = 64\n",
    "        self.epochs = 100\n",
    "\n",
    "    def loadData(self):\n",
    "        dataset_generator = ImageDataGenerator().flow_from_directory(\n",
    "            self.dataFolder, target_size=(self.targetShape[0], self.targetShape[1]),\n",
    "            batch_size=self.batchSize,\n",
    "            class_mode=None)\n",
    "\n",
    "        return dataset_generator\n",
    "\n",
    "    #Next up: our descrimanator!  The basic idea of the GAN is to have a model\n",
    "    #that checks if an image is real, and train another model (the generator)\n",
    "    #on that model.  Here, we're creating the descriminator - i.e., the model\n",
    "    #that checks if something is real or not.\n",
    "    #For now we're just going to define it here - in a few cells, we'll take a look at how\n",
    "    #we train it.\n",
    "    def loadDescriminator(self):\n",
    "        discriminator = keras.models.Sequential()\n",
    "        discriminator.add(keras.layers.Conv2D(filters=64, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform',\n",
    "                                    input_shape=self.targetShape))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Conv2D(filters=128, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform'))\n",
    "        discriminator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Conv2D(filters=256, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform'))\n",
    "        discriminator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Conv2D(filters=512, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform'))\n",
    "        discriminator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Flatten())\n",
    "        discriminator.add(keras.layers.Dense(1))\n",
    "        discriminator.add(keras.layers.Activation('sigmoid'))\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "        discriminator.compile(loss='binary_crossentropy',\n",
    "                                optimizer=optimizer,\n",
    "                                metrics=None)\n",
    "\n",
    "        return discriminator\n",
    "\n",
    "pokeGAN = GAN()\n",
    "\n",
    "model = pokeGAN.loadDescriminator() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_2 (Dense)              (None, 1, 1, 8192)        827392    \n_________________________________________________________________\nreshape (Reshape)            (None, 4, 4, 512)         0         \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 4, 4, 512)         2048      \n_________________________________________________________________\nactivation_2 (Activation)    (None, 4, 4, 512)         0         \n_________________________________________________________________\nconv2d_transpose (Conv2DTran (None, 8, 8, 256)         3277056   \n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 8, 8, 256)         1024      \n_________________________________________________________________\nactivation_3 (Activation)    (None, 8, 8, 256)         0         \n_________________________________________________________________\nconv2d_transpose_1 (Conv2DTr (None, 16, 16, 128)       819328    \n_________________________________________________________________\nbatch_normalization_8 (Batch (None, 16, 16, 128)       512       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 16, 16, 128)       0         \n_________________________________________________________________\nconv2d_transpose_2 (Conv2DTr (None, 32, 32, 64)        204864    \n_________________________________________________________________\nbatch_normalization_9 (Batch (None, 32, 32, 64)        256       \n_________________________________________________________________\nactivation_5 (Activation)    (None, 32, 32, 64)        0         \n_________________________________________________________________\nconv2d_transpose_3 (Conv2DTr (None, 64, 64, 3)         4803      \n_________________________________________________________________\nactivation_6 (Activation)    (None, 64, 64, 3)         0         \n=================================================================\nTotal params: 5,137,283\nTrainable params: 5,135,363\nNon-trainable params: 1,920\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.dataFolder = \"./pocketMonsters\"\n",
    "        self.targetShape = (64, 64, 3)\n",
    "        self.batchSize = 64\n",
    "        self.epochs = 100\n",
    "\n",
    "    #And, here is our generator.  This is a model that attempts\n",
    "    #to create images that will fool the discriminator.\n",
    "    #Again, we'll use this in a few cells to show how it works.\n",
    "    #Of note is the input_shape for the first dense layer -\n",
    "    #this is going to represent an input of random noise\n",
    "    #that we're using to initialize the generator.\n",
    "    def loadGenerator(self):\n",
    "        generator = keras.models.Sequential()\n",
    "        generator.add(keras.layers.Dense(units=4 * 4 * 512,\n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            input_shape=(1, 1, 100)))\n",
    "        generator.add(keras.layers.Reshape(target_shape=(4, 4, 512)))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=256, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=128, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=64, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=3, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.Activation('tanh'))\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=0.00015, beta_1=0.5)\n",
    "        generator.compile(loss='binary_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=None)\n",
    "\n",
    "        return generator\n",
    "\n",
    "pokeGAN = GAN()\n",
    "\n",
    "model = pokeGAN.loadGenerator() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.dataFolder = \"./pocketMonsters\"\n",
    "        self.targetShape = (64, 64, 3)\n",
    "        self.batchSize = 64\n",
    "        self.epochs = 100\n",
    "\n",
    "    #Next up is a small helper function so we can see how the images evolve\n",
    "    #across epochs:\n",
    "    def saveImages(self, genImage, epochNum, batchNum):\n",
    "            #We're generating 64 pokemon each iteration:\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            grid = gridspec.GridSpec(8, 8)\n",
    "            grid.update(wspace=0.1, hspace=0.1)\n",
    "\n",
    "            for i in range(64):\n",
    "                ax1 = plt.subplot(grid[i])\n",
    "                ax1.set_aspect('equal')\n",
    "                image = generated_images[i, :, :, :]\n",
    "\n",
    "                #Scale colors between 1 and 255\n",
    "                image += 1\n",
    "                image *= 255\n",
    "                fig = plt.imshow(image.astype(np.uint8))\n",
    "                plt.axis('off')\n",
    "                fig.axes.get_xaxis().set_visible(False)\n",
    "                fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            save_name = 'myPokemon/epoch' + str(epoch_no + 1) + 'Batch' + str(batch_no + 1) + '.png'\n",
    "            if not os.path.exists('myPokemon'):\n",
    "                os.mkdir('myPokemon')\n",
    "            plt.savefig(save_name, bbox_inches='tight', pad_inches=0)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.dataFolder = \"./pocketMonsters\"\n",
    "        self.targetShape = (64, 64, 3)\n",
    "        self.batchSize = 64\n",
    "        self.epochs = 100\n",
    "\n",
    "    #Now we need to define how the generator and dicriminator\n",
    "    #will interact, and how we'll optimize them.  This is \n",
    "    #the GAN itself.\n",
    "    def loadGAN(self, generator, discriminator):\n",
    "        m = keras.models.Sequential()\n",
    "        discriminator.trainable = False\n",
    "        m.add(generator)\n",
    "        m.add(discriminator)\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=0.00015, beta_1=0.5)\n",
    "        m.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "                    metrics=None)\n",
    "        return m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "| Time: 0.266618013381958 seconds.\n",
      "Epoch19 Batch 8 of 13 | Generator Loss: 0.831 | Discriminator Loss: 0.683 | Time: 0.2585725784301758 seconds.\n",
      "Epoch19 Batch 9 of 13 | Generator Loss: 0.892 | Discriminator Loss: 0.65 | Time: 0.27379918098449707 seconds.\n",
      "Epoch19 Batch 10 of 13 | Generator Loss: 0.83 | Discriminator Loss: 0.739 | Time: 0.25115036964416504 seconds.\n",
      "Epoch19 Batch 11 of 13 | Generator Loss: 0.819 | Discriminator Loss: 0.637 | Time: 0.2487332820892334 seconds.\n",
      "Epoch19 Batch 12 of 13 | Generator Loss: 0.826 | Discriminator Loss: 0.678 | Time: 0.27384161949157715 seconds.\n",
      "Epoch19 Batch 13 of 13 | Generator Loss: 0.916 | Discriminator Loss: 0.631 | Time: 0.2230055332183838 seconds.\n",
      "Epoch: 21 of 100000\n",
      "Epoch20 Batch 1 of 13 | Generator Loss: 0.926 | Discriminator Loss: 0.67 | Time: 0.2105553150177002 seconds.\n",
      "Epoch20 Batch 2 of 13 | Generator Loss: 0.82 | Discriminator Loss: 0.665 | Time: 0.255812406539917 seconds.\n",
      "Epoch20 Batch 3 of 13 | Generator Loss: 0.886 | Discriminator Loss: 0.681 | Time: 0.2800447940826416 seconds.\n",
      "Epoch20 Batch 4 of 13 | Generator Loss: 0.975 | Discriminator Loss: 0.677 | Time: 0.2339472770690918 seconds.\n",
      "Epoch20 Batch 5 of 13 | Generator Loss: 0.897 | Discriminator Loss: 0.695 | Time: 0.24808645248413086 seconds.\n",
      "Epoch20 Batch 6 of 13 | Generator Loss: 0.975 | Discriminator Loss: 0.661 | Time: 0.2452847957611084 seconds.\n",
      "Epoch20 Batch 7 of 13 | Generator Loss: 0.995 | Discriminator Loss: 0.789 | Time: 0.2524237632751465 seconds.\n",
      "Epoch20 Batch 8 of 13 | Generator Loss: 1.082 | Discriminator Loss: 0.76 | Time: 0.25205492973327637 seconds.\n",
      "Epoch20 Batch 9 of 13 | Generator Loss: 0.93 | Discriminator Loss: 0.824 | Time: 0.2595863342285156 seconds.\n",
      "Epoch20 Batch 10 of 13 | Generator Loss: 0.868 | Discriminator Loss: 0.663 | Time: 0.24610280990600586 seconds.\n",
      "Epoch20 Batch 11 of 13 | Generator Loss: 0.831 | Discriminator Loss: 0.653 | Time: 0.23642683029174805 seconds.\n",
      "Epoch20 Batch 12 of 13 | Generator Loss: 0.921 | Discriminator Loss: 0.715 | Time: 0.24261689186096191 seconds.\n",
      "Epoch20 Batch 13 of 13 | Generator Loss: 1.028 | Discriminator Loss: 0.682 | Time: 0.23938488960266113 seconds.\n",
      "Epoch: 22 of 100000\n",
      "Epoch21 Batch 1 of 13 | Generator Loss: 1.015 | Discriminator Loss: 0.668 | Time: 0.22765636444091797 seconds.\n",
      "Epoch21 Batch 2 of 13 | Generator Loss: 0.913 | Discriminator Loss: 0.637 | Time: 0.25458359718322754 seconds.\n",
      "Epoch21 Batch 3 of 13 | Generator Loss: 0.898 | Discriminator Loss: 0.652 | Time: 0.2556777000427246 seconds.\n",
      "Epoch21 Batch 4 of 13 | Generator Loss: 0.866 | Discriminator Loss: 0.611 | Time: 0.24999332427978516 seconds.\n",
      "Epoch21 Batch 5 of 13 | Generator Loss: 0.911 | Discriminator Loss: 0.647 | Time: 0.2775087356567383 seconds.\n",
      "Epoch21 Batch 6 of 13 | Generator Loss: 0.836 | Discriminator Loss: 0.677 | Time: 0.25164222717285156 seconds.\n",
      "Epoch21 Batch 7 of 13 | Generator Loss: 0.869 | Discriminator Loss: 0.685 | Time: 0.25382471084594727 seconds.\n",
      "Epoch21 Batch 8 of 13 | Generator Loss: 0.936 | Discriminator Loss: 0.672 | Time: 0.26961207389831543 seconds.\n",
      "Epoch21 Batch 9 of 13 | Generator Loss: 0.918 | Discriminator Loss: 0.671 | Time: 0.2538633346557617 seconds.\n",
      "Epoch21 Batch 10 of 13 | Generator Loss: 0.939 | Discriminator Loss: 0.696 | Time: 0.25857043266296387 seconds.\n",
      "Epoch21 Batch 11 of 13 | Generator Loss: 1.017 | Discriminator Loss: 0.671 | Time: 0.2587244510650635 seconds.\n",
      "Epoch21 Batch 12 of 13 | Generator Loss: 0.92 | Discriminator Loss: 0.659 | Time: 0.2471935749053955 seconds.\n",
      "Epoch21 Batch 13 of 13 | Generator Loss: 0.859 | Discriminator Loss: 0.66 | Time: 0.2362380027770996 seconds.\n",
      "Epoch: 23 of 100000\n",
      "Epoch22 Batch 1 of 13 | Generator Loss: 0.911 | Discriminator Loss: 0.674 | Time: 0.23636388778686523 seconds.\n",
      "Epoch22 Batch 2 of 13 | Generator Loss: 0.99 | Discriminator Loss: 0.669 | Time: 0.26840734481811523 seconds.\n",
      "Epoch22 Batch 3 of 13 | Generator Loss: 0.981 | Discriminator Loss: 0.615 | Time: 0.279390811920166 seconds.\n",
      "Epoch22 Batch 4 of 13 | Generator Loss: 0.941 | Discriminator Loss: 0.659 | Time: 0.2563164234161377 seconds.\n",
      "Epoch22 Batch 5 of 13 | Generator Loss: 0.921 | Discriminator Loss: 0.658 | Time: 0.2570328712463379 seconds.\n",
      "Epoch22 Batch 6 of 13 | Generator Loss: 0.944 | Discriminator Loss: 0.63 | Time: 0.23902297019958496 seconds.\n",
      "Epoch22 Batch 7 of 13 | Generator Loss: 0.965 | Discriminator Loss: 0.659 | Time: 0.2539527416229248 seconds.\n",
      "Epoch22 Batch 8 of 13 | Generator Loss: 0.958 | Discriminator Loss: 0.684 | Time: 0.24858736991882324 seconds.\n",
      "Epoch22 Batch 9 of 13 | Generator Loss: 0.848 | Discriminator Loss: 0.653 | Time: 0.269883394241333 seconds.\n",
      "Epoch22 Batch 10 of 13 | Generator Loss: 0.832 | Discriminator Loss: 0.666 | Time: 0.2679107189178467 seconds.\n",
      "Epoch22 Batch 11 of 13 | Generator Loss: 0.931 | Discriminator Loss: 0.636 | Time: 0.2501857280731201 seconds.\n",
      "Epoch22 Batch 12 of 13 | Generator Loss: 0.886 | Discriminator Loss: 0.673 | Time: 0.2796351909637451 seconds.\n",
      "Epoch22 Batch 13 of 13 | Generator Loss: 0.856 | Discriminator Loss: 0.672 | Time: 0.23326468467712402 seconds.\n",
      "Epoch: 24 of 100000\n",
      "Epoch23 Batch 1 of 13 | Generator Loss: 0.855 | Discriminator Loss: 0.72 | Time: 0.24012088775634766 seconds.\n",
      "Epoch23 Batch 2 of 13 | Generator Loss: 0.867 | Discriminator Loss: 0.731 | Time: 0.24908447265625 seconds.\n",
      "Epoch23 Batch 3 of 13 | Generator Loss: 0.917 | Discriminator Loss: 0.672 | Time: 0.26514720916748047 seconds.\n",
      "Epoch23 Batch 4 of 13 | Generator Loss: 0.919 | Discriminator Loss: 0.668 | Time: 0.25340747833251953 seconds.\n",
      "Epoch23 Batch 5 of 13 | Generator Loss: 0.833 | Discriminator Loss: 0.675 | Time: 0.24770736694335938 seconds.\n",
      "Epoch23 Batch 6 of 13 | Generator Loss: 0.875 | Discriminator Loss: 0.653 | Time: 0.24533629417419434 seconds.\n",
      "Epoch23 Batch 7 of 13 | Generator Loss: 0.895 | Discriminator Loss: 0.718 | Time: 0.2542424201965332 seconds.\n",
      "Epoch23 Batch 8 of 13 | Generator Loss: 0.843 | Discriminator Loss: 0.66 | Time: 0.2525441646575928 seconds.\n",
      "Epoch23 Batch 9 of 13 | Generator Loss: 0.924 | Discriminator Loss: 0.661 | Time: 0.2543375492095947 seconds.\n",
      "Epoch23 Batch 10 of 13 | Generator Loss: 0.973 | Discriminator Loss: 0.619 | Time: 0.26581835746765137 seconds.\n",
      "Epoch23 Batch 11 of 13 | Generator Loss: 0.778 | Discriminator Loss: 0.635 | Time: 0.27959203720092773 seconds.\n",
      "Epoch23 Batch 12 of 13 | Generator Loss: 0.854 | Discriminator Loss: 0.665 | Time: 0.25423669815063477 seconds.\n",
      "Epoch23 Batch 13 of 13 | Generator Loss: 0.869 | Discriminator Loss: 0.643 | Time: 0.23190879821777344 seconds.\n",
      "Epoch: 25 of 100000\n",
      "Epoch24 Batch 1 of 13 | Generator Loss: 0.869 | Discriminator Loss: 0.634 | Time: 0.22444748878479004 seconds.\n",
      "Epoch24 Batch 2 of 13 | Generator Loss: 0.86 | Discriminator Loss: 0.6 | Time: 0.251187801361084 seconds.\n",
      "Epoch24 Batch 3 of 13 | Generator Loss: 0.779 | Discriminator Loss: 0.654 | Time: 0.2484595775604248 seconds.\n",
      "Epoch24 Batch 4 of 13 | Generator Loss: 0.782 | Discriminator Loss: 0.669 | Time: 0.2472083568572998 seconds.\n",
      "Epoch24 Batch 5 of 13 | Generator Loss: 0.836 | Discriminator Loss: 0.668 | Time: 0.25984716415405273 seconds.\n",
      "Epoch24 Batch 6 of 13 | Generator Loss: 0.957 | Discriminator Loss: 0.638 | Time: 0.2471907138824463 seconds.\n",
      "Epoch24 Batch 7 of 13 | Generator Loss: 0.949 | Discriminator Loss: 0.623 | Time: 0.26844334602355957 seconds.\n",
      "Epoch24 Batch 8 of 13 | Generator Loss: 0.867 | Discriminator Loss: 0.652 | Time: 0.24617815017700195 seconds.\n",
      "Epoch24 Batch 9 of 13 | Generator Loss: 0.842 | Discriminator Loss: 0.641 | Time: 0.26174449920654297 seconds.\n",
      "Epoch24 Batch 10 of 13 | Generator Loss: 0.815 | Discriminator Loss: 0.725 | Time: 0.26546382904052734 seconds.\n",
      "Epoch24 Batch 11 of 13 | Generator Loss: 0.899 | Discriminator Loss: 0.685 | Time: 0.24414920806884766 seconds.\n",
      "Epoch24 Batch 12 of 13 | Generator Loss: 0.935 | Discriminator Loss: 0.75 | Time: 0.25279974937438965 seconds.\n",
      "Epoch24 Batch 13 of 13 | Generator Loss: 0.858 | Discriminator Loss: 0.65 | Time: 0.23046422004699707 seconds.\n",
      "Epoch: 26 of 100000\n",
      "Epoch25 Batch 1 of 13 | Generator Loss: 0.835 | Discriminator Loss: 0.677 | Time: 0.24065518379211426 seconds.\n",
      "Epoch25 Batch 2 of 13 | Generator Loss: 0.854 | Discriminator Loss: 0.644 | Time: 0.2625386714935303 seconds.\n",
      "Epoch25 Batch 3 of 13 | Generator Loss: 0.868 | Discriminator Loss: 0.689 | Time: 0.2348036766052246 seconds.\n",
      "Epoch25 Batch 4 of 13 | Generator Loss: 0.897 | Discriminator Loss: 0.673 | Time: 0.2513711452484131 seconds.\n",
      "Epoch25 Batch 5 of 13 | Generator Loss: 0.907 | Discriminator Loss: 0.643 | Time: 0.26769161224365234 seconds.\n",
      "Epoch25 Batch 6 of 13 | Generator Loss: 0.913 | Discriminator Loss: 0.657 | Time: 0.26824474334716797 seconds.\n",
      "Epoch25 Batch 7 of 13 | Generator Loss: 0.93 | Discriminator Loss: 0.614 | Time: 0.2674276828765869 seconds.\n",
      "Epoch25 Batch 8 of 13 | Generator Loss: 0.917 | Discriminator Loss: 0.669 | Time: 0.26868343353271484 seconds.\n",
      "Epoch25 Batch 9 of 13 | Generator Loss: 0.908 | Discriminator Loss: 0.67 | Time: 0.26570916175842285 seconds.\n",
      "Epoch25 Batch 10 of 13 | Generator Loss: 0.929 | Discriminator Loss: 0.643 | Time: 0.2719132900238037 seconds.\n",
      "Epoch25 Batch 11 of 13 | Generator Loss: 0.879 | Discriminator Loss: 0.642 | Time: 0.26253676414489746 seconds.\n",
      "Epoch25 Batch 12 of 13 | Generator Loss: 0.974 | Discriminator Loss: 0.627 | Time: 0.25643253326416016 seconds.\n",
      "Epoch25 Batch 13 of 13 | Generator Loss: 0.86 | Discriminator Loss: 0.603 | Time: 0.24335503578186035 seconds.\n",
      "Epoch: 27 of 100000\n",
      "Epoch26 Batch 1 of 13 | Generator Loss: 0.827 | Discriminator Loss: 0.638 | Time: 0.2557029724121094 seconds.\n",
      "Epoch26 Batch 2 of 13 | Generator Loss: 0.804 | Discriminator Loss: 0.657 | Time: 0.269092321395874 seconds.\n",
      "Epoch26 Batch 3 of 13 | Generator Loss: 0.818 | Discriminator Loss: 0.63 | Time: 0.2618110179901123 seconds.\n",
      "Epoch26 Batch 4 of 13 | Generator Loss: 0.852 | Discriminator Loss: 0.648 | Time: 0.2545011043548584 seconds.\n",
      "Epoch26 Batch 5 of 13 | Generator Loss: 0.92 | Discriminator Loss: 0.671 | Time: 0.2451770305633545 seconds.\n",
      "Epoch26 Batch 6 of 13 | Generator Loss: 0.909 | Discriminator Loss: 0.65 | Time: 0.24308276176452637 seconds.\n",
      "Epoch26 Batch 7 of 13 | Generator Loss: 0.849 | Discriminator Loss: 0.625 | Time: 0.24382686614990234 seconds.\n",
      "Epoch26 Batch 8 of 13 | Generator Loss: 0.853 | Discriminator Loss: 0.746 | Time: 0.26662349700927734 seconds.\n",
      "Epoch26 Batch 9 of 13 | Generator Loss: 0.844 | Discriminator Loss: 0.704 | Time: 0.25710415840148926 seconds.\n",
      "Epoch26 Batch 10 of 13 | Generator Loss: 0.867 | Discriminator Loss: 0.759 | Time: 0.2590975761413574 seconds.\n",
      "Epoch26 Batch 11 of 13 | Generator Loss: 0.878 | Discriminator Loss: 0.681 | Time: 0.2663712501525879 seconds.\n",
      "Epoch26 Batch 12 of 13 | Generator Loss: 1.012 | Discriminator Loss: 0.669 | Time: 0.24292516708374023 seconds.\n",
      "Epoch26 Batch 13 of 13 | Generator Loss: 0.934 | Discriminator Loss: 0.705 | Time: 0.2332010269165039 seconds.\n",
      "Epoch: 28 of 100000\n",
      "Epoch27 Batch 1 of 13 | Generator Loss: 0.876 | Discriminator Loss: 0.628 | Time: 0.24550485610961914 seconds.\n",
      "Epoch27 Batch 2 of 13 | Generator Loss: 0.835 | Discriminator Loss: 0.684 | Time: 0.2626166343688965 seconds.\n",
      "Epoch27 Batch 3 of 13 | Generator Loss: 0.858 | Discriminator Loss: 0.659 | Time: 0.27402186393737793 seconds.\n",
      "Epoch27 Batch 4 of 13 | Generator Loss: 0.797 | Discriminator Loss: 0.67 | Time: 0.27109193801879883 seconds.\n",
      "Epoch27 Batch 5 of 13 | Generator Loss: 0.804 | Discriminator Loss: 0.645 | Time: 0.25495481491088867 seconds.\n",
      "Epoch27 Batch 6 of 13 | Generator Loss: 0.94 | Discriminator Loss: 0.69 | Time: 0.2539207935333252 seconds.\n",
      "Epoch27 Batch 7 of 13 | Generator Loss: 0.946 | Discriminator Loss: 0.69 | Time: 0.25797009468078613 seconds.\n",
      "Epoch27 Batch 8 of 13 | Generator Loss: 0.818 | Discriminator Loss: 0.667 | Time: 0.25921201705932617 seconds.\n",
      "Epoch27 Batch 9 of 13 | Generator Loss: 0.834 | Discriminator Loss: 0.652 | Time: 0.25574421882629395 seconds.\n",
      "Epoch27 Batch 10 of 13 | Generator Loss: 0.865 | Discriminator Loss: 0.701 | Time: 0.24201297760009766 seconds.\n",
      "Epoch27 Batch 11 of 13 | Generator Loss: 0.942 | Discriminator Loss: 0.672 | Time: 0.25736117362976074 seconds.\n",
      "Epoch27 Batch 12 of 13 | Generator Loss: 0.88 | Discriminator Loss: 0.672 | Time: 0.24722933769226074 seconds.\n",
      "Epoch27 Batch 13 of 13 | Generator Loss: 0.831 | Discriminator Loss: 0.627 | Time: 0.25617456436157227 seconds.\n",
      "Epoch: 29 of 100000\n",
      "Epoch28 Batch 1 of 13 | Generator Loss: 0.788 | Discriminator Loss: 0.678 | Time: 0.25627851486206055 seconds.\n",
      "Epoch28 Batch 2 of 13 | Generator Loss: 0.769 | Discriminator Loss: 0.62 | Time: 0.26059842109680176 seconds.\n",
      "Epoch28 Batch 3 of 13 | Generator Loss: 0.753 | Discriminator Loss: 0.742 | Time: 0.25012969970703125 seconds.\n",
      "Epoch28 Batch 4 of 13 | Generator Loss: 0.871 | Discriminator Loss: 0.689 | Time: 0.27977895736694336 seconds.\n",
      "Epoch28 Batch 5 of 13 | Generator Loss: 0.927 | Discriminator Loss: 0.704 | Time: 0.27123475074768066 seconds.\n",
      "Epoch28 Batch 6 of 13 | Generator Loss: 0.985 | Discriminator Loss: 0.649 | Time: 0.249298095703125 seconds.\n",
      "Epoch28 Batch 7 of 13 | Generator Loss: 0.843 | Discriminator Loss: 0.695 | Time: 0.23838448524475098 seconds.\n",
      "Epoch28 Batch 8 of 13 | Generator Loss: 0.791 | Discriminator Loss: 0.663 | Time: 0.27758026123046875 seconds.\n",
      "Epoch28 Batch 9 of 13 | Generator Loss: 0.794 | Discriminator Loss: 0.657 | Time: 0.2874422073364258 seconds.\n",
      "Epoch28 Batch 10 of 13 | Generator Loss: 0.869 | Discriminator Loss: 0.693 | Time: 0.2573106288909912 seconds.\n",
      "Epoch28 Batch 11 of 13 | Generator Loss: 0.885 | Discriminator Loss: 0.701 | Time: 0.25493717193603516 seconds.\n",
      "Epoch28 Batch 12 of 13 | Generator Loss: 0.809 | Discriminator Loss: 0.65 | Time: 0.2690587043762207 seconds.\n",
      "Epoch28 Batch 13 of 13 | Generator Loss: 0.722 | Discriminator Loss: 0.612 | Time: 0.251065731048584 seconds.\n",
      "Epoch: 30 of 100000\n",
      "Epoch29 Batch 1 of 13 | Generator Loss: 0.765 | Discriminator Loss: 0.644 | Time: 0.25655579566955566 seconds.\n",
      "Epoch29 Batch 2 of 13 | Generator Loss: 0.751 | Discriminator Loss: 0.617 | Time: 0.28387022018432617 seconds.\n",
      "Epoch29 Batch 3 of 13 | Generator Loss: 0.831 | Discriminator Loss: 0.659 | Time: 0.2573580741882324 seconds.\n",
      "Epoch29 Batch 4 of 13 | Generator Loss: 0.877 | Discriminator Loss: 0.634 | Time: 0.27176761627197266 seconds.\n",
      "Epoch29 Batch 5 of 13 | Generator Loss: 0.844 | Discriminator Loss: 0.661 | Time: 0.2779083251953125 seconds.\n",
      "Epoch29 Batch 6 of 13 | Generator Loss: 0.848 | Discriminator Loss: 0.652 | Time: 0.2710995674133301 seconds.\n",
      "Epoch29 Batch 7 of 13 | Generator Loss: 0.785 | Discriminator Loss: 0.633 | Time: 0.2557950019836426 seconds.\n",
      "Epoch29 Batch 8 of 13 | Generator Loss: 0.795 | Discriminator Loss: 0.644 | Time: 0.2733488082885742 seconds.\n",
      "Epoch29 Batch 9 of 13 | Generator Loss: 0.901 | Discriminator Loss: 0.687 | Time: 0.2607588768005371 seconds.\n",
      "Epoch29 Batch 10 of 13 | Generator Loss: 0.835 | Discriminator Loss: 0.658 | Time: 0.27384448051452637 seconds.\n",
      "Epoch29 Batch 11 of 13 | Generator Loss: 0.84 | Discriminator Loss: 0.709 | Time: 0.29183387756347656 seconds.\n",
      "Epoch29 Batch 12 of 13 | Generator Loss: 0.863 | Discriminator Loss: 0.653 | Time: 0.26821017265319824 seconds.\n",
      "Epoch29 Batch 13 of 13 | Generator Loss: 0.775 | Discriminator Loss: 0.702 | Time: 0.24361705780029297 seconds.\n",
      "Epoch: 31 of 100000\n",
      "Epoch30 Batch 1 of 13 | Generator Loss: 0.742 | Discriminator Loss: 0.634 | Time: 0.25683021545410156 seconds.\n",
      "Epoch30 Batch 2 of 13 | Generator Loss: 0.776 | Discriminator Loss: 0.626 | Time: 0.26813507080078125 seconds.\n",
      "Epoch30 Batch 3 of 13 | Generator Loss: 0.894 | Discriminator Loss: 0.675 | Time: 0.2640349864959717 seconds.\n",
      "Epoch30 Batch 4 of 13 | Generator Loss: 0.816 | Discriminator Loss: 0.641 | Time: 0.27208542823791504 seconds.\n",
      "Epoch30 Batch 5 of 13 | Generator Loss: 0.808 | Discriminator Loss: 0.657 | Time: 0.2719252109527588 seconds.\n",
      "Epoch30 Batch 6 of 13 | Generator Loss: 0.838 | Discriminator Loss: 0.653 | Time: 0.2664806842803955 seconds.\n",
      "Epoch30 Batch 7 of 13 | Generator Loss: 0.837 | Discriminator Loss: 0.655 | Time: 0.2588667869567871 seconds.\n",
      "Epoch30 Batch 8 of 13 | Generator Loss: 0.849 | Discriminator Loss: 0.665 | Time: 0.2570371627807617 seconds.\n",
      "Epoch30 Batch 9 of 13 | Generator Loss: 0.92 | Discriminator Loss: 0.71 | Time: 0.25455427169799805 seconds.\n",
      "Epoch30 Batch 10 of 13 | Generator Loss: 0.876 | Discriminator Loss: 0.676 | Time: 0.26149749755859375 seconds.\n",
      "Epoch30 Batch 11 of 13 | Generator Loss: 0.803 | Discriminator Loss: 0.645 | Time: 0.2542858123779297 seconds.\n",
      "Epoch30 Batch 12 of 13 | Generator Loss: 0.909 | Discriminator Loss: 0.683 | Time: 0.2706873416900635 seconds.\n",
      "Epoch30 Batch 13 of 13 | Generator Loss: 0.887 | Discriminator Loss: 0.662 | Time: 0.2786109447479248 seconds.\n",
      "Epoch: 32 of 100000\n",
      "Epoch31 Batch 1 of 13 | Generator Loss: 0.798 | Discriminator Loss: 0.671 | Time: 0.2632937431335449 seconds.\n",
      "Epoch31 Batch 2 of 13 | Generator Loss: 0.867 | Discriminator Loss: 0.63 | Time: 0.2616910934448242 seconds.\n",
      "Epoch31 Batch 3 of 13 | Generator Loss: 0.846 | Discriminator Loss: 0.653 | Time: 0.288898229598999 seconds.\n",
      "Epoch31 Batch 4 of 13 | Generator Loss: 0.846 | Discriminator Loss: 0.646 | Time: 0.28614044189453125 seconds.\n",
      "Epoch31 Batch 5 of 13 | Generator Loss: 0.921 | Discriminator Loss: 0.654 | Time: 0.27805352210998535 seconds.\n",
      "Epoch31 Batch 6 of 13 | Generator Loss: 0.844 | Discriminator Loss: 0.685 | Time: 0.26900267601013184 seconds.\n",
      "Epoch31 Batch 7 of 13 | Generator Loss: 0.792 | Discriminator Loss: 0.667 | Time: 0.25441956520080566 seconds.\n",
      "Epoch31 Batch 8 of 13 | Generator Loss: 0.757 | Discriminator Loss: 0.649 | Time: 0.2725410461425781 seconds.\n",
      "Epoch31 Batch 9 of 13 | Generator Loss: 0.768 | Discriminator Loss: 0.661 | Time: 0.2709639072418213 seconds.\n",
      "Epoch31 Batch 10 of 13 | Generator Loss: 0.794 | Discriminator Loss: 0.651 | Time: 0.2593233585357666 seconds.\n",
      "Epoch31 Batch 11 of 13 | Generator Loss: 0.822 | Discriminator Loss: 0.637 | Time: 0.2625279426574707 seconds.\n",
      "Epoch31 Batch 12 of 13 | Generator Loss: 0.828 | Discriminator Loss: 0.648 | Time: 0.2800476551055908 seconds.\n",
      "Epoch31 Batch 13 of 13 | Generator Loss: 0.731 | Discriminator Loss: 0.698 | Time: 0.256211519241333 seconds.\n",
      "Epoch: 33 of 100000\n",
      "Epoch32 Batch 1 of 13 | Generator Loss: 0.801 | Discriminator Loss: 0.712 | Time: 0.2510666847229004 seconds.\n",
      "Epoch32 Batch 2 of 13 | Generator Loss: 0.792 | Discriminator Loss: 0.683 | Time: 0.26938319206237793 seconds.\n",
      "Epoch32 Batch 3 of 13 | Generator Loss: 0.764 | Discriminator Loss: 0.634 | Time: 0.26325249671936035 seconds.\n",
      "Epoch32 Batch 4 of 13 | Generator Loss: 0.747 | Discriminator Loss: 0.648 | Time: 0.2507643699645996 seconds.\n",
      "Epoch32 Batch 5 of 13 | Generator Loss: 0.687 | Discriminator Loss: 0.639 | Time: 0.27547192573547363 seconds.\n",
      "Epoch32 Batch 6 of 13 | Generator Loss: 0.702 | Discriminator Loss: 0.68 | Time: 0.2562999725341797 seconds.\n",
      "Epoch32 Batch 7 of 13 | Generator Loss: 0.754 | Discriminator Loss: 0.632 | Time: 0.28017258644104004 seconds.\n",
      "Epoch32 Batch 8 of 13 | Generator Loss: 0.859 | Discriminator Loss: 0.728 | Time: 0.2667419910430908 seconds.\n",
      "Epoch32 Batch 9 of 13 | Generator Loss: 0.91 | Discriminator Loss: 0.698 | Time: 0.2599372863769531 seconds.\n",
      "Epoch32 Batch 10 of 13 | Generator Loss: 0.785 | Discriminator Loss: 0.747 | Time: 0.26995229721069336 seconds.\n",
      "Epoch32 Batch 11 of 13 | Generator Loss: 0.833 | Discriminator Loss: 0.674 | Time: 0.25960206985473633 seconds.\n",
      "Epoch32 Batch 12 of 13 | Generator Loss: 0.835 | Discriminator Loss: 0.727 | Time: 0.30684542655944824 seconds.\n",
      "Epoch32 Batch 13 of 13 | Generator Loss: 0.759 | Discriminator Loss: 0.731 | Time: 0.2769293785095215 seconds.\n",
      "Epoch: 34 of 100000\n",
      "Epoch33 Batch 1 of 13 | Generator Loss: 0.794 | Discriminator Loss: 0.662 | Time: 0.2520444393157959 seconds.\n",
      "Epoch33 Batch 2 of 13 | Generator Loss: 0.801 | Discriminator Loss: 0.674 | Time: 0.26629042625427246 seconds.\n",
      "Epoch33 Batch 3 of 13 | Generator Loss: 0.714 | Discriminator Loss: 0.674 | Time: 0.27022814750671387 seconds.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2933664e25ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0mmakePokemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m \u001b[0mmakePokemon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-2933664e25ca>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0mfakePokemonImages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 \u001b[0mgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfakePokemon_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m                 \u001b[0madversarialLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madversarialLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrentBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1725\u001b[0m                                                     class_weight)\n\u001b[1;32m   1726\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Finally, we add everything together into a mega-Class.\n",
    "#The real heavy lifting is in the final method here - our training method!\n",
    "#Running this cell will build our GAN.\n",
    "\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import keras\n",
    "import math\n",
    "import IPython\n",
    "\n",
    "class GAN:\n",
    "    def __init__(self):\n",
    "        self.dataFolder = \"./pocketMonsters\"\n",
    "        self.targetShape = (64, 64, 3)\n",
    "        self.batchSize = 64\n",
    "        self.epochs = 100000\n",
    "\n",
    "    #Note in our data generator, we're not doing any image augmentation.\n",
    "    #You could, but you would want to limit yourself to generators that\n",
    "    #retain the look, feel, heart and soul of pocketmonsters.\n",
    "    def loadData(self):\n",
    "        dataset_generator = keras.preprocessing.image.ImageDataGenerator().flow_from_directory(\n",
    "            self.dataFolder, target_size=(self.targetShape[0], self.targetShape[1]),\n",
    "            batch_size=self.batchSize,\n",
    "            class_mode=None)\n",
    "\n",
    "        return dataset_generator\n",
    "\n",
    "    def loadDiscriminator(self):\n",
    "        discriminator = keras.models.Sequential()\n",
    "        discriminator.add(keras.layers.Conv2D(filters=64, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform',\n",
    "                                    input_shape=self.targetShape))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Conv2D(filters=128, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform'))\n",
    "        discriminator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Conv2D(filters=256, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform'))\n",
    "        discriminator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Conv2D(filters=512, kernel_size=(5, 5),\n",
    "                                    strides=(2, 2), padding='same',\n",
    "                                    data_format='channels_last',\n",
    "                                    kernel_initializer='glorot_uniform'))\n",
    "        discriminator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        discriminator.add(keras.layers.LeakyReLU(0.2))\n",
    "\n",
    "        discriminator.add(keras.layers.Flatten())\n",
    "        discriminator.add(keras.layers.Dense(1))\n",
    "        discriminator.add(keras.layers.Activation('sigmoid'))\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "        discriminator.compile(loss='binary_crossentropy',\n",
    "                                optimizer=optimizer,\n",
    "                                metrics=None)\n",
    "\n",
    "        return discriminator\n",
    "\n",
    "    def loadGenerator(self):\n",
    "        generator = keras.models.Sequential()\n",
    "        generator.add(keras.layers.Dense(units=4 * 4 * 512,\n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            input_shape=(1, 1, 100)))\n",
    "        generator.add(keras.layers.Reshape(target_shape=(4, 4, 512)))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=256, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=128, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=64, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.BatchNormalization(momentum=0.5))\n",
    "        generator.add(keras.layers.Activation('relu'))\n",
    "\n",
    "        generator.add(keras.layers.Conv2DTranspose(filters=3, kernel_size=(5, 5),\n",
    "                                      strides=(2, 2), padding='same',\n",
    "                                      data_format='channels_last',\n",
    "                                      kernel_initializer='glorot_uniform'))\n",
    "        generator.add(keras.layers.Activation('tanh'))\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=0.00015, beta_1=0.5)\n",
    "        generator.compile(loss='binary_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=None)\n",
    "\n",
    "        return generator\n",
    "\n",
    "    def saveImages(self, genImage, epochNum):\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            grid = gridspec.GridSpec(8, 8)\n",
    "            grid.update(wspace=0.1, hspace=0.1)\n",
    "\n",
    "            for i in range(64):\n",
    "                ax1 = plt.subplot(grid[i])\n",
    "                ax1.set_aspect('equal')\n",
    "                image = genImage[i, :, :, :]\n",
    "\n",
    "                #image += 1\n",
    "                image *= 255\n",
    "                fig = plt.imshow(image.astype(np.uint8))\n",
    "                plt.axis('off')\n",
    "                fig.axes.get_xaxis().set_visible(False)\n",
    "                fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "            #Small changes to this function for our run: we'll only save\n",
    "            #once every five epochs for speed.\n",
    "            save_name = 'myPokemon/epoch' + str(epochNum + 1) + '.png'\n",
    "            if not os.path.exists('myPokemon'):\n",
    "                os.mkdir('myPokemon')\n",
    "            plt.savefig(save_name, bbox_inches='tight', pad_inches=0)\n",
    "            #We're commenting out this show for now, to let our script run faster.\n",
    "            #We'll just save to a file.\n",
    "            #plt.show()\n",
    "            plt.close()\n",
    "\n",
    "    def loadGAN(self, generator, discriminator):\n",
    "        m = keras.models.Sequential()\n",
    "        discriminator.trainable = False\n",
    "        m.add(generator)\n",
    "        m.add(discriminator)\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=0.00015, beta_1=0.5)\n",
    "        m.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "                    metrics=None)\n",
    "        return m\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        #Alright!  Now for the big payoff.\n",
    "        #First, we load our three models - the generator,\n",
    "        #discrimanator, and the GAN that strings the together.\n",
    "        generator = self.loadGenerator()\n",
    "        discriminator = self.loadDiscriminator()\n",
    "        gan = self.loadGAN(generator, discriminator)\n",
    "\n",
    "        # Load our pokemon data\n",
    "        dataGenerator = self.loadData()\n",
    "        \n",
    "        #Calculate the number of batches per epoch required\n",
    "        numBatches = math.ceil(dataGenerator.samples/self.batchSize)\n",
    "\n",
    "        # Save our losses\n",
    "        adversarialLoss = np.empty(shape=1)\n",
    "        discriminatorLoss = np.empty(shape=1)\n",
    "        batches = np.empty(shape=1)\n",
    "\n",
    "        #Let's us show the outputs in Jupyter\n",
    "        plt.ion()\n",
    "\n",
    "        currentBatch = 0\n",
    "\n",
    "        #Training loop starts here!\n",
    "        for epoch in range(self.epochs):\n",
    "            print(\"Epoch: \" + str(epoch + 1) + \" of \" + str(self.epochs))\n",
    "            for batchNum in range(numBatches):\n",
    "                startTime = time.time()\n",
    "\n",
    "                #First we load in this batch of real images:\n",
    "                realPokemonImages = dataGenerator.next()\n",
    "\n",
    "                #Here, we normalize to the color scale of the input pokemon\n",
    "                \n",
    "                realPokemonImages /= 255\n",
    "                #realPokemonImages -= 1\n",
    "\n",
    "                #And calculate how many images we got:\n",
    "                curBatchSize = realPokemonImages.shape[0]\n",
    "\n",
    "                #Here, we generate noise to input into our generator (remember the input of 100 on our generators Dense input!)\n",
    "                noise = np.random.normal(0, 1, size=(curBatchSize,) + (1, 1, 100))\n",
    "\n",
    "                #Make our new pokemon\n",
    "                fakePokemonImages = generator.predict(noise)\n",
    "\n",
    "                #We're going to update our discriminator, but don't want to tell it\n",
    "                #exactly what is fake and what is real (as then it would train on our)\n",
    "                #fake images, and we would never be able to fool it.\n",
    "                #So, when we update we update with a \"noisy\" version of our Y, \n",
    "                #in which we randomly label a few true cases as fake,\n",
    "                #and vice-versa.\n",
    "                realPokemon_y = (np.ones(curBatchSize) - np.random.random_sample(curBatchSize) * 0.2)\n",
    "                fakePokemon_y = np.random.random_sample(curBatchSize) * 0.2\n",
    "\n",
    "                #This is where we update the Discriminator.\n",
    "                #We don't allow it to be trained in the adverserial part of the GAN,\n",
    "                #so we must manually do it here.\n",
    "                discriminator.trainable = True\n",
    "\n",
    "                #Here we train with both batches, and then save our loss and turn off\n",
    "                #the discriminator training\n",
    "                discLoss = discriminator.train_on_batch(realPokemonImages, realPokemon_y)\n",
    "                discLoss += discriminator.train_on_batch(fakePokemonImages, fakePokemon_y)\n",
    "\n",
    "                discriminatorLoss = np.append(discriminatorLoss, discLoss)\n",
    "                discriminator.trainable = False\n",
    "\n",
    "\n",
    "                #Now we are going to generate our pokemon!\n",
    "                #Note here we're going to generate 64 \"imaginary\"\n",
    "                #pokemon (i.e., those not living in Williamsburg)\n",
    "                #This number is independent of your batch size - i.e.,\n",
    "                #earlier in the training loop, we created 32 examples\n",
    "                #to train our discriminator - this was so we had the same\n",
    "                #number of real and fake cases.\n",
    "                noise = np.random.normal(0, 1,size=(64,) + (1, 1, 100))\n",
    "\n",
    "                #As before, we are going to assign a fraction of our fake cases to \"true\" cases\n",
    "                #to see how well we can fool the discriminator\n",
    "                fakePokemon_y = (np.ones(64) - np.random.random_sample(64) * 0.2)\n",
    "\n",
    "                #Make the actual images\n",
    "                fakePokemonImages = generator.predict(noise)\n",
    "\n",
    "                gLoss = gan.train_on_batch(noise, fakePokemon_y)\n",
    "                adversarialLoss = np.append(adversarialLoss, gLoss)\n",
    "                batches = np.append(batches, currentBatch)                    \n",
    "                \n",
    "                timeElapsed = time.time() - startTime\n",
    "                print(\"Epoch\" + str(epoch) + \" Batch \" + str(batchNum + 1) + \" of \" + str(numBatches) + \" | Generator Loss: \" + str(round(gLoss, 3)) + \" | Discriminator Loss: \" + str(round(discLoss, 3)) + \" | Time: \" + str(timeElapsed) + \" seconds.\")\n",
    "\n",
    "\n",
    "                currentBatch += 1\n",
    "\n",
    "            # Regularly save model weights and images\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                discriminator.trainable = True\n",
    "                if not os.path.exists('models'):\n",
    "                    os.mkdir('models')\n",
    "                generator.save('models/genEpoch' + str(epoch) + '.hdf5')\n",
    "                discriminator.save('models/discEpoch' + str(epoch) + '.hdf5')\n",
    "                self.saveImages(fakePokemonImages, epoch)\n",
    "\n",
    "            # Update our loss graph\n",
    "            plt.figure(1)\n",
    "            plt.plot(batches, adversarialLoss, color='green',\n",
    "                     label='Generator Loss')\n",
    "            plt.plot(batches, discriminatorLoss, color='blue',\n",
    "                     label='Discriminator Loss')\n",
    "            plt.title(\"GAN Training\")\n",
    "            plt.xlabel(\"Batch Iteration\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            if epoch == 0:\n",
    "                plt.legend()\n",
    "            plt.savefig('trainingLossPlot.png')\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "makePokemon = GAN()\n",
    "makePokemon.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}