{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#In this notebook, we're going to look at how to add convolutions into our network.\n",
    "#We'll be using this to run a convolutional network, and contrasting the convolutional\n",
    "#approach to the non-convolutonal approach we've used to date.\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "with open(\"testTrainLab2.pickle\", \"rb\") as f:\n",
    "    labData = pickle.load(f)\n",
    "\n",
    "y_train = labData[\"y_train\"]\n",
    "y_test = labData[\"y_test\"]\n",
    "\n",
    "def preProcessing(train, test, arrayReshape=True, zeroShift=True, zeroShiftVis = True):\n",
    "    if(zeroShift == True):\n",
    "        mean_image = np.average(train, axis=0)\n",
    "        if(zeroShiftVis == True):\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) \n",
    "            plt.show()\n",
    "        train -= mean_image\n",
    "        test -= mean_image\n",
    "\n",
    "    if(arrayReshape == True):\n",
    "        train = np.reshape(train, (train.shape[0], -1))\n",
    "        test = np.reshape(test, (test.shape[0], -1))\n",
    "\n",
    "    return(train, test)\n",
    "\n",
    "#Note that we're setting arrayReshape to false for convolutional\n",
    "#approaches, as we need to retain the image structure.\n",
    "X_train, X_test = preProcessing(train = labData[\"X_train\"].copy(), \n",
    "                                test = labData[\"X_test\"].copy(),\n",
    "                                arrayReshape = False,\n",
    "                                zeroShift = True,\n",
    "                                zeroShiftVis = False)\n",
    "\n",
    "\n",
    "#Of note, now our X_train dimensions are 50,000 x 32 x 32 x 3 -\n",
    "#this represents the 50k examples that are each 32x32x3(color bands) images\n",
    "#in CIFAR.  IN previous steps, we've flattened these out into a \n",
    "#50,000 x 3072 matrix, but we won't be doing that \n",
    "#if we want to convolve.\n",
    "print(X_train.shape)\n",
    "\n",
    "#We're also going to split our training dataset one more time here, so that we have\n",
    "#an independent validation and testing dataset.  Go all the way back to lecture 2\n",
    "#for more on this approach!\n",
    "s1 = np.random.choice(range(X_train.shape[0]), 10000, replace=False)\n",
    "s2 = list(set(range(X_train.shape[0])) - set(s1))\n",
    "\n",
    "y_val = y_train[s1]\n",
    "y_train = y_train[s2]\n",
    "X_val = X_train[s1, :, :]\n",
    "X_train = X_train[s2, :, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "START: Epoch 1 of 2\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 20 is out of bounds for axis 0 with size 20",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-22e1c89b2385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    383\u001b[0m                   weightType = \"He\")\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m myConvNet.fit(epochs = 2,\n\u001b[0m\u001b[1;32m    386\u001b[0m               \u001b[0mlearningRate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m               \u001b[0mbatchSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-145-22e1c89b2385>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, learningRate, batchSize, visualization, lossType, lossParams)\u001b[0m\n\u001b[1;32m    295\u001b[0m                     \u001b[0mlayerCounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayerCounter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layerType'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'convolutional'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                         layerOutputs[\"L_grad_\" + str(layerCounter) + i['layerType']] = self.convolutionalBackward(upstreamGradient = upstreamGradient,\n\u001b[0m\u001b[1;32m    298\u001b[0m                                                                                        cache = layerOutputs[\"L_out_\" + str(layerCounter) + i['layerType']][1])\n\u001b[1;32m    299\u001b[0m                         \u001b[0mweightGradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layerType'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayerCounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayerOutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"L_grad_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayerCounter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layerType'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-145-22e1c89b2385>\u001b[0m in \u001b[0;36mconvolutionalBackward\u001b[0;34m(self, upstreamGradient, cache)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                                 \u001b[0mdxWindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                 \u001b[0mdxWindow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdxWindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupstreamGradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                                 \u001b[0;31m#Here, we need to grab the *weights* within the window,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 20 is out of bounds for axis 0 with size 20"
     ]
    }
   ],
   "source": [
    "#We're going to use a class this time, but we'll be implementing it in a modular fashion.\n",
    "#Your challenge will be to build on this example to do things like\n",
    "#add more layers for the lab assignment.\n",
    "#We're also going to implement a more rigorous training procedure\n",
    "#with epochs for the first time!  The class below build sheavily on\n",
    "#the class we wrote in part 3.\n",
    "\n",
    "#The example version of this class is going to implement a two-layer ConvNet,\n",
    "#But you're going to want to go further for the lab submission.\n",
    "\n",
    "modelArchitecture = [{'layerType':'convolutional', 'inputSize':3072, 'outputSize':20, 'filterSize':5},\n",
    "                     {'layerType':'relu', 'inputSize':20, 'outputSize':20},\n",
    "                     {'layerType':'affine', 'inputSize':20, 'outputSize':10, 'activation':\"relu\"}]\n",
    "    \n",
    "\n",
    "class convolutionalNeuralNet():\n",
    "    def __init__(self, modelArchitecture,\n",
    "                X_train, y_train, X_test, y_test, X_val, y_val,\n",
    "                weightType = \"He\"):\n",
    "        self.weights = {}\n",
    "        self.gradients = {}\n",
    "        self.weightType = weightType\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.modelArchitecture = modelArchitecture\n",
    "\n",
    "        #Initialize the weights for each layer\n",
    "        layerCounter = 1\n",
    "        for i in modelArchitecture:\n",
    "            if(i['layerType'] == \"affine\"):\n",
    "                if(self.weightType == \"random\"):\n",
    "                    self.weights[\"W\" + i['layerType'] + str(layerCounter)] = np.random.randn(i['inputSize'], i['outputSize'])\n",
    "                    self.weights[\"B\" + i['layerType'] + str(layerCounter)] = np.random.randn(i['outputSize'])\n",
    "                if(self.weightType == \"Xavier\"):\n",
    "                    self.weights[\"W\" + i['layerType']  + str(layerCounter)] = np.random.randn(i['inputSize'], i['outputSize']) / np.sqrt(i['inputSize'])\n",
    "                    self.weights[\"B\" + i['layerType']  + str(layerCounter)] = np.random.randn(i['outputSize']) / np.sqrt(i['inputSize'])\n",
    "                if(self.weightType == \"He\"):\n",
    "                    self.weights[\"W\" + i['layerType']  + str(layerCounter)] = np.random.randn(i['inputSize'], i['outputSize']) / np.sqrt(i['inputSize']/2)\n",
    "                    self.weights[\"B\" + i['layerType']  + str(layerCounter)] = np.random.randn(i['outputSize']) / np.sqrt(i['inputSize']/2)\n",
    "            \n",
    "\n",
    "            #if the layer type is convolutional, we need filter weights that will be equal to the size of the activation surface.\n",
    "            #This is predicated on your filter values. Note the below is assuming all filters are across all 3 channels.\n",
    "            if(i['layerType'] == \"convolutional\"):\n",
    "                #One filter per output\n",
    "                numberOfFilters = i['outputSize']\n",
    "                if(self.weightType == \"random\"):\n",
    "                    self.weights[\"W\" + i['layerType'] + str(layerCounter)] = np.random.randn(numberOfFilters,i['filterSize'],i['filterSize'], 3)\n",
    "                    self.weights[\"B\" + i['layerType'] + str(layerCounter)] = np.random.randn(i['outputSize'])\n",
    "                if(self.weightType == \"Xavier\"):\n",
    "                    self.weights[\"W\" + i['layerType']  + str(layerCounter)] = np.random.randn(numberOfFilters,i['filterSize'],i['filterSize'], 3) / np.sqrt(i['inputSize'])\n",
    "                    self.weights[\"B\" + i['layerType']  + str(layerCounter)] = np.random.randn(i['outputSize']) / np.sqrt(i['inputSize'])\n",
    "                if(self.weightType == \"He\"):\n",
    "                    self.weights[\"W\" + i['layerType']  + str(layerCounter)] = np.random.randn(numberOfFilters,i['filterSize'],i['filterSize'], 3) / np.sqrt(i['inputSize']/2)\n",
    "                    self.weights[\"B\" + i['layerType']  + str(layerCounter)] = np.random.randn(i['outputSize']) / np.sqrt(i['inputSize']/2)\n",
    "\n",
    "            layerCounter = layerCounter + 1\n",
    "\n",
    "\n",
    "    #Here, we add in our different types of layers - in every case but convolutional, these are just copies from part 4.\n",
    "    #All that's changed is a little bit of notation standardization.\n",
    "    def affineForward(self, X, W, B):\n",
    "        N = X.shape[0]\n",
    "        D = np.prod(X.shape[1:])\n",
    "        xReshape = np.reshape(X, (N, D))\n",
    "        out = np.dot(xReshape, W) + B\n",
    "        cache = (X, W, B)\n",
    "        return(out, cache)\n",
    "    \n",
    "    def affineBackward(self, upstreamGradient, cache):\n",
    "        X, W, B = cache\n",
    "        N = X.shape[0]\n",
    "        D = np.prod(X.shape[1:])\n",
    "        xReshape = np.reshape(X, (N, D))\n",
    "        dx = np.reshape(np.dot(upstreamGradient, W.T), X.shape) \n",
    "        dw = np.dot(xReshape.T, upstreamGradient) \n",
    "        db = np.dot(upstreamGradient.T, np.ones(N)) \n",
    "        return(dx, dw, db)\n",
    "\n",
    "    #Here is the new layer - our convolutional layer.\n",
    "    #The inputs are the same as any affine layer, but\n",
    "    #we need to define our stride and filter size.\n",
    "    #We'll dynamically calculate padding to fit everything \n",
    "    #(you could manually specify padding as well, but this will work)\n",
    "    #for most cases.\n",
    "\n",
    "    def convolutionalForward(self, X, W, B, stride=2):\n",
    "        #Alright - first we need to unpack our incoming images.\n",
    "        #Remember that X is in the format (Observations, Height, Width, Channels)\n",
    "        (N, Height, Width, Channels) = X.shape\n",
    "\n",
    "        #W dimensions, where from our init:\n",
    "        #F - number of filters\n",
    "        #fSize - filter size\n",
    "        (F, filterSize, filterSize, imageChannels) = W.shape\n",
    "        \n",
    "\n",
    "        #Let's calculate how big our output will be with no pooling:\n",
    "        convH = int((Height-filterSize)/stride)+1\n",
    "        convW = int((Width-filterSize)/stride)+1\n",
    "        \n",
    "        #Initialize our activation surface with 0s\n",
    "        activationSurface = np.zeros((N, F, convH, convW, imageChannels))\n",
    "        out = np.zeros((N,F))\n",
    "        #Alright - we're going to do this in a piece-by-piece way for now.\n",
    "        #A vectorized implementation of this will be MUCH faster.\n",
    "        for i in range(N):\n",
    "            #Select one image at a time\n",
    "            x = X[i]\n",
    "            #Select one filter at a time\n",
    "            for f in range(F):\n",
    "                for h in range(0, Height, stride):\n",
    "                    for w in range(Width):\n",
    "                        for c in range(Channels):\n",
    "                            #Define the window for this convolution\n",
    "                            y_upper = h * stride\n",
    "                            y_lower = y_upper + filterSize\n",
    "                            x_left = w * stride\n",
    "                            x_right = x_left + filterSize\n",
    "\n",
    "                            #Grab the window!  Note the \":\" at the end means \"grab all 3 bands\"\n",
    "                            window = x[y_upper:y_lower, x_left:x_right, :]\n",
    "\n",
    "                            #If the step is valid, we apply our filter weights.\n",
    "                            #Note right now we aren't doing any padding,\n",
    "                            #but this is where you would add it!\n",
    "                            if((window.shape[0] == filterSize) and window.shape[1] == filterSize):\n",
    "                                s = np.multiply(window, W[f])\n",
    "                                activationSurface[i,f,h,w,c] = np.sum(s)\n",
    "                                activationSurface[i,f,h,w,c] = activationSurface[i,f,h,w,c] + np.sum(B)\n",
    "                                #For this example, we're going to implement a Max-pool.\n",
    "                                #Because we expect our output to be equal to the number of filters, we need\n",
    "                                #to take our activation surfaces and aggregate them.\n",
    "                                #Right now, we have 20 activation surfaces per image, instead of 20 values per image.\n",
    "                                out[i,f] = np.max(activationSurface[i,f,h,w,c])\n",
    "        \n",
    "        cache = [X, W, B, stride]\n",
    "\n",
    "        return(out, cache)\n",
    "\n",
    "    #And, here is our backward for the convolutional layer we implemented:\n",
    "    def convolutionalBackward(self, upstreamGradient, cache):\n",
    "        (X, W, B, stride) = cache\n",
    "        \n",
    "        (N, Height, Width, Channels) = X.shape\n",
    "        (F, filterSize, filterSize, imageChannels) = W.shape\n",
    "\n",
    "        #Initialize our outputs\n",
    "        dx = np.zeros_like(X)\n",
    "        dw = np.zeros_like(W)\n",
    "        db = np.zeros_like(B)\n",
    "\n",
    "        #Just like in the forward pass, we're going to spell this all the way out.\n",
    "        for n in range(N):\n",
    "            for f in range(F):\n",
    "                for h in range(0, Height, stride):\n",
    "                    for w in range(Width):\n",
    "                        for c in range(Channels):\n",
    "                            #Define the window for this convolution\n",
    "                            #We need to recreate these to calculate the backprop\n",
    "                            y_upper = h * stride\n",
    "                            y_lower = y_upper + filterSize\n",
    "                            x_left = w * stride\n",
    "                            x_right = x_left + filterSize\n",
    "\n",
    "                            window = X[n, y_upper:y_lower, x_left:x_right, :]\n",
    "                            \n",
    "\n",
    "\n",
    "                            #If the step is valid, we apply our filter weights.\n",
    "                            #Note right now we aren't doing any padding,\n",
    "                            #but this is where you would add it!\n",
    "                            if((window.shape[0] == filterSize) and window.shape[1] == filterSize):\n",
    "                                #First, we need to calculate the gradients with regard to the pooling\n",
    "                                #function we used.  In the forward pass, we use:\n",
    "                                #out[i,f] = np.max(activationSurface[i,f,h,w,c])\n",
    "                                #In some implementations, you may see this pooling layer\n",
    "                                #as a seperate function - i.e., just like the relu or affine layers here.\n",
    "                                #Because the pooling is so common, here we just integrate it into the convolutional\n",
    "                                #layer.  Because of that, to calculate our gradient, we first need to propogate through\n",
    "                                #the pool.\n",
    "\n",
    "                                #In the case of the max pool, only one of the inputs gets a weight - i.e., the max value\n",
    "                                #gets the full gradient.  All other activation cells get a 0.\n",
    "                                \n",
    "                                dxWindow = np.zeros_like(window)\n",
    "                                dxWindow[np.argmax(dxWindow)] = upstreamGradient[n]\n",
    "\n",
    "                                #Here, we need to grab the *weights* within the window,\n",
    "                                #rather than the X values (like we did in the forward pass).\n",
    "                                dx[n, y_upper:y_lower, x_left:x_right, :] += W[f,:,:,:] * dxWindow\n",
    "\n",
    "                                #Pass back our W\n",
    "                                dw[f,:,:,:] += window * dxWindow\n",
    "\n",
    "                                #And, finally, bias.  \n",
    "                                db[f] += np.sum(dxWindow)\n",
    "\n",
    "        return(dx, dw, db)\n",
    "\n",
    "    #Activations - for now, just relu:\n",
    "    def reluForward(self, X):\n",
    "        out = np.maximum(X, 0)\n",
    "        cache = X\n",
    "        return(out, cache)\n",
    "    \n",
    "    def reluBackward(self, upstreamGradient, cache):\n",
    "        x = cache\n",
    "        dx = np.array(upstreamGradient, copy=True)\n",
    "        dx[x <= 0] = 0\n",
    "        return(dx)\n",
    "    \n",
    "\n",
    "\n",
    "    #Write the functions for our loss functions.\n",
    "    #For now we'll stick with SVM\n",
    "    def svmLoss(self, y, estimatedScores, e):\n",
    "        N = estimatedScores.shape[0]\n",
    "        correctClassScore = estimatedScores[np.arange(N), y]\n",
    "        margin = np.maximum(0, estimatedScores-correctClassScore[:,np.newaxis] + e)\n",
    "        margin[np.arange(N), y] = 0\n",
    "        loss = np.sum(margin)\n",
    "        positiveCount = np.sum(margin>0, axis=1)\n",
    "        dx = np.zeros_like(estimatedScores)\n",
    "        dx[margin > 0] = 1\n",
    "        dx[np.arange(N), y] -= positiveCount\n",
    "        dx /= N\n",
    "        return loss, dx\n",
    "\n",
    "    #We're going to shift from \"maxIterations\" over to \"Epochs!\"\n",
    "    #One epoch is equal to the number of iterations it takes to complete\n",
    "    #enough iterations to cover all of your data, given some number N of observations,\n",
    "    #and a batchSize.  So, if you have 3200 observations and a batch size of 32, you'll\n",
    "    #run 1000 iterations per epoch.\n",
    "    def fit(self, epochs, learningRate, batchSize, visualization=True, lossType = \"svmMulticlass\",lossParams = {\"epsilon\": 1}):\n",
    "        self.startTime = time.time()\n",
    "        self.plotData = {}\n",
    "        self.plotData['iterationLoss'] = []\n",
    "        self.plotData['correctlyClassifiedImagesPercent'] = []\n",
    "        \n",
    "        epoch = 1\n",
    "        epochSize = math.ceil(round(len(self.X_train) / batchSize,0))\n",
    "\n",
    "        while epoch <= epochs:\n",
    "            print(\"START: Epoch \" + str(epoch) + \" of \" + str(epochs))\n",
    "\n",
    "            maxIterations = epochSize\n",
    "            currentIteration = 0\n",
    "            while currentIteration < maxIterations:\n",
    "                if(currentIteration % 10):\n",
    "                    print(str(currentIteration) + \" of \" + str(epochSize) + \" (Epoch \" + str(epoch) + \" of \" + str(epochs) + \")\")\n",
    "\n",
    "                randomSelection = np.random.randint(len(self.X_train), size=batchSize)\n",
    "                xBatch = self.X_train[randomSelection,:,:,:]\n",
    "                yBatch = self.y_train[randomSelection]\n",
    "\n",
    "                #Forward pass through each layer type.\n",
    "                layerCounter = 1 \n",
    "                layerOutputs = {}\n",
    "                incomingX = xBatch\n",
    "                for i in modelArchitecture:\n",
    "                    if(i['layerType'] == 'convolutional'):\n",
    "                        layerOutputs[\"L_out_\" + str(layerCounter) + i['layerType']] = self.convolutionalForward(X = incomingX, \n",
    "                                                  W = self.weights[\"W\" + i['layerType']  + str(layerCounter)], \n",
    "                                                  B = self.weights[\"B\" + i['layerType']  + str(layerCounter)])\n",
    "                        incomingX = layerOutputs[\"L_out_\" + str(layerCounter) + i['layerType']][0]\n",
    "                    \n",
    "                    if(i['layerType'] == 'affine'):\n",
    "                        layerOutputs[\"L_out_\" + str(layerCounter) + i['layerType']] = self.affineForward(X = incomingX, \n",
    "                                                  W = self.weights[\"W\" + i['layerType']  + str(layerCounter)], \n",
    "                                                  B = self.weights[\"B\" + i['layerType']  + str(layerCounter)])\n",
    "                        incomingX = layerOutputs[\"L_out_\" + str(layerCounter) + i['layerType']][0]\n",
    "\n",
    "                        \n",
    "                    \n",
    "                    if(i['layerType'] == \"relu\"):\n",
    "                        layerOutputs[\"L_out_\" + str(layerCounter) + i['layerType']] = self.reluForward(X = incomingX)\n",
    "                        incomingX = layerOutputs[\"L_out_\" + str(layerCounter) + i['layerType']][0]\n",
    "\n",
    "                    layerCounter = layerCounter + 1\n",
    "                \n",
    "\n",
    "                #Loss function for the final set of scores\n",
    "                estimatedScores = incomingX\n",
    "                loss, gradient = self.svmLoss(yBatch, estimatedScores, e=1.0)\n",
    "\n",
    "                #And, our backward pass!\n",
    "                upstreamGradient = gradient\n",
    "                weightGradients = {}\n",
    "                for i in reversed(modelArchitecture):\n",
    "                    layerCounter = layerCounter - 1\n",
    "                    if(i['layerType'] == 'convolutional'):\n",
    "                        layerOutputs[\"L_grad_\" + str(layerCounter) + i['layerType']] = self.convolutionalBackward(upstreamGradient = upstreamGradient,\n",
    "                                                                                       cache = layerOutputs[\"L_out_\" + str(layerCounter) + i['layerType']][1])\n",
    "                        weightGradients[\"W_\" + i['layerType'] + str(layerCounter)] = layerOutputs[\"L_grad_\" + str(layerCounter) + i['layerType']][1]\n",
    "                        weightGradients[\"B_\" + i['layerType'] + str(layerCounter)] = layerOutputs[\"L_grad_\" + str(layerCounter) + i['layerType']][2]\n",
    "                        upstreamGradient = layerOutputs[\"L_grad_\" + str(layerCounter) + i['layerType']][0]\n",
    "                        \n",
    "\n",
    "                    if(i['layerType'] == 'affine'):\n",
    "                        #Inputs: dupstream, cache - three returns (dx, dw, db)\n",
    "                        layerOutputs[\"L_grad_\" + str(layerCounter) + i['layerType']] = self.affineBackward(upstreamGradient = upstreamGradient,\n",
    "                                                                                       cache = layerOutputs[\"L_out_\" + str(layerCounter) + i['layerType']][1])\n",
    "                        #Here, we save our three gradients - the X gradient gets passed to the next layer, and both W and B are saved for us in updating.\n",
    "                        upstreamGradient = layerOutputs[\"L_grad_\" + str(layerCounter) + i['layerType']][0]\n",
    "                        weightGradients[\"W_\" + i['layerType'] + str(layerCounter)] = layerOutputs[\"L_grad_\" + str(layerCounter) + i['layerType']][1]\n",
    "                        weightGradients[\"B_\" + i['layerType'] + str(layerCounter)] = layerOutputs[\"L_grad_\" + str(layerCounter) + i['layerType']][2]\n",
    "\n",
    "                    if(i['layerType'] == 'relu'):\n",
    "                        layerOutputs[\"L_grad_\" + str(layerCounter) + i['layerType']] = self.reluBackward(upstreamGradient = upstreamGradient,\n",
    "                                                                                       cache = layerOutputs[\"L_out_\" + str(layerCounter) + i['layerType']][1])\n",
    "                        upstreamGradient = layerOutputs[\"L_grad_\" + str(layerCounter) + i['layerType']][0] \n",
    "                    \n",
    "\n",
    "\n",
    "                currentIteration = currentIteration + 1\n",
    "            epoch = epoch + 1\n",
    "        \n",
    "        self.fitDoneTime = time.time()\n",
    "\n",
    "    #Some minor updates here so that we can use this after run:\n",
    "    def plotFit(self, figsize=(7,5), title=''):\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=figsize)\n",
    "        i = 0\n",
    "        colors = ['red', 'deepskyblue', 'orange', 'green']\n",
    "        loc = ['upper left', 'upper right', 'lower left', 'lower right']\n",
    "        for label,data in self.plotData.items():\n",
    "            plt.plot(data, label=label, color=colors[i])\n",
    "            plt.ylabel(label)\n",
    "            plt.legend(loc = loc[i])\n",
    "            if(i == 0):\n",
    "                plt.twinx()\n",
    "            i = i + 1\n",
    "        plt.title(title)\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    #A short prediction function - this is very similar to our forward pass\n",
    "    def predict(self, X):\n",
    "        #This is identical to the forward pass:\n",
    "        hiddenLayerValues = np.dot(X, self.params['W1'])\n",
    "        scores = hiddenLayerValues.dot(self.params['W2'])\n",
    "        y_pred = np.argmax(scores, axis=1)\n",
    "        return(y_pred)\n",
    "\n",
    "    #Finally, we have our modelValidation.  Here, we use our test data to see how well\n",
    "    #our final model performed.\n",
    "    def modelValidation(self):\n",
    "        percentCorrect = np.mean(np.equal(self.y_test,self.predict(self.X_test)))\n",
    "        print(\"Best Training Dataset Accuracy: \" + str(round(max(self.plotData['correctlyClassifiedImagesPercent']),2)*100) +\"%\")\n",
    "        print(\"Final Iteration Accuracy: \" + str(round(self.plotData['correctlyClassifiedImagesPercent'][-1],2)*100) +\"%\")\n",
    "        print(\"Test Dataset Accuracy: \" + str(round(percentCorrect*100,2)) + \" %\")\n",
    "        print(\"Total Runtime: \" + str(round(self.fitDoneTime - self.startTime,2)) + \" seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Just for illustration, here is a more complex network\n",
    "#with more hidden nodes.  You'll note a large increase\n",
    "#in computation time, because the first layer now has\n",
    "#3072 * 1000 weights!\n",
    "#On my computer, this configuration gives me:\n",
    "#Best Training Dataset Accuracy: 38.0%\n",
    "#Final Iteration Accuracy: 27.0%\n",
    "#Test Dataset Accuracy: 27.92 %\n",
    "#Total Runtime: 31.2 seconds\n",
    "\n",
    "myConvNet = convolutionalNeuralNet(modelArchitecture = modelArchitecture,\n",
    "                  X_train=X_train, \n",
    "                  y_train=y_train,\n",
    "                  X_test=X_test,\n",
    "                  y_test=y_test,\n",
    "                  X_val = X_val,\n",
    "                  y_val = y_val,\n",
    "                  weightType = \"He\")\n",
    "\n",
    "myConvNet.fit(epochs = 2,\n",
    "              learningRate=.001, \n",
    "              batchSize=32, \n",
    "              visualization=True, \n",
    "              lossType = \"svmMulticlass\",\n",
    "              lossParams = {\"epsilon\": 1.0})\n"
   ]
  }
 ]
}