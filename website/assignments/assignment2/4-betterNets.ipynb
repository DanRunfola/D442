{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You've now built your first neural network from scratch, but\n",
    "#the code we used was not very modular - i.e.,\n",
    "#it would be hard to add more layers,\n",
    "#swap in different activation functions,\n",
    "#and more.  So, in this notebook we're going\n",
    "#to build a more robust neural network framework\n",
    "#that we can more practically use\n",
    "#going forward.  This will become absolutely\n",
    "#necessary when we move into convolutional spaces.\n",
    "\n",
    "#You'll also get experience with different types of activation and loss\n",
    "#functions throughout this excersize.\n",
    "\n",
    "#Setup pieces:\n",
    "import pickle\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "import copy\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "with open(\"testTrainLab2.pickle\", \"rb\") as f:\n",
    "    labData = pickle.load(f)\n",
    "\n",
    "y_train = labData[\"y_train\"]\n",
    "y_test = labData[\"y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 288x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.919844pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.919844\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-20T13:30:45.319505</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.919844 \nL 251.565 248.919844 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 225.041719 \nL 244.365 225.041719 \nL 244.365 7.601719 \nL 26.925 7.601719 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p3832ed1144)\">\n    <image height=\"218\" id=\"image89571079de\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-7.041719\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mac4ed0a78c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.3225\" xlink:href=\"#mac4ed0a78c\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(27.14125 239.640156)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"64.2975\" xlink:href=\"#mac4ed0a78c\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(61.11625 239.640156)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"98.2725\" xlink:href=\"#mac4ed0a78c\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(91.91 239.640156)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"132.2475\" xlink:href=\"#mac4ed0a78c\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(125.885 239.640156)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"166.2225\" xlink:href=\"#mac4ed0a78c\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(159.86 239.640156)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"200.1975\" xlink:href=\"#mac4ed0a78c\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(193.835 239.640156)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"234.1725\" xlink:href=\"#mac4ed0a78c\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <g transform=\"translate(227.81 239.640156)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m6b96634c01\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m6b96634c01\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m6b96634c01\" y=\"44.974219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 48.773437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m6b96634c01\" y=\"78.949219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 82.748437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m6b96634c01\" y=\"112.924219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 116.723437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m6b96634c01\" y=\"146.899219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 150.698437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m6b96634c01\" y=\"180.874219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 184.673437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m6b96634c01\" y=\"214.849219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 30 -->\n      <g transform=\"translate(7.2 218.648437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 225.041719 \nL 26.925 7.601719 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 225.041719 \nL 244.365 7.601719 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 225.041719 \nL 244.365 225.041719 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.601719 \nL 244.365 7.601719 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p3832ed1144\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.601719\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "#The first thing we need to do is implement a function to handle our data preprocessing.\n",
    "#For now, we'll just do two things - we'll move our reshaping of the data into an array\n",
    "#here (it was in the data loading in the last assignment), and we'll\n",
    "#add a zero shift.   I also added an option to visualize the mean image for this\n",
    "#cell's example.  Note the mean image is only based on our training data -\n",
    "#this is reflective of a \"real world\" case where we wouldn't have access to \n",
    "#the testing data at training time.\n",
    "def preProcessing(train, test, arrayReshape=True, zeroShift=True, zeroShiftVis = True):\n",
    "    if(zeroShift == True):\n",
    "        #First, we're going to calculate the overall mean image across our training dataset.\n",
    "        mean_image = np.average(train, axis=0)\n",
    "        if(zeroShiftVis == True):\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) \n",
    "            plt.show()\n",
    "        #And - we subtract!  That's all there is to this.\n",
    "        train -= mean_image\n",
    "        test -= mean_image\n",
    "\n",
    "    if(arrayReshape == True):\n",
    "        #Here, we're reshaping CIFAR-10 from 32x32 to a 1x3024 array.\n",
    "        #We are moving this into preprocessing, as once we get to \n",
    "        #convolutional nets, we won't want to do this anymore.\n",
    "        train = np.reshape(train, (train.shape[0], -1))\n",
    "        test = np.reshape(test, (test.shape[0], -1))\n",
    "\n",
    "    return(train, test)\n",
    "\n",
    "#Note the .copy() where we call the below funciton.\n",
    "#You'll start seeing this more in the code - this instructs\n",
    "#python to make a physical copy of labData in memory.\n",
    "#If you don't do this, labData is treated as equivalent to train and\n",
    "#test in memory, and thus will update when you run the process.\n",
    "#This is referred to as a \"Deep copy\".  To understand this process,\n",
    "#I recommend you temporarily remove the .copy() and look at how the\n",
    "#image output changes if you run this cell more than once.\n",
    "X_train, X_test = preProcessing(train = labData[\"X_train\"].copy(), \n",
    "                                test = labData[\"X_test\"].copy(),\n",
    "                                arrayReshape = True,\n",
    "                                zeroShift = True,\n",
    "                                zeroShiftVis = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[  -4.90591774   12.24254123   -1.6801487    16.65395799  -57.73796315\n    11.19363272  -30.54325011  -20.69502448 -120.25470599  -27.61058489\n    38.0115244   -73.73874205    6.23220141  -63.82063941 -125.01459386\n   -69.88607818   59.14714874   92.63714335  -36.2328846    13.11797575\n   -35.57740618   47.49684758   86.10986988  -28.93507006  121.88707987\n   183.47470905    9.34691226  -28.96915763   44.1010474    96.88605415\n    18.93375649 -134.69392698  -74.19241144   35.6025262    70.74189396\n    57.76901144  -83.59821228   98.21344863  -84.6395241   -79.77912473\n   -59.11296698 -103.59641924   -8.05710545   28.1401273  -149.84013545\n   163.81703822  -11.96327602  -60.96993982   66.87114649  -49.03041578]\n [ 103.64640869   43.1338977   -37.74776336 -138.37651605  -89.00555333\n    -9.09212736  142.61964688   22.63291832   98.01539483   86.15028992\n    56.93926936  116.53151741   36.6485442   159.9396455    86.2371603\n   -50.82635348  -36.38951013   24.37372695  100.31490117   86.16874909\n    -6.1456667   -28.76959216  241.09899738 -131.80776309   33.85164015\n    30.75444257   26.34020245  -98.10778491  -83.99363697   52.77106532\n   160.1529328   -79.48421072   35.0623926    59.69904313  -31.11067186\n    -5.0759486  -113.98826034   82.15037383   53.03033752   41.93439096\n  -133.17210742    6.07661624   31.74916982   15.5621583   -82.05732938\n   -52.71800223   28.53123306 -128.40614494   -3.18197332  172.41543364]]\n"
     ]
    }
   ],
   "source": [
    "#Next, we need to define a layer of our network.\n",
    "#To date in lecture - and in this assignment - \n",
    "#we've been dealing with fully connected layers.\n",
    "#That is, if you have an input of CIFAR 10 (3072 pixels),\n",
    "#and 50 hidden nodes, the total number of weights is equal to\n",
    "#3072 * 50 (153,600) - i.e., every single input is connected\n",
    "#to every single hidden node by a weight.  Outputs (i.e., what goes \n",
    "#into the hidden nodes) are defined by the dot product between the inputs \n",
    "#(CIFAR-10 pixels) and the weights.\n",
    "#This is also referred to as an \"Affine\" layer.\n",
    "\n",
    "#So, let's implement the logic of what happens during an affine layer\n",
    "#in the forward pass procedure.\n",
    "#Note that for the first time we'll be including a bias term - b.\n",
    "#In practice, fully connected (or affine) layers are defined by \n",
    "#dot(x, w) + b, where b is a constant bias term.  This bias term\n",
    "#practically allows your network to have more flexibility in\n",
    "#'biasing' towards certain solutions, which can be helpful in\n",
    "#many cases.  We won't go into too much detail on this, but you\n",
    "#can think back to your elementary statistics course.  When we\n",
    "#fit a line, it's generally of the form y = mx + b, where b represents\n",
    "#the constant / y - intercept.  The same concept is applied here.\n",
    "\n",
    "def affineForward(X, W, B):\n",
    "    #Total number of observations:\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    #Number of dimensions - in this example, 3072 (i.e., each observation has 3072 values)\n",
    "    D = np.prod(X.shape[1:])\n",
    "    \n",
    "    #Reshape our inputs to be (N,D), matching our expectation for the weights dot product.\n",
    "    xReshape = np.reshape(X, (N, D))\n",
    "    \n",
    "    #Calculate the dot product:\n",
    "    out = np.dot(xReshape, W) + B\n",
    "\n",
    "    #Save a cache for use later in the backprop:\n",
    "    cache = (X, W, B)\n",
    "\n",
    "    return(out, cache)\n",
    "\n",
    "\n",
    "#We can run our CIFAR-10 data through this forward pass - here's a quick example\n",
    "#with the first two.\n",
    "#I encourage you to check out the shapes of W, B, and X to better understand what's going on.\n",
    "W = np.random.randn(3072, 50) / np.sqrt(3072/2)\n",
    "B = np.random.randn(50) / np.sqrt(50/2)\n",
    "\n",
    "exampleAffineOutA, exampleAffineCacheA = affineForward(X_train[0:2], W, B)\n",
    "\n",
    "print(exampleAffineOutA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[  0.          12.24254123   0.          16.65395799   0.\n  11.19363272   0.           0.           0.           0.\n  38.0115244    0.           6.23220141   0.           0.\n   0.          59.14714874  92.63714335   0.          13.11797575\n   0.          47.49684758  86.10986988   0.         121.88707987\n 183.47470905   9.34691226   0.          44.1010474   96.88605415\n  18.93375649   0.           0.          35.6025262   70.74189396\n  57.76901144   0.          98.21344863   0.           0.\n   0.           0.           0.          28.1401273    0.\n 163.81703822   0.           0.          66.87114649   0.        ]\n"
     ]
    }
   ],
   "source": [
    "#Following the example in lecture, the next stage is to pass our outputs \n",
    "#Through our activations.  In this case, we'll be using ReLU.\n",
    "def reluForward(reluInput):\n",
    "    out = np.maximum(reluInput, 0)\n",
    "    cache = reluInput\n",
    "    return(out, cache)\n",
    "\n",
    "exampleReluOut, exampleReluCache = reluForward(exampleAffineOutA)\n",
    "print(exampleReluOut[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[   3.43633513   14.04668519  -59.43988992   80.10756541 -103.88391532\n   -38.97405769  -77.69850393   29.48006239   18.22580956   13.73787108]\n [  28.34679977 -164.51984761   19.01618792  294.85322925  -69.38273182\n    14.6497417   -60.5064468   100.37577717   46.88305262   59.60640603]]\n"
     ]
    }
   ],
   "source": [
    "#To follow the lecture 10 process, we'll apply the affine net one more time, to replicate\n",
    "# a two layer net.\n",
    "\n",
    "#Now, our weights are 50 x 10 - i.e., we have 50 input hidden nodes, and 10 final score nodes.\n",
    "#The bias matrix also shrinks down to 10 (from 50) - one bias for each output.\n",
    "W2 = np.random.randn(50, 10) / np.sqrt(50/2)\n",
    "B2 = np.random.randn(10) / np.sqrt(10/2)\n",
    "\n",
    "exampleAffineOutB, exampleAffineCacheB = affineForward(exampleReluOut, W2, B2)\n",
    "\n",
    "#These are the final scores for the forward pass - 10 for each row.\n",
    "print(exampleAffineOutB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "968.2246069391364\n[[ 0.5  0.5  0.5  0.5  0.   0.5 -4.   0.5  0.5  0.5]\n [ 0.   0.   0.   0.5  0.   0.   0.   0.5  0.  -1. ]]\n"
     ]
    }
   ],
   "source": [
    "#Now we need to pass the 10 scores we solve for to our loss function.\n",
    "#In this function we'll calculate both our loss as well as the \n",
    "#gradient with respect to our incoming values.\n",
    "\n",
    "def svmLoss(y, estimatedScores, e):\n",
    "    N = estimatedScores.shape[0]\n",
    "\n",
    "    #This takes the estimated score for the correct class, y.\n",
    "    #correctClassScore will have one entry per observation.\n",
    "    correctClassScore = estimatedScores[np.arange(N), y]\n",
    "\n",
    "    #Now we calculate SVM loss, adding a new dimension to correctClassScore.\n",
    "    margin = np.maximum(0, estimatedScores-correctClassScore[:,np.newaxis] + e)\n",
    "\n",
    "    #Set our correct cases to 0 as per the SVM Loss function:\n",
    "    margin[np.arange(N), y] = 0\n",
    "\n",
    "    #Calculate the total loss\n",
    "    loss = np.sum(margin)\n",
    "\n",
    "    #Now we want to solve for our gradients.\n",
    "    #Because SVM loss only changes if the value is greater than 0,\n",
    "    #first we need to identify those cases.\n",
    "    positiveCount = np.sum(margin>0, axis=1)\n",
    "\n",
    "    #Now let's solve for dx - first create an empty matrix\n",
    "    #the same size as our inputs (estimatedScores).\n",
    "    dx = np.zeros_like(estimatedScores)\n",
    "\n",
    "    #Identify each case with a postiive value\n",
    "    dx[margin > 0] = 1\n",
    "\n",
    "    #Because the true cases result in a negative change, we subtract\n",
    "    #the total positive cases from the y entries:\n",
    "    dx[np.arange(N), y] -= positiveCount\n",
    "    \n",
    "    #And, finally, divide by our sample size\n",
    "    dx /= N\n",
    "\n",
    "    return loss, dx\n",
    "\n",
    "loss, gradient = svmLoss(y = y_train[0:2], estimatedScores = exampleAffineOutB, e = 1.0)\n",
    "print(loss)\n",
    "print(gradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.11282879  0.14643656  0.79115998  0.21459815  0.43959615  0.66586288\n  0.09406074 -0.27023692 -0.47597321 -0.72509195  0.09182375  0.5871272\n  0.76478279  0.18463825  0.30961783 -0.64321603 -0.48181167 -0.0538147\n  0.98957133  1.78042645  0.53548426 -0.16145957  0.20065888  0.14116045\n  0.45572161  0.62687847  0.20669583 -1.88647338  0.64409995  0.30779447\n  0.23775306 -1.33515814  0.67431192 -0.3837569  -0.60862883  0.04450386\n -1.1952434   0.4647366   0.25419886  0.6580817   1.35424019  0.38969897\n -0.54658089  1.37577244  0.65787111  0.14492441 -1.26580148  0.08871381\n  0.46599225 -0.31207321]\n[   0.            0.            0.           51.82320435    0.\n    0.            0.           51.82320435    0.         -103.64640869]\n0.5\n"
     ]
    }
   ],
   "source": [
    "#Now we're ready for a backwards pass.  \n",
    "def affineBackward(dUpstream, cache):\n",
    "    X, W, B = cache\n",
    "    \n",
    "    #Same steps as the forward pass:\n",
    "    N = X.shape[0]\n",
    "    D = np.prod(X.shape[1:])\n",
    "    xReshape = np.reshape(X, (N, D))\n",
    "    \n",
    "    #Gradient calculations for the affine case - nothing you haven't\n",
    "    #seen before!\n",
    "    dx = np.reshape(np.dot(dUpstream, W.T), X.shape) \n",
    "    dw = np.dot(xReshape.T, dUpstream) \n",
    "    db = np.dot(dUpstream.T, np.ones(N)) \n",
    "\n",
    "    return(dx, dw, db)\n",
    "\n",
    "#Here, dx is the gradient for h (our inputs into this affine);\n",
    "#dw is the gradient for W,\n",
    "#db is the gradient for the bias term in the affine.\n",
    "#The \"b\" prefix is just to indicate this is the set of gradients\n",
    "#from the second affine in our network.\n",
    "b_dx, b_dw, b_db = affineBackward(gradient, exampleAffineCacheB)\n",
    "\n",
    "#Note that if you initialize weights randomly - instead of with He\n",
    "#you'll see the gradient decay and saturation in action on these gradients.\n",
    "#Most of the time, dw[0] will be equal to all 0s!\n",
    "print(dx[0])\n",
    "print(dw[0])\n",
    "print(db[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 50)\n(50, 10)\n"
     ]
    }
   ],
   "source": [
    "#Alright, now we head backwards through the ReLu!\n",
    "def reluBackward(upstreamGradient, cache):\n",
    "    x = cache\n",
    "\n",
    "    #Remember this gradient is just copying our incoming,\n",
    "    #and then setting anything less than 0 to 0!\n",
    "    dx = np.array(upstreamGradient, copy=True)\n",
    "    dx[x <= 0] = 0\n",
    "\n",
    "    return(dx)\n",
    "\n",
    "reluBackGradient = reluBackward(b_dx, exampleReluCache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[  5.77846628  -5.52427057   0.         ...   0.         -33.41664935\n   -3.19140043]\n [ 10.15887642  -2.09501945   0.         ...   0.         -34.50958758\n   -5.6106657 ]\n [ 13.50903938   1.44967308   0.         ...   0.         -32.41153202\n   -7.46093375]\n ...\n [  4.05942587   2.96335586   0.         ...   0.          -1.69578309\n   -2.24198824]\n [  1.77396364  -3.42906474   0.         ...   0.         -15.77397759\n   -0.97974584]\n [  7.34877034   0.12300096   0.         ...   0.         -19.74961846\n   -4.05866673]]\n[ 0.24811721  0.36013176  0.          0.21459815  0.          0.66586288\n -0.09677854 -0.35756535  0.18493582  0.44219147  0.04203756  0.09298704\n  1.25442603  0.27280887 -0.12553162  0.         -0.48181167 -0.17235108\n  0.21609541  1.44494277  0.         -0.16145957  0.29501763  0.\n  0.5242007   0.45900123  0.81776416  0.          0.64409995  0.09040913\n  0.28484575  0.         -0.07304672 -0.17699938 -0.60862883  0.04450386\n  0.          0.37444815  0.12780198 -0.18270497  0.          0.25328004\n  0.11495374  1.56746088  0.          0.14492441  0.08596118  0.\n  0.46599225 -0.13703314]\n"
     ]
    }
   ],
   "source": [
    "#And through the final affine...\n",
    "a_dx, a_dw, a_db = affineBackward(reluBackGradient, exampleAffineCacheA)\n",
    "\n",
    "print(a_dw)\n",
    "print(a_db)\n",
    "\n",
    "#And, we're done!  We now have everything we would need to update our weights:\n",
    "#W in the first layer is updated by a_dw\n",
    "#Our bias in the first layer is updated by a_db\n",
    "#Our W in the second layer is updated by b_dw\n",
    "#Our bias in the second layer is updated by b_db\n",
    "#Once we update, we can repeat the entire process -\n",
    "#But, now it's very easy to swap pieces of that process in and out.\n",
    "#In the next section, we're going to put all of these pieces\n",
    "#together."
   ]
  }
 ]
}