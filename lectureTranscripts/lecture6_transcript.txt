#SLIDE
Welcome back to DATA 442 - today we're going to be digging into neural networks for the first time in this course, which will form the baseline for just about everything we do from here.  I've been waiting for today all semester, so get excited!

#SLIDE
Before we get started, I wanted to do a very brief recap of last lecture.  We focused on optimization, which is how we update weights W iteratively to find the best solutions we can.  

#SLIDE
The primary method we discussed was gradient descent, which seeks to find the gradient of our loss function at any point defined by our weights W, and then use that gradient to estimate what a better set of Ws might be.  

#SLIDE
As you'll see on your lab, we use an analytic gradient in practice to solve for this, as it is a fairly computationally effecient approach to estimating gradients with large sets of W and data.  Specifically, we rely on an implementation called stochastic gradient descent, which makes these estimates based on a subset of our data (i.e., batches).  At the end of all this, we have a method to identify an "optimal" set of weights, W, that give us the lowest loss function we can identify.  By including regularization in this equation, we also seek to find the W that is "simplest" and thus, we hope most generalizable.

#SLIDE
Ok!  Now for the fun stuff. You've all seen a figure that looks something like this, which represents a neural network.  This type of figure is called a computational graph - essentially, this is a flexible way to describe any arbitrary function.

#SLIDE
Take, for example, a linear classifier with a SVM loss function.  We can easily represent this through a computational graph just like we would any neural network.  Because the linear classifiers are much easier to understand (as they have fewer steps), we'll walk through this first.  Remember what we're trying to accomplish with these functions, which we introduced back in lecture 3.  The linear function on the left is a function that takes in a set of images, represented by X, and a set of Weights, represented by W.  It then multiplies them together, and uses the resultant scores to assign a class to each image.  We then feed these estimated scores into the loss function on the right to derive our measure of "badness", where higher loss values are worse.

#SLIDE
So, let's start picking these equations apart and transforming them into a computational graph.  First, we have our inputs into the function - weights W and images X.  We'll represent them on the graph here.

#SLIDE
Both our X and W go into the linear function, and our scores are output (one for each of the classes we're classifying images across).  

#SLIDE
These scores then go into the hinge loss function, which calculates our data loss.

#SLIDE
In parallel, we also pass the weights to our regularization function.  In this example we're using L2 regularization, but this function could be anything.  

#Slide 
We then add our data loss and regularization loss together to get our total loss.  This figure now represents the computational graph for our linear model with a SVM multiclass loss and L2 Regularization.  The big advantage to expressing our functions like this is that it allows us to use backpropogation to compute the gradient while taking into account every computation represented in the graph.  This is really important when we get to more complex functions!

#Slide
Ok - so, what is this backpropogation?  Let's start out with an even simpler example function which takes 3 inputs - x, y and z.  It then does the computation of adding x and y together, and multiplying the sum by z. 

#Slide
Just like our linear svm, this function can also be represented with a computational graph that looks like this.  Imagine we are trying to find the gradient - i.e., when x changes by one unit, what is the expected change in the output?  

#SLIDE
Note that in this computational graph, there is one intermediate product - we have to add before we multiply.  So we can solve for the gradient, let's call this step Q.  The function for Q is simply Q = x + y. 

#SLIDE
Remember back to our discussion on optimization and gradients.  In this case, we have three questions we need to answer: what is the change in the function if we shift X by one, y by one, or z by one?  We can start by writing out the gradients for Q with respect to x and y.  First, we know that a shift of one in either x or y would result in a change of 1 in q - because all we're doing is adding in this case.  So, in both cases the gradients are 1.

#SLIDE
Now let's call our multiplication node F.  This equation is similarly simple, F = q * z.

#SLIDE
The second set of gradients can be understood as trying to identify the change in our function when either Q or z change - in this case, because it is multiplication, a one unit change in Q would result in a change in the function of z (i.e., imagine z was 2 - if you increase q by one, you would get 2 more!).  The same is true of if you change z, with regard to Q.

#SLIDE
Ultimately, what we want to find is the gradients of F with respect to x, y and z.

#SLIDE
To do this we are going to use backpropogation.  We're going to start out the end of our graph (that is, the output), and work our way backwards, computing each gradient as we go.  The first stop on this road is the gradient of the output given the final variable - in this case, we only have one, which is F.  

#SLIDE
So, this reduces down to 1, because if you changed F by one, the output would also change by 1; this first one is nice and easy.

#SLIDE
Now we're going to follow our function backwards.  The next step we can look at is the gradient of F given the input z. We already know that this one is equal to Q - i.e., if you increase z by one, the total function output increases by Q.

#SLIDE
Similarly, we know that the gradient of F with respect to Q is z.

#SLIDE
Now we get to the fun part - solving for the gradient of F given x and y.  Let's start with y.  In this case, we're trying to find dF over dy, but y is not directly connected to F in our computational graph.  So, we're going to apply a chain rule.  Because we know the computations that connect y to F, we can "chain" the gradients together - i.e., in this case, the change in output F given a one unit change in y would be equal to the gradient of q multiplied by the gradient of y (or dF/dq times dQ/dy). To give some intuition - finding the effect of y on F requires first finding the effect of y on Q, and then the effect of Q on F.  Essentially, we are trying to identify the portion of a change in our function output F that can be attributed to a change in y with this chain.

#SLIDE
X is essentially the same as Y - we would use the same exact chain rule, but replacing y with x. 

#SLIDE
So, let's briefly reflect on our goal - we want to know the shift in F given a change in x, y and z.  We now have equations to do each of these things - so let's walk through the solution of dF / dx as an example.  Remember from a few slides ago that we noted dF / dQ resolves to z (as increasing Q by one increases the output of the function by z).  So, the equation reduces to ...

#SLIDE
Further, we know that dQ / dx is equal to one - that is, if X increases by one, so does x.  This leads to a further reduction of our equation to...

#SLIDE
z * 1, or z.  So - we now know that the gradient of F with regard to x is equal to z.  This makes intuitive sense in this simple case - if you increase x by one, the would increase Q by one.  Because we multiple Q by z, the resultant output is going to increase by z!  This is the chain rule at play.

#SLIDE
So, let's think about this exact same function in terms of how most backpropogation is implemented algorithmically.  The first thing to note is that here we're using very simple computations - addition and multiplication.  This is because it is very easy for us to solve for the gradients of these simple computations - i.e., we know a one-unit increase in y will always result in a one-unit increase in Q, simply because the computation is addition.  Similarly, you know a one unit increase in z results in an increase of Q in the final output.  Because we keep the equations in these computational nodes very, very simple, it allows us to apply backpropogation techniques (and the chain rule) across very deep nets.

#SLIDE
To illustrate this, consider an even simpler computational graph, with only one computation - addition.  In this graph, we don't need any additional information to calculate a few things.  First, we can calculate the local gradients within the computation - i.e., the change in our output F with respect to x, and the same for y.  We know both of these are 1 in this case - i.e., if this is a computational addition function, the gradients will always be 1.

#SLIDE
In backpropogation, we have all of our upstream gradients being passed backwards to this set of local gradients.  So, at any given node, we would also know the gradient of downstream nodes based on a change in the output F.  I.e., some function L (let's assume it's a loss function) changes by some amount when our output F changes.  This is denoted by dL / dF.  

#SLIDE
So, given that we will know dL over dF, we now want to compute the next gradient backwards, which would be the change in our loss function L when x changes - i.e., dL / dx.  

#SLIDE
You'll remember from the earlier example that we can use the chain rule to solve for this - i.e., dL / dx is equal to dl/dF times dF/dx.  


#NOTES
\frac{\partial q}{\partial x} = 1
\frac{\partial f}{\partial q} = z