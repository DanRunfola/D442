Now on to Variational Autoencoders (VAE)! PixelCNNs defined a density function we could solve for using maximum likelihood.  With variational autoencoders, we're going to use an intractable density function with an additional latent variable z (more on that in a second), and so now our data likelihood is this integral - the expectation over all possible values of z.  This is, obviously, a problem - we can't optimize this directly, so instead we derive and optimize a lower bound on the likelihood. 
#SLIDE
So, one step back. Let's talk a little about what an encoder is in the first place.  Essentially, the idea of an encoder is some function that takes in your data, X, does ... something ... to it, and then extracts features z.  
#SLIDE
The encoder itself can take a wide range of forms - the one you'll be most familiar with in this course is a convolutional neural network.  I.e., if you do a forward pass through a CNN, you could output (for example) one maxpool value per filter.  That would be one type of encoder, where you're extracting features from some input.  Z is almost always smaller than X, with the goal of summarizing the most relevant features in our input.
#SLIDE
The way an autoencoder is designed is with the goal of creating some smaller set of features (z) that can be fed into a second algorithm - a decoder - that can recreate the original input.  If it is possible to recreate the original input from some reduced-dimension set of Z features, then the logic is that the representation Z is effective.
#SLIDE
So, in a VAE we would do something like this - pass our X estimates into a loss function, compare how similar they are to the true X, and then use some backpropogation to update our decoder and encoder.  Note that in this approach there are no labels - just an input X.
#SLIDE
The point of all of this is once you have a well trained encoder, you can remove the decoder and simply use the features Z as input into any model you might want - i.e., a classification model.  Essentially, we have a better input than X, as we've reduced the dimensionality without losing (hopefully) any meaningful information.
#SLIDE
So - the question insofar as Generative networks is concerned is if we can use this process to generate new samples - essentially, can we turn the decoder into a probabilistic process that allows us to sample from possible outcomes to generate new data?  Hence a "Variational" auto encoder.
#SLIDE
The first thing we're going to do is assume that all of our data (X) is generated from some unknown representation (i.e., latent) z.  Think back a slide - imagine you built an autoencoder that was able to perfectly re-create a face.  You would input the whole image of the face, with every value (X) being represented.  This would be reduced down to some number of Z values, which you then put into a decoder to re-build the face.  Those Z values would have to represent important parts of the face - i.e., hair color, hair style, etc.  So, to create a new face, we can imagine sampling different "Z" parameters to capture different types of features.
#SLIDE
Let's pick this apart a bit.  On this slide we have step 1, which is about our assumptions.  In this case, we're assuming our data is generated from some underlying process based on an input of Z; we don't actually know what this decoder is.  We're assuming it exists, but would need to solve for it.  This is the algorithm that tells us - when Z changes - how our output X might change.  The second part of this is our Z.  To solve for the decoder, we're going to assume Z is drawn from some prior and sample from it; generally, this is a gaussian.  Finally, in part 3, we assume there is some conditional probability of X being output, given a Z - this is obviously a much more complicated probability distribution, so we'll end up representing it as a neural network.  The trick is, given these assumptions, how do we solve for the correct parameters for our decoder (which we'll refer to as Theta)?
#SLIDE
Ok - so how do we actually train this network?  We want our decoder to be able to take in any Z values, and then output an X that looks reasonably like our input data.  In our PixelCNN and PixelRNN, the way we did this was by solving for the maximum likelihood of our probability function; unfortunately, the integral here is intractable as we're trying to estimate over a continious value (z).  Let's briefly walk through why this is the case.
#SLIDE
FIrst, we have p(z), which we can assume is a simple distribution, something like a gaussian prior.  Nothing to see here.
#SLIDE
The second thing is the probability of an output X, given the inputs Z.  We'll seek to represent this with our decoder neural network, so again - all good, move along.
#SLIDE
The problem is the integral - to calculate this, we would need to calculate p(x|z) for every Z, which is an intractible problem.  This means that the posterior density calculation is also intractable - i.e., we can't solve for the probability of a given Z given X.  
#SLIDE
So, what are we to do?  The solution lies in creating a second neural network.  Just like we represent the proobability of X given Z with a decoder neural net, we can also represent an estimate of the posterior density (p(z|x) using a neural net, which takes X as an input.  This allows the deriviation of a lower bound on the image function at the upper-right of this slide, that has a tractable solution.  I won't go into the details on this deriviation in this lecture, but if you're interested the article Auto-Encoding Variational Bayes in the lecture notes has full details on how this posterior inference can be made.  Importantly, once you have this encoder neural net, it allows you to optimize the parameters theta.  
#SLIDE
Going back to the purpose of the autoencoder, once theta is optimized against a given set of data, manipulating Z will allow you to create arbitrary outputs (X), representing different views of the data. One of the neat things about VAE's is that, as you change the Z inputs in different dimensions, you can see the features that a given Z was measuring.  Here's an example of two dimensions - the vertical axis represents a Z value that is changing hairl color; the X axis is representing a Z value that is controlling sunglasses.  You can see as the dimensions change, slightly different faces are generated.  You can arbitrarily sample the Z space to generate any number of images once your VAE is parameterized.  One other neat thing: unlike our PixelRNN and PixelCNN, these images are all generated at once (i.e., not pixel-by-pixel), so are multiple orders of magnitude faster to implement.