
===== SLIDE 1
Ok - let's look at a more practical example of how gradient descent optimization works.  The simplest way you might imagine calculating a gradient might be finite differences - on the top, you have an arbitrary weights vector, let's say for a linear model attempting to classify our CIFAR10 dataset.  When you run the weights through, lets say you get a hypothetical total loss of 1.25347.  Remember, our goal is to find some set of Weights that reduce this total loss.  To do that, we are going to first define our gradient vector dW, which is going to be the same length as our W.  Each element of this vector will tell us how much we could expect the total loss to change if we changed it's associated weight by some small value.

====== SLIDE 2
A finite differences approach would allow you to calculate this by changing one weight by a very small value - say, .0001.  We would then recalculate the Loss value for this new weights vector (using our loss function, classifier, etc.), in which only the first weight has been changed. 

===== SLIDE 3
In this hypothetical, let's say that the small change of .0001 in our first element in vector W resulted in a new loss, 1.25322; slightly lower than our original loss of 1.25347.  

===== SLIDE 4
So, now we can use our limit definition, and calculate a finite differences approximation of our gradient for this first dimension (i.e., our first W).  In other words, we would expect a decrease in our loss function if the first weight increased by .0001. See the previous video covering SGD conceptually if the idea of a finite difference is new to you!

===== SLIDE 5
We would then need to repeat this process for each element of vector W, resetting all other elements back to their original values, and adding .0001 to each in turn.  So, in this example we would be calculating the gradient for the second dimension; the first weight (0.34) is reset back to it's original value, and we add the .0001 to the second weight (-1.11).  We then repeat the process of calculating the loss, and use the finite differences approximation to compute the gradient for this second dimension.  

===== SLIDE 6
After repeating that for every case, we will have solved for dW.  Needless to say, this approach is generally not a very good idea.  First, it is very, very, very computationally intensive - even computing your function and related loss function for a single set of Weights W could be quite slow, especially for large sets of weights like those you would find in any deep learning approach.  Further, we aren't just talking about ~9 or 10 repeats, one for each weight.  We're talking about millions or tens-of-millions of weights in deep convolutional nets, so you would have to re-fit the function millions or tens-of-millions of times.  In some cases, we're even talking hundreds of millions!  So, finite differences simply would take too long to evaluate for gradients. 

===== SLIDE 7
 Instead, we can compute an analytic gradient - an exact, and much faster approach.  We know that the vector dw (i.e., our gradient) is actually just some function of our weights and data.  So, if we can solve for this function, we can solve for our gradient in one step (instead of one step for every weight!). 

 ===== SLIDE 8
Let's discuss how all of this math gets translated into our machine learning algorithms.  Gradient descent is not only one of the most popular approaches to training our weights vector W, but also a pretty darn effective one for many solutions.  It's also quite simple - here, you can see a basic implementation of how a gradient descent algorithm can be written.  Let's walk through this - first, we define W as a completely random set of weights - it doens't matter where these start for this example, just know it's a vector of some random weights.  We then define some number of maximum iterations - in this case, 1000, and a basic counter.  We then loop 1000 times, each time doing two things.  First we calculate the gradient dW, just like we discussed in the last few slides.  We save this gradient as W_gradient_dw.  Then, we add the inverse of this gradient to our initial weights vector; the reason for the inverse is that we want to *minimize* our total loss.  One more new concept to introduce here is the hyperparameter step size - this is sometimes referred to as a learning rate.  This hyperparameter is extremely important in how well our weights can be fit - if you set it too high, you risk skipping over the best solutions; if you set it too low, you risk getting stuck in a "valley".  We'll talk a bit more about these challenges later, but in practice getting your learning rate or step size right is a really good first step to consider in many cases.

===== SLIDE 9
Gradient Descent will form the backbone of most optimization techniques we use, but it has one big problem - if you have a very large training dataset, the analytic solution for the weights gradient can be very difficult to solve for due to the number of weights and individual pieces of data - i.e., ultimately you have to solve for the analytic gradient of every weight for every piece of data, and then take the average of those, which is quite costly.

====== SLIDE
Because of this limitation, the most common implementation of gradient descent you will likely encounter is Stochastic Gradient Descent, or SGD.  In SGD, a small batch of the data is taken at each step - i.e., in this example we would be randomly sampling 256.  By sampling our data randomly, we expect the eventual solution we find to converge towards the true solution.  While this replaces the actual gradient with an estimate of the gradient, the stochastic approximations you can find with SGD are suitable for a very wide range of applications.