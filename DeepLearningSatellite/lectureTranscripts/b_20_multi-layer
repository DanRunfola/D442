########SLIDE 1
In this very, very short video I'm going to talk a little about just why "deep" learning is, well.. deep!  Two layer neural networks allow us to capture heterogeneity in our images - a critical quality of networks when your goal is to capture the wide range of different ways objects can be presented in the real world.  By defining the interim score outputs - our hidden layers h - we enable the neural network to create different templates - and then decide which of those templates to assign to each class.  In this example, we're creating 50 different templates - our 50 interim outputs in the hidden layer - and then ascribing those to 10 different possible outputs (i.e., the CIFAR-10 classes). Weights beta define the 50 "intermediary" classes - so, you might have one of those 50 representing horses that face left, one representing horses that face right, and one for horses on white backgrounds.  Weights alpha then define which of those belong with each class.  There is a lot of complexity in this - i.e., multiple templates h may feed into multiple scores s in cases of similarity - but this basic concept of nonlinearity leads to tremendous gains in our ability to recognize images.
########SLIDE 2
We can keep going with this - as we seek to engage with more and more complex issues, we can stack layers in a neural network to arbitrary depths.  So, here, instead of just one hidden layer, we add a second one - this time of 20.  Because adding an additional layer increases the total number of parameters in our model, it is possible to capture more heterogeneity.  However - this does come with costs, which we will be discussing later in the course. If you've ever heard the term "deep learning", this is where the term comes from.