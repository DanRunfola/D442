########SLIDE
A two-layer neural network allows us to overcome this limitation.  By defining h (the middle, 'hidden' layer) as 50, we essentailly enable the neural network to create 50 different templates - and then decide which of those templates to assign to each of 10 classes.  Weights beta define the 50 "intermediary" classes - so, you might have one of those 50 representing horses that face left, one representing horses that face right, and one for horses on white backgrounds.  Weights alpha then essentially define which of those belong with each class.  There is a lot of complexity in this - i.e., multiple templates h may feed into multiple scores s in cases of similarity - but this basic concept of nonlinearity leads to tremendous gains in our ability to recognize images.
########SLIDE
We can keep going with this - as we seek to engage with more and more complex issues, we can stack layers in a neural network to arbitrary depths.  So, here, instead of just one hidden layer, we add a second one - this time of 20.  Because adding an additional layer increases the total number of parameters in our model, it is possible to capture more heterogeneity.  However - this does come with costs, which we will be discussing later in the course. If you've ever heard the term "deep learning", this is where it comes from - i.e., we could keep adding layers until our fingers get numb.
#########SLIDE
We're going to briefly switch gears a bit now and start talking about node functions in a little more detail.  You'll recall that nodes carry out some computation - in the examples we've looked at so far, those are generally either multiplication or subtraction.  However, in neural networks the node computations are most commonly of a class very loosely designed to imitate how the human mind works.  If you remember back to biology, you'll probably remember that neurons "fire" - i.e., there is something that causes the neurons in our brain to send electrical impulses to other neurons, and the culmination of all that activity is what drives our body to do stuff - eating, drinking, sleeping, breathing - it's all controlled this way.
#########SLIDE
To imitate this "firing" of neurons, we use a range of different functions within our computational nodes.  Some of these functions operate on "toggles" - i.e., if a value is above 0, it passes on a 1 ("fires"), otherwise it remains zero (i.e., the relu function illustrated here).  In other cases it may be an intensity that goes up rapidly at a certain value - i.e., a tanh function.  All of these different activation functions have different qualities, and we're going to dig into those further later. To provide one word of caution, though - neural networks are very, very coarse representations of the power of the human mind; there are a litany of factors that distinguish how the mind works and how neural networks work.  So, while the basic inspiration for the algorithms may be the elementary function of neurons, we are not yet close to being able to claim a neural network is itself functioning even similarly to human cognition.