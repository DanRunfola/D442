######SLIDE
Functionally, when we go to implement a neural network, we are reliant on two different bodies of code: an implementation of a forward and backward pass.  The forward pass is the code that takes in the input X (image) and W (weights for the image).  It then provides a set of scores - no optimization of any kind is done, and forward passes are lightning fast in most cases.  After the forward pass is completed, the backward pass can then be done to solve for the gradients of each input (i.e., weights W) with respect to the loss function you select; these gradients can then be used to update your weights.  Once you weights are updated, you repeat the process - doing a forward pass and then another backward pass - until some threshold is reached (i.e., a maximum number of iterations).
######SLIDE
In code, you can imagine a neural network being split into the same two steps - forward and backward passes.  In the forward pass, we take our input data (say, image pixels X and weights for each pixel W).  We would then start at the beginning of the graph and solve for each node - X1 * W1, X2 * W2, and the addition computation (i.e., in our forwardPass for loop, we would conduct a computation for every blue circle in this picture).
######SLIDE
In the back propogation, we flip the computational graph so that we're running the nodes in reverse order, and we're passing the loss function backwards through the function.  Again, we go node-by-node, solving the gradients for each node in a for loop.
######SLIDE
A pragmatic approach to implementing forward and backpropogation is to create independent functions (or a class, as we will do here) for each one of the blue circles - i.e., every type of computation gets a function we can pass data through in a forward or backward direction.  
######SLIDE
In this case, we have two different types of computation - multiplication and addition - so would need two different classes to handle the different types of passes.
######SLIDE
Let's consider multiplication first.  For our forward pass, the implementation of the calculation that happens in a multiplication node is trivial - we multiply the two incoming inputs together.  Remember, because this is a function, we can replace the variables "W" and "X" to be more generic, as we'll simply be passing data into this function, so we'll call these...
######SLIDE
input1 and input2.
######SLIDE
Now we need to consider the backwards function.  In this case, we have the input of the upstream gradient - i.e., the change in the loss function when the node output changes.  We expect our backward function to have two outputs - d_input1 and d_input2 - i.e., the expected change in the loss function when input1 and input2 change, respectively.  We'll get to how to solve for those in a minute.
######SLIDE
First, remember back to what we discussed about multiplication nodes in the last lecture.  In this example, we have node F, which is the final computational node in our graph.  We know that F is equal to the two inputs multiplied together - i.e., Q * z in this example.  Thus, it is very easy to intuit that a one unit increase in Q would result in a change in function F of z - because, it's Q * z!  Thus, the gradient of F with respect to Q is z.  This is what we want to implement in code.
######SLIDE
So, let's crosswalk that to our current example, just focusing on this one multiplication node for now.  In the forward pass, following our code, we are taking in X1 and W1, multiplying them, and returning the output.  Because we're multiplying we know that a change of 1 in X1 would result in a change of W1 in this output - because all we're doing is multiplying.
######SLIDE
This can be represented in our code like this in the backward pass for dInput1 - i.e., the expected change if you change dInput1 by 1 is equal to the value of input2 * the upstream gradient (represented by dOutput here).  If you have a keen programming eye, you'll see a big problem with this code as written right now - pause if you want to sleuth.
######SLIDE
For a given multiplication node, in the forward pass we need to save the two inputs - because we're going to need those inputs to solve for the gradient on the backward pass.  As a class in python, we need to save the variables in a way all functions can access; we do so by editing the forwardPass to include a temporary cache of both input1 and input2.  This allows us to reference those values in the backward pass. These types of node definitions are what make up the backbone of neural network implementations - essentially, different architectures are stringing classes that look like this together in different ways. 