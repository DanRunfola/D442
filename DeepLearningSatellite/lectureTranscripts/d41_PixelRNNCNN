First we're going to talk about PixelRNN and PixelCNN.  PixelRNNs and CNNs are a type of fully visible belief networks, which means that we are explicitly modeling our probability density.  In this case, we have our image data (i.e., a matrix of pixels), and we want to establish the probability that that image data would occur.  To do this, we decompose the likelihood into a product of 1-d distributions - i.e., for each pixel, the probability that a given pixel would be of a certain value, conditioned on the values of all other pixels. This decomposition is represented on the right-hand side of the equation on this slide now.  
#SLIDE
One challenge with this approach is that, as you can imagine, the probability distribution we're trying to model is very complex - i.e., we're conditioning on every pixel, for every pixel in the image.  A PixelRNN or CNN essentially recognizes that complexity, and uses a neural network to attempt to represent these distributions. To think about this very broadly - in your introductory statistics course, you learned about the so-called 'normal' distribution, which is a curve that is centered around some mean, with some area under the curve falling within a certain prescribed number of standard deviations.  This curve is represented as a function, and you can sample it at any point.  We're doing the same thing here - except, because we have such a complex distribution, we're using a neural network to capture the distribution.  
#SLIDE
PixelRNN came around in 2016, which defines an approach for solving this probability density.  Take the example pixels of an image on the right. In the PixelRNN, a RNN is fit with every pixel as an input.  Remember in a RNN, we have to have some notion of ordering - i.e., the frames in a video.  Of course, an image doesn't have inherent definitions of order - it's just a 2D surface.  So, in a PixelRNN there was a model architecture which a LSTM was fed with an initial point of data - the upper-left-hand-corner - and then every subsequent pixel was fed in all prior pixels.
#SLIDE
So, the second and third pixels would take as input the first pixel...
#SLIDE
And the fourth, fifth, and sixth pixels would take in their neighbors history, on so on until all pixels are fit.  Needless to say, this would generate a VERY long, multi-layer RNN, and so is a very slow approach to generating images. 
#SLIDE
A PixelCNN seeks to estimate the same type of probability - i.e., for the red X here, what is the probability of it being a given value, conditional on all other pixels?  In this case, we're still starting from the upper-left hand corner, but for each X we're passing a filter across the pixels that it is being conditioned on, and using that information to generate the probability distribution. So, thinking about our probability function at the upper-right, what we seek to do is identify the filter weights that provide the maximum likelihood of the input training images.  Put another way - what convolutional weights, when plugged into our probability function, result in values as close as possible to our input matrices?  In practice, this is fit via a softmax loss function within a PixelCNN, where every pixel estimate receives a loss which is backpropogated through the CNN.  While this approach, similar to the PixelRNN, can produce fairly strong results, it is relatively slow due to the sequential nature of the calculations.
#SLIDE
To briefly recap the PixelRNN and CNN - this is our example where we are explicitly calculating a likelihood for each pixel, given surrounding pixels.  This is powerful in helping us understand model performance, but can be relatively slow.  This is a path of extensive inquiry right now, with a wide range of models being created.  One note that we won't cover here but you may encounter is an idea called 'attention', which essentially limits the number of pixels considered in the probability calculation to a smaller, local neighborhood, and then ascribes some number of pixels to a long-term memory.  This and many other explicit probability approaches are proving very powerful.