Alright, switching gears now to model architectures!  This is a big, constantly evolving topic, but at the core comes down to understanding what types of neural nets work when it comes to image classification.  Ultimately, a lot of our knowledge now is seated way back in the first lecture - how humans, cats, and other organisms cellular hierarchies work.  We're going to take a few dives into different approaches, starting with something called AlexNet, which was released back in 2012.   
#SLIDE
AlexNet was the best network we had for the large ImageNet challenge back in 2012, and was trained on a pair of state-of-the-art GPUs with a whopping 3 *gigabytes* of ram.  The architecture is fairly straightforward - starting with an input image (in this case, 224 x 224), first 96 filters of 11x11 were applied (stride of 4), followed by additional convolutions of smaller filter sizes, leading into a max pool and finally a dense net.  This architecture was notable for a number of reasons - it was the first implementation of a ReLU based CNN architecture, it featured an adaptive learning rate - which was manually tweaked! - and it integrated a 7-CNN ensemble, in which 7 models were trained on the same data (with lots of data augmentation) and then averaged to get the final result.  AlexNet was an 8 layer model, so very shallow by contemporary standards.
#SLIDE
Things remained fairly similar in 2013, and then in 2014 two new, deeper nets came onto the scene.  The theme of these networks - of which VGGNet is the first we'll discuss - was to have smaller filters, and more layers. VGGNet used either 16 (in the case of VGG16) or 19 (VGG19) layers, and never had a filter size larger than 3x3; this was shown to improve accuracy over AlexNet, with the best accuracies getting all the way down to 7.3% error.
#SLIDE
But, why? Think back to our original discusison of convolutions, where we used a 2x2 filter.  If you consider this first convolution, the information going into the activation surface is representative of a total of 16 pixels at this point.
#SLIDE
At this second convolution, we're now considering another 16 pixels, of which 6 (highlighted in green) are unique - i.e., pixels that were not included in the first convolution. 
#SLIDE
The same thing happens when we move vertically - i.e., the value calculated here would have 6 new pixels, and then it's neighbor...
#SLIDE
Would have 3 more - i.e., the size of the square we draw information from in this example is 3x3x3.
#SLIDE
Now bring your attention to the activation surface at the bottom - if you convolve over this activation surface with the same 2x2x2 filter, we know that the actual window we're drawing information from - on the image - is 3x3x3.  This is referred to as the "effective receptive field".  As you stack filters, this continues to grow; i.e., in VGGNet, the stack of three 3x3 convolutional layers gives an effective receptive field of 7x7 on the image, in pixel space.   The advantage a set of 3 3x3 layers gives you, as contrasted to one 7x7, is that you have more filter weights that can capture more non-linearities in the images you're observing; this is borne out in the improvements we see in the VGG architecture.  Another nice benefit is that you can capture this with fewer parameters in the net!
#SLIDE
The other strong performer in 2014 was GoogleNet, which is also sometimes referred to as Inception v1.  This net outperformed VGG in some cases, but under in others, but what sets it apart is it's heavy focus on computational effeciency.  Across all of Googlnet, there are only a total of 5 million parameters that need to be fit - this contrasts to over 60 million for AlexNet, and 138 million for VGG16!  One of the ways this happened was through the use of so-called inception models, of which an example is shown on the slide, and we'll dig into on the next slide.  Another surprising element of this net: it has very few affine layers.
#SLIDE
So, what made this Inception approach so special?  The first goal was to build the smallest unit - "Inception Layer" - which is essentially a small network in and of itself.  Then, once the best inception layer is identified, stack them on top of eachother to achieve high levels of depth.  What they identified was a structure in which there are multiple receptive field sizes - i.e., we are convolving using 3x3, 5x5, and 1x1, all simultaneously, as well as keeping a 3x3 max pool of our inputs.  Very importantly, they include 1x1 convolutional layers as precursors to the 3x3 and 5x5 convolutions - in the case of Inception v1., these 1x1 layers have 64 filters; thus, the inputs into the 3x3 and 5x5 is always limited to 64 channels from the previous input.  This dramatically decreases the number of parameters required to fit an inception layer, even though it's conducting a wider range of convolutions.
#SLIDE
Here, you can see the full GoogleNet architecture, with a red box around one of the inception modules (which you can see stacked here).  
#SLIDE
The GoogleNet further has a stem network, which serves as the input into the first set of inception modules.  This is a standard network, which serves to reduce the dimensionality of the inputs before any inception occurs.
#SLIDE
In purple is where the final classification layer is - i.e., these classifications are fed into your test function to establish the final accuracy of your model, and used with a loss funciton to pass back gradients.
#SLIDE
One of the most interesting elements of the Inception net is these auxillary classification branches.  Essentially, these branches do the same thing as the final classification layer, but are only fed into the loss function - i.e., they are used to boost the gradient of the lower levels of the network, but are not used in establishing the final classification for a test.  This is another solution to trying to prevent gradient decay or saturation in deep networks. 
#SLIDE
In 2015, another key architecture emerged that has served as the baseline for a huge array of contemporary models - ResNet.  ResNet took depth to the extreme, increasing from the around 22 layers of GoogleNet to 152 layers, and implementing a new type of architecture around residual blocks.  The payoff of this was huge: they knocked error down to 3.57%, and equally impressively swept every single category.
#SLIDE
One of the key rationales behind the development of ResNet was the fact that, if you just stack convolutional blocks on top of eachother forever, you hit a saturation point pretty quickly at which it becomes challenging (if not impossible) to find a reasonable set of optimal parameters.  This can lead to dramatic underperformance of deep networks relative to shallow networks, as the number of parameters simply becomes unmanageable too quickly.  Working under this hypothesis, He et al in 2015 sought to construct a model architecture in which a deep model could always perform *at least as well* as a shallow model.  In practice, the concept of what He et al. came up with is fairly simple - rather than try to fit a function on the basis of the input data alone, as would be the case in a standard net, instead the function you seek to optimize is F(x) + x - i.e., the input X value is actually added to the final set of 'scores' (or outputs of any kind).  Imagine, for example, you have an exceptionally deep network wiht over 50 residual blocks, all strung together.  You identify an optimal solution after three of these blocks.  By solving for F(x) + X, you can simply set the remaining block values to "0", precluding the need to optimize deeper layers.
#SLIDE
As an example of the full ResNet architecture, here is the ResNet34.  I show you ResNet 34, because 34 layers can fit on a single powerpoitn slide, annd the only difference between this and ResNet 152 is - you guessed it - more residual layers!  The architectures are, as you can see, very straightforward - you just keep stacking, and then throw an affine layer at the end with a target number of nodes equal to the number of classes in your case.  The flexibility of ResNet - which can handle both simple and complex problems due to the residual architecture - has made it an immensely powerful "go to" for a very wide range of problems.