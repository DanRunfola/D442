
===SLIDE 1

Now we're going to talk about another specific approach to loss that you will very commonly encounter - a multinomial logitic regression, otherwise known as a softmax classifier.  Softmax interprets scores in a different way from multiclass SVM - instead of trying to identify the "best" class (i.e., the class with the highest score is the classification chosen), instead softmax seeks to predict the *probability* that a given image belongs to a given class.  So, in this made up case, the scores are read into the function, and then based on these scores we predict probabilities of class inclusion.  We're ultimately trying to identify the model that creates probabilities closest to our training data - i.e., in a perfect case, this example would have a probability of 100% for Cat, and 0% for both Car and Frog.

=======SLIDE 2
To solve for this softmax, we first make a key assumption - that the scores our model predicted are representative of unnormalized probabilities of each class.  That is, the number 3.2 for "Cat" is - in some way - reflective of the probability that the image is a cat, we just haven't scaled it to be a probability yet!  For softmax, we are specifically making the assumption that the scores are unnormalized *log* probabilities - this lets the classifier play nicer with optimization techniques later on.

========SLIDE 3
This can be formalized as seeking to identify the probability that the true class (Y) is class k, given an input Xi (the image). 

=======SLIDE 4
For softmax, we solve this probability using the softmax function.  Here, we exponentiate each of our scores s (this makes them positive), and then normalize by the sum of all exponentiated scores.  The numerator of this equation would be the exponentiated score for class k (i.e., cat), while the denominator would be the exponentiated scores for all classes (cat, car and frog).  

========SLIDE 5
Thus, in a perfect world we would get our desired behavior - if the cat score was very, very high, and the car and frog scores were very, very low, the cat value would approach 1, while the other two would approach 0.  

========SLIDE 6
So, how do we actually use this information?  We have a function that tells us how good or bad our scores are at approximating the probabilities we want, but how can that inform our modeling efforts?  Well, we want to use this as a loss function - something to inform the algorithm guessing weights the level of badness of a given set of weights.  There are a few things we need to worry about to do this.  First, we need to multiply by -1 - the reason for this is because loss functions need to go up when the algorithm is bad!  

======== SLIDE 7
Now, we could technically use this equation as is at this point - however, there are a number of benefits to taking the log of our probabilities for the loss function.  While these are beyond the purview of this course, suffice to say that mathematically it is easier to solve for weights when using loss functions derived from logs, rather than the absolute data.  So, the final equation becomes what is shown here.

========= 8
Note, you may also commonly see the softmax loss function represented in short-hand like this, and this is how I'll represent it for the next few slides.

========= 9
Ok!  Let's walk through an example with our cat licking it's paw here.  We want to calculate for that estimate what the loss function would be in this case, just like we did with the multiclass SVM in the last lecture.  Following the loss function on the screen, the first thing we'll do is exponentiate each of the three scores.  This is the numerator of the softmax function.

========= 10
Next, we sum all of our exponnentiated scores - this gives us 188.68. Then, we divide each exponentiated class score by the sum of all exponentiated scores to normalize, which gives us probabilities that range between 0 and 1.  These probabilities are also guaranteed to sum to 1.

========== 11
Finally, our loss will be the negative log of the correct class (k).  In this case, we know the image is a cat, so we take the negative log of 0.13, which is 0.89.  Remember - big numbers are worse for our loss function!  

========== 12
Take a look at the chart at the lower-right - this chart shows the softmax loss for each of the three images.  As expected, the frog has a really high score (high is bad!), and the car very low, with the cat in the middle.

========== 13
So, why do we choose different loss functions in different cases?  Let's compare our multiclass SVM loss to the softmax.  Remember - in both of these hypothetical cases, the weights vector is the same - all we're trying to do is teach that algorihtm how bad the weights vector is.  Both of these loss functions tend to agree on the general trends - i.e., the frog is bad, and the car is good.  The big difference you can see is when it comes to the car - in the case of the car, the SVM hinge loss recognizes that the classification is correct - that is, we would not only call the car a car, but we would do so with confidence greater than our epsilon term.  Thus, the weights would not be tweaked any further to get a "better" car classification; rather, we would seek to retain similar weights for car cases, because it's doing well!  Conversely, the softmax still shows that there is some error in the Car case - not much, but more than 0.  Thus, in the case of softmax, the weights we select will continue to be changed to preference an even BETTER car classification, all the way down to approach 0.  You'll see a number of different loss functions used to train algorithms throughout your applying machine learning, and the selection of an appropriate loss function generally comes down to your knowledge of your data, problem, goals and intuition.
