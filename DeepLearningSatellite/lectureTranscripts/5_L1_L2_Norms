
=== SLIDE
Many different machine learning algorithms - including KNN - require the selection of a distance metric.  The example we just walked through was an example of the use of a L1 distance metric.  Contrasting to this is a L2 distance metric.  While it may seem bland, the choice of a distance metric has a number of implications.  A few important things to note.  First, L1 distance is predicated on the idea that differences are linear - i.e., a one-unit change in difference between 0 and 1 matters as much as a one-unit change in diference from 10 to 11.  L2 biases towards large differences - i.e., a one-unit change in difference between 0 and 1 matters less than a one-unit change in difference from 10 to 11.  However, L1 distance is sensitive to changes in your underlying coordinate systems, which inter-relates to the standardization strategies you choose.  As a broad rule of thumb, L2 distance is generally preferable if you are working in a generic feature space, but L1 might be preferable if you know the specific data you have is highly relevant to your classification (i.e., the precision of the measurement matters a lot). 

The two figures on the slide here present a visual of the difference between the L1 and L2 norm.  In both cases, imagine you are trying to distinguish between two images - an image represented by the yellow dot in the center, and an image represented by the green dot at the lower-right.  In both cases, you took the value of the upper-left hand pixel and plotted it on the Y-axis, and the lower-right hand pixel, and plotted it on the X-axis.  So, the yellow image had a value of 0 in both pixels; the image represented by the green point had positive values in both pixels.  The green dot and yellow dot are on the same spot in both cases.

=== SLIDE
Now, let's add a second image - we want to identify if the image represented by the yellow dot is more similar to the image represented by the green dot, or the image represented by the purple dot.  On the left figure - the L1 Norm - both images would have an *identical distance*, represented by the blue diamond.  That is to say, the measured distance between the yellow image and the green image would be exactly equivalent to the yellow image and the purple image.  Conversely, in the L2 Norm, the exact same images would result in different distances - in this example, the Green image would be measured as more similar to the yellow image. 

To re-iterate: in general, L1 norm will be important if the precision of your underlying measurements is well known and important, as all differences between values are treated the same irrespective of their magnitude.  L2 is more generalizable, and more readily useful in generic feature space.

=== SLIDE
So you may be asking yourself - outside of these rules-of-thumb - how do we select the best distance metric?  This is a broad class of problem, and not just about distance metrics - think about KNN so far.  There are two different choices we have to make to implement the algorithm: what value of K do we use, and what distance metric to use.  Both of these are referred to as "hyperparameters" - choices we make that influence the algorithm, but are not trained based on the data. 

A big challenge is that the correct hyperparameters are very problem dependent - the underlying nature of your data, the target you're trying to predict, sparsity, and hundreds of other things come into play in driving the best.  So, generally, we seek to try them all out and see what works best! 