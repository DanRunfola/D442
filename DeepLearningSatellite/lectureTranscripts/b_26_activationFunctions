Ok - let's head back to our CIFAR-10 example.  You'll recall that we first technically implement our image as 3 matrices, i.e., a tensor.  We thenn pass some number of filters over that tensor, which will result in one activation surface for each filter.
#SLIDE
The entire process looks something like this.  You start with your input image tensor, and define some number of filters.  Then, based on your strides and zero padding, you convolve that filter over the tensor to create one activation surface per filter.  In this example, the activation surface is slightly smaller than the input tensor - representing the number of valid convolutions possible with a 5x5x3 filter given the 32x32x3 tensor.  Finally, we aggregate all of the output activation surfaces into a new tensor, with a depth equal to the number of filters.  Of note here, the terms "activation surface", "activation map", and "activation layer" are used somewhat interchangeably to refer to these surfaces; you may also hear the term "activation tensor" to refer to the collated group of all activation surfaces.
#SLIDE
Once we have our activation layers, we can then use them as inputs into other models instead of the image itself, to generate our final scores.  While this is a very simplified linear model, as we have discussed activation layer information is most commonly passed forward into another neural network. 
#SLIDE
When we're building our activation surfaces, the filters that we convolve across the tensor are made up of a set of dimensions and, also weights.  If we blow up the filter, we would see something like this - a stack of matrices with numeric weights.  These weights are used when convolving, and the dot product between the weights and the image tensor values are taking as we convolve to generate the activation surfaces (you'll recall we do a deeper dive on this in our first lecture on convolutional nets).  
#SLIDE
A few lectures ago, we also discussed the idea of optimization - i.e., identifying the best weights to minimize a loss function.  We also chatted about backpropogation and the chain rule as approaches to identifying optimal gradients.  Specific flavors of optimization we've discussed have been Gradient Descent, Stochasist Gradient Descent, and Mini-batch Stochastic Gradient Descent.  We also chatted about a few techniques that simply aren't used in practice - i.e., randomly guessing weights; full searches of all values.
#SLIDE
Recall for example Mini-batch SGD.  In this approach, we have a 6-step process we follow to optimze our network.  First we sample our data, taking some number of observations equal to a hyperparameter batch size.  We then take that data and run a forward propogation, and calculate the loss (i.e., how bad the current set of weights are).  That loss is then used to backpropogate across the network to calculate the gradients of weights with respect to the loss, and we update the weights using that gradient.  We then repeat this process until we reach some hyperparameter-defined threshold - the number of loops, the change in loss over the last few iterations, or other related metrics.
#SLIDE
There are a number of steps that you must go through to build and optimize a network - starting with network architecture, then optimization, and finally evaluation.  We're going to start at the top - choices you need to make regarding the overall architecture of your network - i.e., how to design your computational graph, and work our way through the entire process.
#SLIDE
First, let's start out with some fundamentals.  What you're looking at now is the basic computational graph we've been using throughout this course - we have two inputs (a pixel value in red and a weight in teal), and one computational node (multiplication).  
#SLIDE
A common practice in neural networks is to build a computational graph that looks like this - multiplications between weights and values fed into an additive function.
#SLIDE
This added sum is then passed into some activation function (in this example, ReLu), which then passes the signal on.
#SLIDE
A more common way to visualize networks incorporates the weights into the outputs of each node.  For example, here we have the same two data inputs - pixel 1 (p1) and pixel 2 (p2).  We still have two weights - w1 and w2 - but now they are visualized along the path between nodes.  
#SLIDE
Within this receiving, computation node you can visualize two different processes - the process used to aggregate data coming in, and then the activation function used to pass a signal on.  The aggregation algorithm in neural networks is nearly always assumed to be the sum of the weighted inputs - i.e., in this example, w1*p1 + w2*p2.  Frequently this aggregation algorithm will not be visualized and is instead assumed.  
#SLIDE
The more commonly explored process is the activation function - i.e., the function applied to the value the aggregation function calculates.  There are many different activations, and we'll get into them in just a moment.  Here, the output of that activation function is notated as o_1, and is the value that would be sent to the next node.  Also, note w3 - just like the other two inputs, another weight is applied to any outputs from this node.
#SLIDE
This can also be written like this - i.e., the output of this computation is equal to some activation function f applied to the weighted sum of inputs to the node. 
So, how do we define this activation function? What are the pros and cons of different approaches?  Ultimately when we code our networks, we will have to write some function that takes in the weighted values from earlier in our computational graph, does something, and returns a value to pass on to the next nodes.  Here is a very, very simple function - i.e., one that simply returns the input value.  In this example, if the activation function f was the defined activationFunction in the dummy python code, only the raw values themselves would be passed throughout the network.
#SLIDE
There has been a tremendous amount of research into the best performing activation functions for different purposes.  I've discussed ReLU a few times before, which you can see here at the lower-left.  All ReLU does is take in the weighted value, and then return the maximum of either that value or 0.  In code, this would be as simple as...
#SLIDE
This short function.  So - why would you pick a given activation function?  Let's chat about a number of the most popular functions, and what the various reasons are to use one over another.
#SLIDE
First, we have the sigmoid activation function - we've already discussed this in this course in the context of loss functions.  The sigmoid activation has two key features as it pertains to neural networks.  First, all values are rescaled to a range between 0 and 1, irrespective of how large the inputs are.  Second, and why sigmoid has been used historically, is that they roughly approximate how human neurons work - i.e., the neuron "fires" only once a certain value is reached.
#SLIDE
Sigmoid activation functions have two challenges that you should be aware of related to gradient decay and zero centering.  Let's dig into the challenge of gradient decay first.  
#SLIDE
Imagine for a moment that we are trying to solve for the gradients of weight 1 and weight 2.  We just initialized our model, and they both randomly received a weight of 10.  Now, look at the sigmoid function - if both weights changed from 10 to 9 (i.e., a decrease of 1), what would the change in the function be?  Pause for a moment if you want to think about this.
#SLIDE
The answer: nothing!  A change from 10 to 9 in the incoming values would make no difference - i.e., the output of the sigmoid would remain (very, very nearly) identical.  You would need to change your weights to something around 0.25 in this example before you would start to see meaningful gradients.  This is due to the nature of the sigmoid function - values greater than around 4 and below -4 simply don't change very much (As you can see in the graph).  So, if you have sigmoid functions, they can effectively decay your gradient to 0, which can be problematic for finding optimal solutions.
#SLIDE
The second issue about sigmoid is that it is not zero centered - i.e., there is no shift in the data that is input, so if all of your values are positive on input, they would remain positive on output (and vice-versa).  While this sounds good in principal, in practice it causes some big problems with gradient solutions.  The most important of these is when all inputs to a neuron are negative or positive.  In these cases, a sigmoid function gradient will result in all positive or negative solutions - there is no mixing.  So, imagine you're trying to optimize two weights (w1 and w2 in our example on the slide).  The best answer is when w1 = 1000, and w2 = -1000.  Sigmoid activiation functions would be very, very inefficient at finding such a solution, because each iteration it would only adjust both weights in a positive or negative direction - no mixing allowed.
#SLIDE
Tanh has some of the same benefits of sigmoid - specifically, it has a functional form that roughly approximates how neurons work (with a rapid increase at a certain "activation" value).  The big difference is that outputs are between -1 and positive 1; i.e., it's zero centered.  So, the problems a sigmoid function would have when all inputs are positive or negative do not apply to tanh.  However, gradient decay due to saturation is still an issue, just like the sigmoid case.
#SLIDE
Next, let's chat about what is probably the most common activation function across models today - the Rectified Linear Unit, or ReLU.  ReLU has been shown to be exceptionally powerful for a wide range of uses, and is commonly employed in networks today.  One of the biggest reasons for this is that it is computationally exceptionally simple - i.e., taking a max is a nearly instantaneous operation - and thus can be used to fit very deep networks with less computer power than might otherwise be required.  However, it does have some similar drawbacks - it is not zero centered, which can lead to ineffeciencies, and it can also have a saturation if values are below 0.  Across networks built primarily on ReLU nodes, the issue of "dead ReLUs" is prominant, as entire regions of a network can stop updating due to the negative saturations.  Some of the most effecient networks we have identified still have around 20% of nodes as "dead relus".  This doesn't mean the network is bad, but it does indicate large inefficiencies in model architecture and concommitent optimization.  Of note, because ReLUs are shown to be powerful and are still desired to be used, one common approach is to initialize network weights as weight + .01, rather than simply random weights.  This positive direction bias may help mitigate the percentage of neurons that end up "dead".
#SLIDE
There have been other approaches, however, to try and mitigate the issue of ReLU death.  One of these is called a "Leaky ReLU".  This function simply takes the max of either 0.01*X, or X.  So, in the case of a negative value, instead of a 0, the output is .01X, or something close to 0.  This has a number of benefits, but the most important is this means that there is no saturation - i.e., a change in X will always result in a change in the output.  It's also still very computationally effecient, and retains our approximation of "firing" neurons.
#SLIDE
The Parametric Rectifier - PReLU - is a generalization of a Leaky ReLU.  Here, instead of hard-coding 0.01 in the max equation, we instead replace it by a parameter (alpha on this slide).  Because it is parameterized, it can effectively be treated as a weight and optimized as a part of the backpropogation procedure.  A PReLU with alpha of .01 would be identical to a leaky ReLU.
#SLIDE
Next up - the Exponential Linear Units (ELU) function.  Here, the primary goal is to create a function which has many of ReLUs qualities, but is also zero centered.  It also uses the parameter alpha, which can be fit during backpropogation.  In this function, you'll note that saturation also happens fairly slowly - i.e., if we compare to the last few cases (TOGGLE BACK TO RELU and COMPARE), you can see that while saturation might be an issue, it is less likely in the ELU case (however, it is still there!).  It is also much closer to zero-centered, but not *actually* zero centered; this has a number of benefits; however, all of these benefits come at the cost of relatively high computational cost due to the use of an exponent.