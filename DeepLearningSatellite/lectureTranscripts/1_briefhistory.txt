As you hopefully know, this course is heavily focused on computer vision - how we train algorithms to interpret data collected by sensors that is represented in a graphical form.  We'll be specifically digging into the plethora of ways that computer vision is being applied in the context of satellites, but before we go there I want to give you a short background on the field.

The vast majority of computer vision research has happened over the last decade or two, prompted largely by industrial needs.  Fields where this has been prevalent include advertising (analyzing endless videos uploaded to popular social media sites for advertising suitability), surveillance (security camera footage), mobile computing (cellphone camera quality), and many others.  Just to give some perspective, circa 2019 about 60% of the data downloaded over the internet was some form of video; if a website such as youtube sought to use human's to manually assess videos, it would take around 20,000 full time staff. Thus demand to replace an infeasible amount of human labor with algorithmic approaches has been enormous.  

== FIRST SLIDE
Unfortunately, computers - as they have been coded over the last 100 or so years - are not well equipped to analyze image data.  Instead, nearly all of the algorithms we have derived focus on analyzing discrete "units of observation" - think rows in a spreadsheet.  Because of the scope of the challenge, computer vision has tremendous overlap with a wide range of disciplines.  If you consider the types of knowledge needed to replicate the process of human data intake through our eyes, processing and transport to our brains, and then cognition of that data - it's really quite a lot!  We draw on fields like neuorobiology to understand how the human eye works and sends information - much of what you will learn in this class seeks to replicate biophysical processes.  We rely on math, computer science, and related disciplines to implement the algorithms required.  And, we rely on a range of physical sciences to help design and interpret the information from sensors.  Increasingly, we also rely on physical engineering to help build new hardware that allow us to implement algorithms for computer vision more effeciently than is possible today.

== SECOND SLIDE
The quest to understand how our minds interpret visual information has been a challenging one.  In 1959, Hubel and Wiesel conducted studies in which the electrical responses of cats minds (i.e., when neurons were firing) were measured as they were shown different types of shapes on a screen.  Much of computer vision was inspired by this work - it was one of our first opportunities to understand how mammals processed data which came in through the eye.  The key thing they learned was that in the visual cortex of the cat's brain simple structures - relatively simple groups of neurons - directly responded to stimulus, and especially simple stimulus like changing the orientation of edges in an image.  These simple structures then passed their findings (electrical impulses) on to more complex structures, allowing for an integration of information and conceptualization of visual inputs.  This general approach is what forms the basis of the deep learning strategies you'll be learning about in this course.

== THIRD SLIDE
Alongside developments in understanding how the mind worked, computer scientists were busy at work theorizing how they could computationally define the world around them.  This started back in the 1960s, with some of the earliest work showcasing how most entities could be represented by arbitrary numbers of blocks with different sizes or shapes.

== FOURTH SLIDE
In the 1970s, this was expanded on by exploring how groups of other objects - cones, or arbitrary pixels - could be used to define shapes.  Of particular interest to us is that this is when we first start seeing some of the first applications of computer vision in the aerial imagery domain.

== FIFTH SLIDE
The 1980s were dominated by explorations of how to capture the outlines between different entities - something that later emerged as object-based classification in the remote sensing domain.

== SIXTH SLIDE
And, come 2001 we had the famous Viola and Jones algorithm which illustrated the use of all of these approaches to implement real-time face localization on a relatively low power processor.  This work was not only important for it's illustration of the capability of machine learning for imagery recognition, but boosted computer vision into the forefront of industrial innovation - today, nearly every camera shows the bounding box of faces! 

== SEVENTH SLIDE
With industrial investment came a number of rapid breakthroughs.  First, Lowe and others helped to redefine the challenge of object identification through the concept of Scale Invariant Feature Transformation - or, the idea that when we're trying to identify something in an image, we are really trying to identify the things that don't change, even if the angle of the image changes.  This helped to reduce the challenge of identifying objects in images to that of identifying groups of pixels that represent a given object, even if the image itself shifts angle.  

== SLIDE 8
From 2006 onward, a number of large-scale image object competitions launched, which have spurred extensive development of computer vision models. This started with PASCAL in 2006, containing 500,000 consumer photographs from Flickr which are labeled according to the objects they contain (i.e., sheep, dog, potted plant - a total of 20 labels).  The limitation of 20 labels led a large group of scholars based out of the Stanford Vision Lab to create ImageNet, a collection of approximately 14 million images with 22,000 labeled categories of objects.  With this dataset, a guantlet was thrown down: how well could these new computer vision algorithms classify such a massive corpus of data?

== SLIDE 9
From this gauntlet emerged the idea of "deep learning".  In 2015, a model called "Residual Network" was released, which blew every other model away in terms of accuracy - classifying ImageNet with only around 3.5% error.  How did it do this?  By dramatically increasing the so-called "Depth" of a neural network, adding 152 layers.  Most of the improvements we've seen since AlexNet have focused around Convolutional Neural Networks, which are a fundamental building block on which this course is built.  Core concepts of CNNs - how convolutions are done, feature maps, subsampling, pooling, and fully connected layers are all still present today - and we'll be doing a deep dive into each of these throughout the course.  

== SLIDE 10
With that, we'll head into the next lectures which will focus on the inner workings of convolutional neural networks and deep learning, and finally answer the age old question: how can you tell a cat, is a cat?