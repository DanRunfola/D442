A quick note before I get started - part four of assignment 2 follows along with much of this lecture, so I suggest you engage with both at the same time!  Ok - now we're goign to talk about the actual process of optimization.  At this point, we've defined our network architecture - i.e., the number of layers, nodes, inputs, batch normalizations, activations - everything.  We have a data processing pipeline - normally just a zero-mean shift for image data - and we've defined some strategy to initialize our weights (probably Xavier).  That is - we're ready to click "go".  So what do we do now?  Chances are your network is going to fail to fit your first try, so how do you diagnose the issues?
#SLIDE
To recap - last lecture, we first discussed the most common approach to visualizing network structure.  We also discussed the two different computations done within these nodes - aggregating the incoming values, and then that aggregation is fed forward into an activation function.
#SLIDE
There are many different types of activation functions, with a range of pros and cons.  Both saturation (i.e., increases or decreases in your inputs don't change the function) and a lack of zero centering (i.e., outputs are all unidirectional if inputs are unidirectional) can both pose major challenges for optimizing your network.
#SLIDE
In most cases, these issues can be mitigate through three processes.  The first of these is preprocessing - most commonly, zero centering our image data by subtract the mean value of each band from every image.
#SLIDE
Weights initialization strategies can also help by ensuring the distribution of weights is balanced with respect to the complexity of your network.  
#SLIDE
And, finally, we discussed batch normalization - a computational stage in the network that forces all weights to a given distribution.
#SLIDE
Now, we're going to move on to some practical pointers on how to actually optimize. We've already picked a preprocessing and weights initialization strategies, and we're going to use ReLU activations for this example.  Our network architecture is shown here as well - we have CIFAR-10 as an input (3072 pixels), 50 hidden nodes, and 10 outputs - one per CIFAR-10 class.
#SLIDE
#In code, we will implement this type of network in stages.  First, we would have some function that defines our data preprocessing.  In most cases, this will reduce down to two things.  First, we decide if we are going to zeroShift.  The top of the code to the right is what does this - first, we calculate our mean image value, and then we subtract that value from the train and test inputs.  Here, I also have a visualization element so you can see what's happening.  Second, we have to choose if we are going to reshape our array.  For non-convolutional approaches that require one long matrix (i.e., the 3072 element long matrix on the left), we need to reshape.  Later on, when we get to convolutions, we'll disable this option.  The output of this code would be "P" in the figure to the left - i.e., our 3072 x 1 CIFAR arrays.
#SLIDE
Next, we need to define our layer algorithm - i.e., something that takes in P and Weights beta, and then outputs h.  In a fully connected case, this would be equal to 3072 x 50 weights - i.e., every input P is connected to every hidden node h.  The dot product between inputs P and weights W are what give us our outputs h.
#SLIDE
This type of fully connected layer is also referred to as an "Affine" layer.  In code, we need to define two seperate operations here.  First, we need the outputs h when we do our forward pass; second, we need the gradients for the backward pass.  Here is the forward pass, which is fairly straightforward - let's go line by line.  In the portion above the red box, we're doing some reshaping so that our weights and inputs are as-expected.  In the red box is where the real action happens.  First, we're taking the dot product between our x inputs (the 3072 pixels) and w weights (weights beta in this case).  For the first time in this course, we're also adding the term "b" - a bias term which we've been omitting to simplify our examples.  This bias term provides the network with more flexibility regarding systematic biases in your data - you can conceptuliaze it just like you would an intercept in a normal linear model.  On the next line, we have our cache - this is a variable we are going to save, because we'll need this information to solve for the gradient on our backward pass.  Finally, we return our output (in this case, a 50x1 hidden layer) and our cache of inputs.
#SLIDE
Now, we take the outputs from the affine layer, and pass them through the activation function we defined for hidden layer h.  In this case we're going to use a ReLu, which is just taking the max!
#SLIDE
We then pass the outputs of that relu to our second affine layer.  This is done in the exact same way - and with the same function.  All we have to do is change our weights initializations to account for the fact our inputs and outputs are different sizes (50 and 10). 
#SLIDE
The scores that are output - all 10 scores - are then fed into a loss function.  Here we solve for loss and the gradient to send back as per the chain rule.  Let's briefly walk through this, but this is the same SVM loss function we've worked with throughout the course.  First, we take the number of observations (N).  We then identify for each observation, the correct class score (i.e., if an observation was truly a Cat, we grab the score for "Cat" for each row).  We then calculate the SVM
loss function and save it into margin.  This is corrected on the next line by setting all "y" cases to 0 - i.e., we don't calculate loss for the correct class.  We take the sum of these for the total loss.  Now, we turn to solving for the gradient of the loss with respect to changes in our scores s (remembering that those scores are the input into this function).  Nothing new here - we build an empty matrix, count the number of positive margin cases (to solve for the gradient in the correct cases), and set cases with positive values to 1.  Finally, we divide by the sample size and, viola!  We have dX we can use to back-propogate through the network.
#SLIDE
Now we're ready for our backward pass!  The first pass back requires us to solve for the changes in our loss function when our weights W-alpha (and associated biases) are changed.  The code for that is on the screen now - we first reshape our inputs in the same way as our forward pass, and then we take our upstream gradient and solve for each of dx, dw, and db.  Head back a few lectures if this looks like nonsense to you - you've seen all of this before in our lectures on gradients!
#SLIDE
Now we head back again - now we're solving for the change in the loss function given a change in our hidden node values.  You'll recall we use a RElU activation for these hidden nodes, and so we need a backward relu function.  This is nearly as simple as the forward pass - we just pass the gradient straight on unless the incoming value was less than 0, in which case we pass on a 0.
#SLIDE
Finally, we do one last propogation back through our weights in the first affine layer.  And with that, we're done!  We now have everything we would need to update our weights.
#SLIDE
We're going to switch gears a little now, and start talking about some practical considerations for fitting your networks - i.e., how to debug issues when you come across them.  And, you will come across them!
#SLIDE
One of the first challenges with network design is that you can select nearly any network architecture to engage with a challenge.  From complex architecture like ResNet, to simple Dense nets with one hidden layer, each architecture is prone to different types of challenges during the learning process.  Take this network as an arbitrary example - here, we have two different weights layers.  An easy way to make this net perform MUCH worse in optimization would be to intialize each set of weights with a dramatically different magnitude - i.e., W_alpha initialized as values from 1000 to 10,000, and W_beta initalized as values from 0 to 1.  While this is obvious in this simple example, as architectures get more advanced - or you begin to ensemble different models together - you want to ensure that you're designing all of your initialization strategies in a way that won't result in either saturated regions of your net, or changes so minimal they would never effect the parameters you want changed.
#SLIDE
Practically, when your net isn't fitting there are a few discrete steps you'll want to take in your code.  First of these, print out your gradients - if all of your gradients are equal to 0, that is a key indicator that something is wrong.  The specific reason something is wrong can be highly variable - i.e., in some cases, it may be you have a bug; however, just as common is something wrong with your weights initialization, learning rate, or a saturation problem.  You can print your gradients out at any step of your backpropogation to try and isolate where the error is occuring; this is frequently my go-to for debugging complicated nets.
#SLIDE
Another important thing to look at is your loss function - you want to confirm that it's (a) not 0, if it shouldn't be; and (b) a reasonable value based on the loss function you chose.  This can be a giveaway of errors related to (for example) your input data - i.e., if images are corrupted and have all 0 values, your loss function may not change at all no matter what you do (as you're always multiplyign by 0).  
#SLIDE
More broadly, on your first pass your loss function should be similar to what you might expect with random noise - not identical, but similar.  You can quickly solve, for example, that the expected value of the softmax loss function under completely random noise would be 1; thus, on your first forward pass, you would expect a value ~around~ 1.  If you're seeing values in the thousands or tens-of-thousands, that's a good indicator something is wrong.
#SLIDE
When you add regularization into your models, it’s also helpful to first run the model with λ = 0 first, record the loss value, then set λ = 1.  As λ controls the impact of regularization, when you change λ to 1, you would expect the total loss to go up.  If it doesn’t something is wrong with your regularization code or algorithm.
#SLIDE
Once you're confident that the mechanics of your model are performing as expected, you'll want to create a development dataset.  This should be very small - i.e., the first hundred or so values in a dataset - and is what you'll use to test your models are performing as you expect.  Having a very small dataset also has the advantage of allowing you to quickly overfit, getting near perfect solutions for your set of data, which lets you further confirm your model is behaving as expected (i.e., you should be able to see loss go down to 0 with a small N).  I personally will slowly increase the size of my dataset as I become more confident in my model - starting with around 100 examples, then 1000, then 10000, and so on until I am using the full dataset.  Each step up comes with very large computational - and time - costs, so you want to be increasingly confident you won't run into silly errors (like missed commas)!
#SLIDE
After those checks all pass (i.e., your model gets a loss of near-0 after the small-N run, your gradients are updating as expected, the loss function makes sense), then we can really start working on the network itself.  The first practical thing you want to choose is a learning rate - i.e., the value you will multiply the gradients by to update your weights.  Large learning weights result in more rapid changing of your parameters, but setting the value too large can result in you 'skipping' optimal solutions; smaller weights give you a more granular estimate of your loss surface, but takes longer to fit (and, in some cases, may never fit).  Take this slide for example - here, wusing the same code as you use in assignment 2, I am printing out the weights values after each backpropogation of a simple two layer network.  You can see here with LR = .00001, the weights are updating each iteration.  Note that not all models are equally sensitive to learning rate; it has a lot to do with the specifics of your data and loss function.  Bottom line is: if you see your weights aren't updating, try turning your learning rate up to see if you can improve the situation.
#SLIDE
While accuracy is a good way to establish what learning rate is most effective, sometimes you may also want to diagnose the *rate* of learning - i.e., how quickly the loss is improving.  What you see here is an output from your assignment 2, in which the red line plots loss over epochs.  This curve looks very reasonable - i.e., it's gradually approaching some lower limit; in general, this is what you hope to see if your learnign rate is well tuned.
#SLIDE
The fushia line on the chart now is what you might expect to see if your learning rate is too high - i.e., you find a solution fairly rapidly, but it's not globally optimal (i.e., your stuck in some minima that is not truly the best).
#SLIDE
Finally, the green line is indicative of what you might see if you have a learning rate that is disproportionately high relative to your weights - you frequently just see your gradients explode, and loss grow until your net stops solving because of overflow issues. 
#SLIDE
Also of note here is that - as you search through parameters like learning rate - it is frequently sensible to conduct randomized searches rather than structured grid searches.  In the earlier slide, when I set LR equal to 1, .1, .01, etc., that was representative of a grid search in which I am predefining the parameter space to search across.  Randomized search is another, more effective (in theory) way to do this; going a step farther, there are other techniques like Bayesian Search, which take prior information on the outcome of different parameter values to try and identify the best combinations.  
#SLIDE
Now, we're going to start talking about alternative approaches to optimization that are commonly used to fit networks. Recall back to the very generic problem we're trying to solve with optimization - given some set of parameters, what is the set that reduces the loss function to the lowest possible value.  In the case of stochastic gradient descent, we do this by solving for the gradient of the weights at any position, and then update our weights according to those gradients so that we are always moving to a state of the system with a lower loss.  We keep updating in the negative direciton of the gradient until we converge on some region of low loss.
#SLIDE
SGD itself is, unfortunately, limited in a number of ways; you may run into some of these cases in practice.  Imagine for starters this loss surface, where we're trying to find the best value for two weights (w1 and w2). Our first random guess is denoted by the orange circle, which has a loss of 5 (denoted by the red isoline).  
#SLIDE
If we moved the orange dot downhill along the W1 dimension - i.e., we didn't change W2 at all - we could very quickly see a change in our loss from L=5 to L=1.
#SLIDE
Covnersely, if we move a similar distance along W2, we see little to no change in our loss.  This is referred to as the loss having a bad condition.  Thinking about this logically, what it means is that when we go to update our weights, we will tend to update by larger amounts in the "up down" (W1") dimension, and very little in the W2 dimension.  This results in..
#SLIDE
A highly nonlinear set of steps to find the best solution, in which large shifts occur in one direction, but not the other.  While you'll eventually find the optimum, it will be very slow.  
#SLIDE
Another challenge with SGD is local minima.  We talked a little about this before - essentially, this is when the loss function has some inflection point for some values of W.  In this example, it would be possible for a gradient descent approach to be "stuck" at an incorrect solution, indicated by the red circle.  As we have implemented it, gradient descent would get "stuck" here, becuase our gradient would be 0 at that local point, so it would not be able to go out of the incorrect solution and over the "hill" towards the better solution farther along the W1 line.  
#SLIDE
Similarly, SGD can be stuck at saddle points - i.e., at the *very* top of a loss function, gradient is technically equal to 0 in both directions.  While this seems unlikely, note that saddle points get more common as you get into higher dimensional cases; conversely, local minima get less common in higher dimensions.
#SLIDE
So, how do we fix these issues with SGD?  Let's start by refreshing ourselvse as to the actual equation defining SGD, which is shown here.  The weight we test at the next iteration is calculated based on the current iteration's weight minus the gradient of that weight with respect to the loss function.  The alpha term defines the learning rate - how large of a change we implement relative to the gradient.  This form has all of the aforementioned issues.
#SLIDE
To mitigate the challenges of tradition SGD, a minor addition called SGD+Momentum was created.  Here, we add an additional equation that, at each step, calculates a velocity based on the running mean of gradients, and contrasts that to a new term - rho- which defines friction.  To walk through this equation, first we calculate the gradient of W with respect to the loss, as before.  We then add to this our velocity term, multiplied by Rho.  This gives us the velocity for the next iteration.  We then use that velocity to solve for the weights update itself - essentially, the weights are updated not only based on the instantaneous gradient measured at a single point in space, but rather are updated based on both the current set of weights and the preceeding set. Rho is what defines the amount of the last iteration considered in this estimation; it is commonly set to a fairly high level (0.9 or higher).
#SLIDE
This addition of momentum greatly helps to mitigate all of the issues we just discussed. For both points of local minimia and saddlepoints, the interpretation is fairly straightforward - the momentum can help us essentially get over points of zero-gradient where we might otherwise be trapped.  In the case of poor conditioning, we essentially rely on velocity to allow us to "cancel out" movements in opposite directions - i.e., instead of a zig-zag, we can pull our gradient estimates onto a line that will (theoretically) head towards the true minima, instead of bouncing around in a slower fashion.
#SLIDE
Another common solution you'll see to SGD shortcomings is AdaGrad.  At each iteration, AdaGrad seeks to record a value - represented by Gamma here - equal to the square of all gradients.  Each iteration, this gamma value is equal to the sum of squares for each preceeding iteration, so the gamme value always increases over time (i.e., more iterations means the gamma value is higher).  The update rule implemented then divides the gradients by this gamma value.  This is done element wise - i.e., each element of the gradient is scaled based on the historical sum of squares in each dimension.   Adagrad provides another way to estimate a form of momentum over time, but slows down considerabley as step sizes increase due to the constantly-growing nature of the gamma value.  This can be desireable in some cases, but can have shortcomings for complex loss surfaces --- which are of course very common in the high dimesnional space we are operating within.
#SLIDE
RMSProp seeks to solve this issue, adding a new term (rho here, or a decay rate) to the calculation of gamma. This allows you to control the penalty term for AdaGrad so that the Gamma value will actually decay over time.  Generally, RMSProp could be used for our high dimensional cases, as we can slow the growth of Gamma, and thus allow for more iterations of update.  However, it is rarely used in practice, because of...
#SLIDE
Adam.  Adam essentially combines the two earlier idea - SGD with Momentum and RMSProp.  It does so by integrating two terms explicitly - first, it calculates momentum, and then second it captures the square of the gradients.  It also renames our earlier friction term to Beta1, and decay rate to Beta 2 (noting they have slightly different intepretations), and addsin bias terms to help prevent overshooting.  Very pratically, ADAM it's a great starting point for most problems - relatively fast and effecient, but does a good job mitigating the three aforementioned challenges to traditional SGD. 
#SLIDE
That's all for this lecture.  To briefly recap, we started by talking about some of the fundamentals for network implementation, and then switched over to some practical considerations for network fitting.  These included debugging - remember to print your gradients!, building a dev dataset, tracking your learning loss, and some guidelines as to how to pick optimal hyperparameters.  Finally, we introduced some of the limitations of SGD, and discussed four alternative optimization techniques that are commonly employed.  That's it for now, see you next time!

===============


W_{i+1} = W_{i} - \alpha \Delta / (\sqrt{\gamma} + .00000001)
\gamma = \sum_{i}^{N}\Delta^{2}
Velocity_{i+1} = \rho Velocity_{i} + \Delta f(W_{t})

V_{i+1} = \rho V_{i} + \Delta f(W_{i})
