======SLIDE 1
Next, we're going to start talking about a general limitation of loss functions - as traditionally written, they will encourage models to blindly seek the best fit to your training data, or a problem generally referred to as "overfitting".  This is predominantly a challenge when there is a difference between your training data and data "in the real world"; it is a problem that is particularly important for machine learning due to the very large numbers of parameters (i.e., weights) our models seek to fit.  Take the scatterplot on the screen now as an example.  Imagine you are trying to distinguish between pictures of dogs and pictures of cats, and you have two pixels of each (recognizing this is a bit contrived and you would have more than 2 pixels, but the point holds in any number of dimensions!).  For example, the red dot in the upper-left would be a picture of a dog with a pixel value 2 that is very high, and a pixel value 1 that is very low.  Conversely, a blue dot at the lower-right would be a picture of a cat with a high value of pixel 1, and a low value of pixel 2.

======SLIDE 2
If we wanted to draw a straight line inbetween these two classes to seperate them, we would probably want something that looked like this - it would minimize the error (just one blue dot is on the "red" side), and we would likely get future dog vs. cat cases correct.

======SLIDE 3
However, because of the high number of parameters most machine learnign models have, they will be willing to go far beyond straight lines.  Frequently, this is appropriate, but sometimes it can result in situations like this - where the "decision bounds" end up fitting to essentially replicate your training data.  This is a problem because...

======SLIDE 4
Imagine your trying to classify this yellow dot.  Intuitively, it would make sense it's probably a dog - it's pixels look a lot more like most dogs.  However, because of the overfit nature of this example model, we would end up calling it a cat.  Essentially, if you allow your model to perfectly replicate your training data, then you are unlikely ot be able to predict out-of-sample to new cases very well!  And, this is after all what we want to do - we don't need to know what the labels for our training data are, as we alreayd know them.  We want to be able to accurately predict new things!

======SLIDE 5
So, how do we mitigate overfitting?  In traditional modeling, i.e. polynomial regression, this is frequently done by reducing the number of weight parameters to be fit - if you have fewer parameters, you limit the model in terms of it's capability to overfit.  This is unfortunately very challenging for most machine learning models, as the very advantage they give us - being able to detect patterns and trends across huge datasets - requires large sets of weight parameters.  So, as an alternative we frequently modify our loss function.  On the screen now you'll see our equation from last lecture, the Total Loss (which we will now refer to as Data Loss).  If you'll recall, this resolves to the average loss for each test case - i.e., the level of "wrongness" across your test data.  Left unchecked, this is the loss function that will likely result in overfitting.

=======SLIDE 6
To mitigate this, we can add a new term to our loss function, what we refer to as a regularization term.  This term essentially penalizes our model for being too complex - i.e., if you have a lot of non-zero weights, you might get a higher value for the regularization term.  This would increase your loss function, and so the model would be considered "worse".  There are a range of different strategies for regularization, but the fundamental idea is that we want to bias towards simpler models to avoid concerns of overfitting to our training data.  By adding data loss - a measurement of how bad your model is, with higher values being worse - to regularization loss - a measurement of how complicated your model is, with higher values being worse - you can help to guide the selection of weights that avoid being both too complicated and too inacurate when contrasted to your training data.

=======SLIDE 7
Briefly, I want to highlight lambda in this function.  The function R(W) is the regularization loss itself - i.e., given W, you produce a value of complicated-ness that you add to the data lass.  Lambda determines how important this regularization loss is relative to the data loss - i.e., if you had lambda set to 0, this equation would reduce down to only data loss.  The value of lambda you choose is highly dependent on your regularization approach, and generally treated as a hyperparameter to select.

========SLIDE 8
So, what are a few options for regularization functions?  By far the most common encountered is L2 regularization, also known as weight decay.  In this case, it's just the euclidean norm of the weights matrix (you will sometimes see this as 1/2 of the squared norm as well).  To briefly walk through the equation, for a set of K weights parameters, we square each parameter and take the sum.  So, larger values indicate that your weights were larger in magnitude.  The basic idea is that you are penalizing the loss function based on the squared size of the weights vector. In practice, this results in models that tend to have relatively small values for weights parameters - i.e., our selection algorithms will preference weights values that approach 0.

========SLIDE 9
Contrasting to L2 regularization is L1 regularization.  Instead of squaring each weight parameter, here we take the absolute value.  The implication of this is that model weights tend to get forced all the way to 0, so you end up with many weight parameters with 0s and only a few greater than 0 (i.e., a sparse weights matrix).  This can be highly desireable for some applications in which you need a relatively small set of weights (i.e., running smaller models on cellphones). 

========SLIDE 10
There are many other regularization approaches you may encounter, and even a few that are hyper-specific to regularizing in the context of neural networks.  For example, elastic nets are a combination of L1 and L2 normalization; dropout networks work by simplifying a neural networks parameters.  The key takeaway is that regularization can be helpful in mitigating overfitting, and even applying simple L1 or L2 regularization can have positive impacts on your model performance when testing against unseen data.