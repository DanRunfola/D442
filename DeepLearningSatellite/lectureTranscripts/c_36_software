#SLIDE
Alright, enough of the hardware - let's start talking about the frameworks that most people (including us) use to interact with deep learning models.  This is a rapidly evolving landscape, and in this course we're primarily going to focus on Torch and Tensorflow.  There are a *ton* of frameworks out there - while they all orginated in academia, major industry players (Baido, Microsoft, Amazon) have all now started developing their own frameworks; however, most of these are much less broadly adopted.  
#SLIDE
Most of these frameworks really serve two key purposes, which can save us a lot of time vs. writing all of our own implementations (like on Assignment 2).  One of the most important things they provide is in simplifying building large computational graphs, which may have hundreds, thousands, or millions of computations.  This is in two ways - first, by providing a formalized way to string computational elements together in a graph (i.e., code to define a graph), and then second by - once the graph is established - automatically solving for the gradient in each step of a backpropogation.  The second big thing they provide is effecient integration of this code with GPUs (or other dedicated cards) - i.e., you can largely ignore the intracacies of integrating CUDA code with your numpy arrays, and just harvest the benefits!
#SLIDE
So - ultimately, what should you choose to do your implementations?  Both Torch/PyTorch and TensorFlow/Keras have most of the core implementations and capabilities you might want; most of the information on the internet about how Keras is "higher level" and PyTorch "lower" is a bit misleading, as you can get under the hood of either using Python.  However, there are some key distinctions to be aware of.  First, TensorFlow has a well established website - tensorHub - which has hundreds of pre-trained models you can pull off-the-shelf for transfer learning.  While pyTorch has something nascent up and coming (TorchHub), it - as of this writing - only has 38 models. 
#SLIDE
A second differentiating factor is in how models are constructed.  In PyTorch, you can change your computational graphs during the fitting process itself, potentially having your network architecture change; i.e., dynamic computational graphs.  TF/Keras assume a hard-coded architecture that is defined before your model fit begins (i.e., static).  While it seems like a dymanic graph would give a huge advantage to Torch, this can cut both ways - the big advantage of static graphs is that you can optimize your computational graph to be very effecient at run time.  
#SLIDE
Next up is general usability.  Writ large, Torch/PyTorch are a bit harder to read - i.e., you have to define nearly everything explicitly, up to and includign layer structures.  This can be wonderful when you want to dig into how everything works, but makes debugging a bit more challenging.  In contrats, TensorFlow - and especially Keras - is designed to be much more human-readable, with condensed code that allows you to define entire layers - or even sets of layers - in one line.  Recognizing this is a bit of a streotype for both cases, and both can get the job done, both my and general consensus is that Keras does a better job of making code that's readable, while PyTorch requires you to define more elements of your network upfront.  Both have similar levels of power at the end of the day, but just choose to reveal it to the developer in different ways.
#SLIDE
Finally - on speed, across the board, it's mostly a wash.  Torch, PyTorch, and TensorFlow will all give relatively similar performance, with some exceptions depending on the specific task.  However, of note, Keras is simply a little slower in every respect, which is the price paid for a simplified, higher-level coding environment.  Personally, I use both!  If I'm building a prototype of a simpler network, or doing a deep learning project with a new dataset for the first time, I generally dig in with TensorFlow or Keras.  If I need to build new network architectures of layers, or engage with some sort of novel approach to classification, I hop over to PyTorch.  