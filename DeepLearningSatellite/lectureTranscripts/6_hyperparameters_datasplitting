=== SLIDE 1
So you may be asking yourself - outside of these rules-of-thumb - how do we select the best distance metric?  This is a broad class of problem, and not just about distance metrics - think about the K-Nearest Neighbors algorithm.  There are two different choices we have to make to implement the algorithm: what value of K do we use, and what distance metric to use.  Both of these are referred to as "hyperparameters" - choices we make that influence the algorithm, but are not trained based on the data. A big challenge is that the correct hyperparameters are very problem dependent - the underlying nature of your data, the target you're trying to predict, sparsity, and hundreds of other things come into play in driving the best.  So, generally, we need to treat hyperparameters as an optimization - i.e., we want an effective way to test lots of different combinations, and then select the optimal set.

=== SLIDE 2
We're going to dig into this notion of hyperparameters a little further.  When we talk about "what works best", there are a huge number of ways to define "best", and the community has explored a wide range of them.  The first idea you might consider is simply choosing hyperparameters on the basis of whatever provides the most accurate estimates for your dataset.  While intuitive, this is a terrible idea, but let's walk through why.  In this approach, we would take all of our data ("The Data") and feed it forward into 10 different models - 5 different permutations of K, and two different distance definitions.  We would then look at the accuracy of every model, pick the best one, and choose the one with the lowest overall error.

So, why is this a terrible idea?  At the end of the day, we don't actually want a model that can predict only the data we have on hand - the entire point of the vast majority of machine learning is to predict for cases we *don't* have.  So, this will give us the best hyperparameters for our data, but no insight into how effective our model is for other cases.  This is generally referred to as overfitting.

=== SLIDE 3
A natural extension of this would be to take our data and split it into two pieces - a set of data we're going to use for training, and then a completely independent set we're using to validate.  As before, we will fit our 10 models, but this time with only the training data.  We'll then use each of the ten models to predict the *validation* data, and choose the one that works best to establish the best hyperparameters.

This is a much better idea, as it ensures that your model is being tested against an external dataset.  However, in practice, your ultimate aim is to choose optimal hyperparameters based on the results of the validation data tests.  Because you're choosing optimal hyperparameters based on these validation datasets, you still have bias in your accuracy statistics: i.e., you don't know how well your model will perform on a completely independent set of data, as all of your data is still being used in the optimization of your model. 

=== SLIDE 4
A third approach - and a fairly good one - is to split your input data into three discrete buckets.  The first two are the training and validation data - these are the datasets that will inform our modeling.  The third, and new one, is a test dataset.  This test dataset will be left completely aside until the very, very end of our analysis - after we've done all of our training, including selecting the best model based on our validation data.  Only then will we test the final set of hyperparameters chosen by contrasting it to the test data.  That result will provide the numbers we ultimately report, with the goal of estimating the likely accuracy on some unseen, external dataset. If you see other approaches to accuracy assessment which do not utilize a completely independent test set, be VERY dubious!

=== SLIDE
While an independent validation and test dataset is the most common approach in deep learning, cross-validation is another important tool.  This is especially important if you are fitting a model with a relatively small amount of data - i.e., a few thousand observations, instead of millions.  Essentially, it is possible that your model will be biased due to the random subset of data you chose for testing and validation - i.e., imagine trying to predict the contents of an image, and by purely random chance you choose 100 images of the eifel tower out of a database of 1000 for your testing set; your model might be REALLY good on paper, but in practice can't predict anything but french architecture.  

Cross validation (sometimes referred to as "K-Fold" Validation, where k is the number of folds) is an approach to mitigating this challenge.  In cross validation, we subset our data into a testing dataset, and then some number of folds - in this slide example, 4 folds.  Then, we set aside a single fold to use as validation during the modeling process, and use the remaining three folds for training.  This process is repeated for each permutation of the folds - i.e., in this example, we use fold A for validation, then B, then C, then D.  Finally, we choose the model with the lowest overall error for each fold (i.e., through a voting mechanism, though there are many approaches to choosing the best hyperparameters across all folds).  Strategies like this help us implement reasonable tests of the external validity of any model we might produce.