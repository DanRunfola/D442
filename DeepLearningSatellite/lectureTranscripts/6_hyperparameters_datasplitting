
=== SLIDE
We're going to dig into this notion of hyperparameters a little further.  When we talk about "what works best", there are a huge number of ways to define "best", and the research community has explored a wide range of them.  The first idea you might consider is simply choosing hyperparameters on the basis of whatever provides the most accurate estimates for your dataset.  While intuitive, this is a terrible idea, but let's walk through why.  In this approach, we would take all of our data ("The Data") and feed it forward into 10 different models - 5 different permutations of K, and two different distance definitions.  We would then look at the accuracy of every model, pick the best one, and choose the one with the lowest overall error.

So, why is this a terrible idea?  At the end of the day, we don't actually want a model that can predict only the data we have on hand - the entire point of the vast majority of machine learning is to predict for cases we *don't* have.  So, this will give us the best hyperparameters for our data, but no insight into how effective our model is for other cases.  Further, we are highly likely to dramatically overfit to our data, resulting in a model that can't be generalized to any other cases.

=== SLIDE
So, a natural extension of this would be to take our data and split it into two pieces - a set of data we're going to use for training, and then a completely independent set we're using to validate.  As before, we will fit our 10 models, but this time with only the training data.  We'll then use each of the ten models to predict the *validation* data, and choose the one that works best to establish the best hyperparameters.

Yet again - this is a bad idea.  The point of this validation set is to give us some notion of how well the algorithm will work for new data.  Because our validation set is driving which model is chosen - i.e., the error of the validation data helps us choose the "best" model - we are left without a truly external test, where the data being used to choose a model in no way influences the estimated errors.  Bottom line: don't choose a model based on data that in any way influences model accuracy!

=== SLIDE
So, what is a budding computer vision student to do?  A third approach - and a fairly good one - is to split your input data into three discrete buckets.  The first two are the training and validation data - these are the datasets that will inform our modeling.  The third, and new one, is a test dataset.  This test dataset will be left completely aside until the very, very end of our analysis - after we've done all of our training, validation, selected the best model based on our validation.  Only then will we test the final set of hyperparameters chosen by contrasting it to the test data.  That result will provide the numbers we ultimately report in academic papers, reports on the accuracy of your model, and the speech you use to illustrate how smart you are to your colleagues.  If you see other approaches to accuracy assessment which do not utilize a completely independent test set, be VERY dubious!

=== SLIDE
While an independent validation and test dataset is the most common approach in deep learning, cross-validation is another important tool.  This is especially important if you are fitting a model with a relatively small amount of data - i.e., a few thousand observations, instead of millions.  Essentially, it is possible that your model will be biased due to the random subset of data you chose for testing and validation - i.e., imagine trying to predict the contents of an image, and by purely random chance you choose 100 images of the eifel tower out of a database of 1000 for your testing set; your model might be REALLY good on paper, but in practice can't predict anything but french architecture.  

Cross validation (sometimes referred to as "K-Fold" Validation, where k is the number of folds) is an approach to mitigating this challenge.  In cross validation, we subset our data into a testing dataset, and then some number of folds - in this slide example, 4 folds.  Then, we set aside a single fold to use as validation during the modeling process, and use the remaining three folds for training.  This process is repeated for each permutation of the folds - i.e., in this example, we use fold A for validation, then B, then C, then D.  Finally, we choose teh model with the lowest overall error for each fold (i.e., through a voting mechanism, though there are many approaches to choosing the best hyperparameters across all folds).