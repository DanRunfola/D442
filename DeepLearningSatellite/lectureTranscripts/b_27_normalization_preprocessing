Ok!  You've picked an activation function, now it's time to think about how you want to manipulate your data inputs (i.e., data preprocessing).  This is the set of steps you will take to manipulate your inputs - in our case, images, but this is generally applicable to any problem.  Here, you want to consider the type of activation function you're using, as different activation functions will perform better (or, worse) depending on the strategy you choose for standardization.  Some fairly straightforward manipulations are things like zero-centering your data (i.e., subtracting the mean) or taking the standard deviation for your observations.  Zero centering in particular is very important, as...
#SLIDE
You'll recall that activation functions that are not zero centered will return only positive or negative - not both - gradients if all inputs are positive are negative.  So, if you zero-center your data, it makes it much less likely that all inputs will be in one direction, mitigating this limitation of many functions.  In practice, you'll generally want to apply zero centering in most cases, as it won't hurt and it can help with activation functions that are not zero centered; more complex approaches (i.e., whitening, PCA, standard deviations) are only necessary if you are working with data that has issues with outliers or very large changes in magnitude, which is relatively uncommon in the image processing space.
#SLIDE
The next choice we have to consider is how we want to initialize our initial weights - i.e., what is the first set of weights we want to guess?  Consider the network visualized on the right - here, we have some input image - say, the letter A - and we're going to pass that image data through the network to predict 26 scores.  If all of the weights were the same acros the entire network, what would the output scores be?  Pause here if you want to think about it. <PAUSE> If you guessed "the same" - you're right!  All of the scores would be identical if all of the weights in the network are identical.  And if all scores are equal, then the backpropogation will be the same across the entire network, and so all updates will be identical, and you'll never have a network that improves!
#SLIDE
So, what do we do?  In your assignment to date, you've done somethign like this - initializing all weights as a very small random number.  This does prevent the problem of initializing all weights with the same value, and will work well for relatively small networks.  However, as we go deeper this approach will begin to fail - very intuitively, imagine your first iteration with randomly generated, small weights.  Every layer of the network you're going to multiple a small number by a small number - with even 10 layers, you'll end up with a number that is around .00000000000000000.... something very, very small.  Thus, gradietns you send back throught he network will be similarly small, essentially resulting in almost no change.
#SLIDE
Recognizing that small numbers don't work because you essentailly zero out your gradients, the next obvious idea would be to use.. big numbers!  This has a new problem though - think about what we talked about when it comes to RELU, Sigmoid, and Tanh functions...
#SLIDE
Using tanh as an example, as numbers get big, tanh will saturate - i.e., all outputs will be equal to 1 or -1.  So, you end up with the same problem - all the scores are the same, the gradietn is 0, and you're out of luck during backpropogation!
#SLIDE
Bottom line is - weight initialization is a problem that is frequently overlooked, and it's really tricky to get right.  If you get it too big, you saturate your activations; too small and you zero your gradients.  Thankfully, we aren't the only ones that have run into this, and the best/brightest have come up with a number of approaches to get it right.  Here, we're just going to focus on one - Xavier initialization.
#SLIDE
Xavier initialization came around in 2010, and was designed for linear activation functions.  I definitely recommend digging into Xavier Glorot's 2010 piece on this if you want to dig into exactly how and why this approach works, but in practice the fundamental idea is that depending on the total complexity of your network - i.e., the number of inputs and ouputs - you need to adjust weights so as to ensure that the variance of your initialized weights is equal to 1 over the number of incoming connections.  So, in practice, solving for W looks like...
#SLIDE
This. However, this approach is still limited when non-linear activation functions are used to define the network.  In 2015, He et al. recognized that this was largely because in the ReLU paradigm, mmany of the nodes are getting saturated (half, to be precise), and so to fix that implemented...
#SLIDE
This small change - dividing by 2 in the denominator, to account for node saturation in RElU. Again - these choices have huge implications, and can be the difference between a good and bad fit for your network (or, being able to solve it at all!).
#SLIDE
Weight initialization is key, but there are other approaches to ensuring resilience of the model to saturation and gradient decay.  One of the most popular of these is batch normalization.  The idea of batch normalization is fairly simple: when you're optimizing a network, you're generally selecting some number of images - i.e., your batch size - to feed through the network. 
#SLIDE
Let's say our hypothetical network looks like this - all we are doing is taking the first two pixels in both images, and passing them forward into a single computational node with weights.  The activation function we choose is tanh, which is susceptible to saturation.  Normally, we would do our forward and backward passes one image at a time - i.e., the forward pass for the truck can be done independently of the forward pass of the boat.
#SLIDE
In batch normalization, we instead run both the truck and the boat at the same time, and record the input value for each node. Importantly, no activation function is performed before batch normalization - i.e., the weighted sums of the inputs are passed directly to the batch normalization (...most of the time).
#SLIDE
What the batch normalization does is take in all of these values and simply normalize them in a zero-centered way - i.e., using standard deviations.  This guarantees that there will be some negative and some positive values, and that a certain distribution (most commonly gaussian) is followed.  By forcing the input weighted values to follow this distribution, batch normalization can then pass the values on to other activations...
#SLIDE
just like before, except now the risk of saturation far lower, and gradient decay is less likely to matter as well (as the standardization will re-inflate your values).  I'm not going to go into the details of exactly how the distribution of values is normalized, but on the website you'll find a link to Ioffe and Szegedy, 2015; I highly recommend reading through it to get a depth of understanding on why these batch approaches work.  Also briefly note that there are many different approaches to normalization, including some that allow for you to re-scale values and recover the identity mapping of your inputs (and everything inbetween).  This range of approaches allows you to enable *some* saturation in your network, which can be valuable in some cases.