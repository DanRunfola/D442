#SLIDE
Let's take a step back and remember what we're trying to do with these models.  To this point, we've really focused on our Loss for our training data - i.e., during the modeling process, we're trying to minimize the blue line on this figure (some measure of loss), which will increase the accuracy we're looking for (the red line here).  However, that's only half the battle - what we REALLY want is for the yellow line to be as high as possible - our validation accuracy. 
#SLIDE
So, another focus for us is how to minimize the gap between the yellow line - our independent validation accuracy - and our red line.  This difference ultimately comes down to patterns that our model captures in our training data that are not reflected in our validation data, or vice-versa.  So, how do we build models that help us close this gap?
#SLIDE
First, we're going to talk about model ensembles - by putting multiple models together, we can hypothetically capture more variance in our training data, and do a better job predicting to our validation data.  Ensembles are a common approach to imagery analysis, where you essentially build multiple models and train them all simultaneously, and then combine their results into a single output.  You can imagine any number of different ensemble approaches - even if you're simply ensembling the same model numerous times, you still might see gains of around 1-2% simply because one model may by random chance have a better set of parameters than another.  In this example ensemble, you see each image being replicated 4 times, input into a convolutional model (ResNet - more on this later), and then integrated together to produce the final estimate.
#SLIDE
Another type of model ensemble is a snapshot ensemble. Consider your normal model fitting routine, in which you do a forward pass, backward pass, and then update the weights.  Normally, you would repeat iteratively until you find the single best set of weights, and use those in your "final" model.  
#SLIDE
In a snapshot ensemble, we instead first fit our model until it reaches convergence (i.e., the values of our weights aren't changing very much).  We then save these weights, and that becomes the first member of our ensemble.  We then dramatically increase the learning rate, and repeat the process once for each ensemble member we want.
#SLIDE
These two images, from Huang, Li and Pleiss in 2017, illustrate the fundamental idea of a snapshot ensemble.  On the top is an example of how learning rate would change when building the snapshot (in red).  As you can see, as learning rate approaches a very small value each iteration, we save the state of the weights, and then dramatically increase the learning rate again (in Red).  In this example, we repeat that process 6 times, creating a 6 model ensemble.  On the bottom, you can see the intuition behind this approach.  The first image (the one on the left) shows a normal SGD optimization, in which the leraning rate is gradually decreased.  It searches across the loss function until it finds the minimum value, and saves the weights at that point.  On the right is an example of a snapshot ensemble.  Here, we're building multiple snapshots for an ensemble, and so the algorithm - at least hypothetically - is going to find multiple minima; each time the learning rate is increased, we hope to find a new local minimia; thus, each model would be representative of a different local minima in our loss surface.  This can be very helpful in capturing heterogeneity across your input data.