Alright!  Now we're going to change gears a bit and discuss deep reinforcement learning.  The fundamental idea of reinforcement learning is to update our model as it learns more about the behavior of the system it's trying to engage with as the system changes into different states.  Taking checkers as an example, you can imagine an algorithm we design that has three characteristics.  First we have a state, which is - for each turn of the game - the positions of pieces.  Second, we have the action the model can choose between - i.e., where to move a piece on the board.  And, finally, we have some reward - a binary 0 or 1 if you win or lose the game.  The idea is to calibrate a model that can adapt to different states, and then make a decision to reach a given outcome.  This is the same fundamental strategy that is used for all sorts of algorithms, the most prominent of which being robotics.
#SLIDE
We can formalize a reinforcement learning problem as an iterative relationship between an agent - i.e., some algorithm that makes a decision about what actions to take - and an environment - i.e., something else (can even be a black box) that responds to those actions.  This problem formalization allows us to determine the state of the environment (s) at any given time step, as well as any reward (positive or negative) the environment might given in response to a given action (a).  You can imagine any number of different ways to construct this agent-environment relationship, ranging from giving rewards every time something specific changes (i.e., a checker is taken; a pawn is removed) to updating the environmental state due to elements outside of the action (i.e., for a robot, a new rock has moved into the field of vision after a step was taken).
#SLIDE
The most common way to formalize this is a Markov decision process.  This is predicated on the Markov Property.  A process is said to have the markov property if the conditional probability distribution of the future state of a process depends only on the current state.  A simple example of this is checkers - once a piece is moved, all future possibilities are now constrained by that fact, i.e. the way the game is played after a piece is moved is always dependent on how that piece moved.  You don't need to know anything about the state of the board before the piece moved to predict what possibilities still exist after the piece moved.  This is true of most games we play - chess, and even things like Texas Hold'Em.  This contrasts to processes in which some information about past states is needed to predict the next state - a common example is trying to solve for how fast a runner is going to move over the next 10 seconds.  This requires information not just about the state of the runner (i.e., where they are, how fast they are moving), but also information about - for example - for how long the runner has already been running, and thus how tired they are likely to be over the coming 10 seconds.  
#SLIDE
A markov decision process can be defined by a tuple of five objects.  S is the set of possible states, A possible actions, R - our distribution of rewards for different pair, P - transition probabiliy distributions to the next states given a chosen state/action, and then gamma, which is how much we value current rewards (i.e., at this step) vs. later.  By combining these objects, we are able to fully define how our system behaves and reacts to different sequences of actions. 
#SLIDE
How we implement a markov decision process is through a five-step loop, in which we repeat each of the later four steps until some convergence or objective function is satisfied (i.e., a game is won).  First, and only once at the first step, the envrionment is defined as some initial state s0.  While s0 can be assumed to be a sample drawn from some probability of possible initial states, the easiest way to think about this is a chessboard - the pieces all arranged and ready for war.
#SLIDE
Once the environment is defined the agent must choose an action to perform, defined as a_{t}.  This can be pushing the "jump" button in a mario game, or moving a pawn forward. 
#SLIDE
Once an action is chosen, the environment then identifies the reward that a given action would return, given the current state of the environment.  Again, this is defined as sampling from the distribution of all possible rewards, given a state and action.  This could be receiving a reward for taking a piece from the opponent.  In the case of super mario, this is generally defined as moving forward along the level.  Essentially, the reward provides an algorithm with information about how to optimize it's play at any given state.
#SLIDE
Now, the environment samples the next state, St+1, given the action.  This again is a probability we're sampling from, i.e. - given the white pawn moved forward, there are 10 valid responses the black player could make (moving all 8 pawns and the 2 horses).  So, we sample from that according to some probability function, and the environment would return the black players move.
#SLIDE
Finally, the environment sends the information about rt and st+1 back to the agent, and the process is repeated.
#SLIDE
Looking at this loop for a moment, we have three other important terms.  First is our policy, pi, which determines what action to take given a state and a selection of possible actions.  Second is our constraints, omega, which identify actions that cannot be taken given a specific state.  We are ultimately trying to optimize the final objective function on the screen, which uses our discount rate gamma, multiplied by the reward at each timestep t (excluding the first time step 0, before any actions are taken).  The goal is to find the policy pi, given constraints omega, that results in the highest total reward.
#SLIDE
The most common way to find an optimal pi is trhough something called Q-Learning, which is effectively the use of a function to estimate the action-value function: i.e., given a state and action, what is our anticipated reward given a discount rate?  Because a deep neural network is a natural approach to build this sort of model, this frequently takes the form of Deep Q-Learning. We're going to walk through this with an example of everyones favorite italian plumber, Mario! 
#SLIDE
Let's break our problem down into state, action and reward.  In this case, our State is going to be the raw pixels of the game - i.e., a given frame frozen in time, so we'll process every frame.  Our actions available at any time are to jump, push the left button, or push the right button (again, we make this decision once per frame).  Finally, our reward is to increase if we move farther right (towards the end flag), and decrease if we move left.
#SLIDE
This translates into the need for a network architecture that looks something like this - first, some convolutional filter and activation function to extract the information from a frame, and then an affine or fully connected layer that assigns three scores - one for each of our actions. The goal of our network is to identify the weights that are most helpful for making the decision between the four possible actions, based only on the pixel information in the state.
#SLIDE
Once we've made our action, we then record the next frame of our game - i.e., the state at t+1.  This is used to compute the reward, rt, for that action.  In the mario case, this would simply be the number of pixels advanced to the right (a positive reward), or negative rewards if we're moving left.
#SLIDE
Now, we calculate Loss.  Loss in the case of a Q-Learning architecture is different, because we don't have any kind of "Truth" to compare to - that is, we don't know in advance what the correct action (or sequence of actions) was.  So, instead, we seek to calculate the optimal score we *could* have at a given time step, and then contrast that to our current score; we seek to minimize the difference between those.  Because we're seeking to minimize this loss, we can then backpropogate through our network just like we would any other loss function!
#SLIDE
Finally, we update our state, and repeat the process until the finish line (...or we fall into a pit and die).  While there is a tremendous amount more to deep reinforcement learning, they all follow the basic principals you've learned throughout this course, with the added challenge of determining a correct action.  RNNs, for example, are very commonly used in Q-Learning models, as are attention-based models. 