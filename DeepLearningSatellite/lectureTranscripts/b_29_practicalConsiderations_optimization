#SLIDE
We're going to switch gears a little now, and start talking about some practical considerations for fitting your networks - i.e., how to debug issues when you come across them.  And, you will come across them!
#SLIDE
One of the first challenges with network design is that you can select nearly any network architecture to engage with a challenge.  From complex architecture like ResNet, to simple Dense nets with one hidden layer, each architecture is prone to different types of challenges during the learning process.  Take this network as an arbitrary example - here, we have two different weights layers.  An easy way to make this net perform MUCH worse in optimization would be to intialize each set of weights with a dramatically different magnitude - i.e., W_alpha initialized as values from 1000 to 10,000, and W_beta initalized as values from 0 to 1.  While this is obvious in this simple example, as architectures get more advanced - or you begin to ensemble different models together - you want to ensure that you're designing all of your initialization strategies in a way that won't result in either saturated regions of your net, or changes so minimal they would never effect the parameters you want changed.
#SLIDE
Practically, when your net isn't fitting there are a few discrete steps you'll want to take in your code.  First of these, print out your gradients - if all of your gradients are equal to 0, that is a key indicator that something is wrong.  The specific reason something is wrong can be highly variable - i.e., in some cases, it may be you have a bug; however, just as common is something wrong with your weights initialization, learning rate, or a saturation problem.  You can print your gradients out at any step of your backpropogation to try and isolate where the error is occuring; this is frequently my go-to for debugging complicated nets.
#SLIDE
Another important thing to look at is your loss function - you want to confirm that it's (a) not 0, if it shouldn't be; and (b) a reasonable value based on the loss function you chose.  This can be a giveaway of errors related to (for example) your input data - i.e., if images are corrupted and have all 0 values, your loss function may not change at all no matter what you do (as you're always multiplyign by 0).  
#SLIDE
More broadly, on your first pass your loss function should be similar to what you might expect with random noise - not identical, but similar.  You can quickly solve, for example, that the expected value of the softmax loss function under completely random noise would be 1; thus, on your first forward pass, you would expect a value ~around~ 1.  If you're seeing values in the thousands or tens-of-thousands, that's a good indicator something is wrong.
#SLIDE
When you add regularization into your models, it’s also helpful to first run the model with λ = 0 first, record the loss value, then set λ = 1.  As λ controls the impact of regularization, when you change λ to 1, you would expect the total loss to go up.  If it doesn’t something is wrong with your regularization code or algorithm.
#SLIDE
Once you're confident that the mechanics of your model are performing as expected, you'll want to create a development dataset.  This should be very small - i.e., the first hundred or so values in a dataset - and is what you'll use to test your models are performing as you expect.  Having a very small dataset also has the advantage of allowing you to quickly overfit, getting near perfect solutions for your set of data, which lets you further confirm your model is behaving as expected (i.e., you should be able to see loss go down to 0 with a small N).  I personally will slowly increase the size of my dataset as I become more confident in my model - starting with around 100 examples, then 1000, then 10000, and so on until I am using the full dataset.  Each step up comes with very large computational - and time - costs, so you want to be increasingly confident you won't run into silly errors (like missed commas)!
#SLIDE
After those checks all pass (i.e., your model gets a loss of near-0 after the small-N run, your gradients are updating as expected, the loss function makes sense), then we can really start working on the network itself.  The first practical thing you want to choose is a learning rate - i.e., the value you will multiply the gradients by to update your weights.  Large learning weights result in more rapid changing of your parameters, but setting the value too large can result in you 'skipping' optimal solutions; smaller weights give you a more granular estimate of your loss surface, but takes longer to fit (and, in some cases, may never fit).  Take this slide for example - here, wusing the same code as you use in assignment 2, I am printing out the weights values after each backpropogation of a simple two layer network.  You can see here with LR = .00001, the weights are updating each iteration.  Note that not all models are equally sensitive to learning rate; it has a lot to do with the specifics of your data and loss function.  Bottom line is: if you see your weights aren't updating, try turning your learning rate up to see if you can improve the situation.
#SLIDE
While accuracy is a good way to establish what learning rate is most effective, sometimes you may also want to diagnose the *rate* of learning - i.e., how quickly the loss is improving.  What you see here is an output from your assignment 2, in which the red line plots loss over epochs.  This curve looks very reasonable - i.e., it's gradually approaching some lower limit; in general, this is what you hope to see if your learnign rate is well tuned.
#SLIDE
The fushia line on the chart now is what you might expect to see if your learning rate is too high - i.e., you find a solution fairly rapidly, but it's not globally optimal (i.e., your stuck in some minima that is not truly the best).
#SLIDE
Finally, the green line is indicative of what you might see if you have a learning rate that is disproportionately high relative to your weights - you frequently just see your gradients explode, and loss grow until your net stops solving because of overflow issues. 
#SLIDE
Also of note here is that - as you search through parameters like learning rate - it is frequently sensible to conduct randomized searches rather than structured grid searches.  In the earlier slide, when I set LR equal to 1, .1, .01, etc., that was representative of a grid search in which I am predefining the parameter space to search across.  Randomized search is another, more effective (in theory) way to do this; going a step farther, there are other techniques like Bayesian Search, which take prior information on the outcome of different parameter values to try and identify the best combinations.  
#SLIDE
Now, we're going to start talking about alternative approaches to optimization that are commonly used to fit networks. Recall back to the very generic problem we're trying to solve with optimization - given some set of parameters, what is the set that reduces the loss function to the lowest possible value.  In the case of stochastic gradient descent, we do this by solving for the gradient of the weights at any position, and then update our weights according to those gradients so that we are always moving to a state of the system with a lower loss.  We keep updating in the negative direciton of the gradient until we converge on some region of low loss.
#SLIDE
SGD itself is, unfortunately, limited in a number of ways; you may run into some of these cases in practice.  Imagine for starters this loss surface, where we're trying to find the best value for two weights (w1 and w2). Our first random guess is denoted by the orange circle, which has a loss of 5 (denoted by the red isoline).  
#SLIDE
If we moved the orange dot downhill along the W1 dimension - i.e., we didn't change W2 at all - we could very quickly see a change in our loss from L=5 to L=1.
#SLIDE
Covnersely, if we move a similar distance along W2, we see little to no change in our loss.  This is referred to as the loss having a bad condition.  Thinking about this logically, what it means is that when we go to update our weights, we will tend to update by larger amounts in the "up down" (W1") dimension, and very little in the W2 dimension.  This results in..
#SLIDE
A highly nonlinear set of steps to find the best solution, in which large shifts occur in one direction, but not the other.  While you'll eventually find the optimum, it will be very slow.  
#SLIDE
Another challenge with SGD is local minima.  We talked a little about this before - essentially, this is when the loss function has some inflection point for some values of W.  In this example, it would be possible for a gradient descent approach to be "stuck" at an incorrect solution, indicated by the red circle.  As we have implemented it, gradient descent would get "stuck" here, becuase our gradient would be 0 at that local point, so it would not be able to go out of the incorrect solution and over the "hill" towards the better solution farther along the W1 line.  
#SLIDE
Similarly, SGD can be stuck at saddle points - i.e., at the *very* top of a loss function, gradient is technically equal to 0 in both directions.  While this seems unlikely, note that saddle points get more common as you get into higher dimensional cases; conversely, local minima get less common in higher dimensions.
#SLIDE
So, how do we fix these issues with SGD?  Let's start by refreshing ourselvse as to the actual equation defining SGD, which is shown here.  The weight we test at the next iteration is calculated based on the current iteration's weight minus the gradient of that weight with respect to the loss function.  The alpha term defines the learning rate - how large of a change we implement relative to the gradient.  This form has all of the aforementioned issues.
#SLIDE
To mitigate the challenges of tradition SGD, a minor addition called SGD+Momentum was created.  Here, we add an additional equation that, at each step, calculates a velocity based on the running mean of gradients, and contrasts that to a new term - rho- which defines friction.  To walk through this equation, first we calculate the gradient of W with respect to the loss, as before.  We then add to this our velocity term, multiplied by Rho.  This gives us the velocity for the next iteration.  We then use that velocity to solve for the weights update itself - essentially, the weights are updated not only based on the instantaneous gradient measured at a single point in space, but rather are updated based on both the current set of weights and the preceeding set. Rho is what defines the amount of the last iteration considered in this estimation; it is commonly set to a fairly high level (0.9 or higher).
#SLIDE
This addition of momentum greatly helps to mitigate all of the issues we just discussed. For both points of local minimia and saddlepoints, the interpretation is fairly straightforward - the momentum can help us essentially get over points of zero-gradient where we might otherwise be trapped.  In the case of poor conditioning, we essentially rely on velocity to allow us to "cancel out" movements in opposite directions - i.e., instead of a zig-zag, we can pull our gradient estimates onto a line that will (theoretically) head towards the true minima, instead of bouncing around in a slower fashion.
#SLIDE
Another common solution you'll see to SGD shortcomings is AdaGrad.  At each iteration, AdaGrad seeks to record a value - represented by Gamma here - equal to the square of all gradients.  Each iteration, this gamma value is equal to the sum of squares for each preceeding iteration, so the gamme value always increases over time (i.e., more iterations means the gamma value is higher).  The update rule implemented then divides the gradients by this gamma value.  This is done element wise - i.e., each element of the gradient is scaled based on the historical sum of squares in each dimension.   Adagrad provides another way to estimate a form of momentum over time, but slows down considerabley as step sizes increase due to the constantly-growing nature of the gamma value.  This can be desireable in some cases, but can have shortcomings for complex loss surfaces --- which are of course very common in the high dimesnional space we are operating within.
#SLIDE
RMSProp seeks to solve this issue, adding a new term (rho here, or a decay rate) to the calculation of gamma. This allows you to control the penalty term for AdaGrad so that the Gamma value will actually decay over time.  Generally, RMSProp could be used for our high dimensional cases, as we can slow the growth of Gamma, and thus allow for more iterations of update.  However, it is rarely used in practice, because of...
#SLIDE
Adam.  Adam essentially combines the two earlier idea - SGD with Momentum and RMSProp.  It does so by integrating two terms explicitly - first, it calculates momentum, and then second it captures the square of the gradients.  It also renames our earlier friction term to Beta1, and decay rate to Beta 2 (noting they have slightly different intepretations), and addsin bias terms to help prevent overshooting.  Very pratically, ADAM it's a great starting point for most problems - relatively fast and effecient, but does a good job mitigating the three aforementioned challenges to traditional SGD. 