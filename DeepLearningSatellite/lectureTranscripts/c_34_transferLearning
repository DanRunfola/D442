SLIDE
Transfer Learning builds on the idea of image augmentation, using networks trained on one substantive domain (i.e., 'Farm Animals'), and using them as a starting point in another domain (i.e., "Household Pets"). This not only helps with regularization (as your initial weights will be more generalized by definition), but also can help mitigate the need for absolutely massive datasets to train a network for a specific purpose.  
#SLIDE
Imagine, for example, you're training a network on CIFAR 10.  The input of this net - P - is the 3072 pixels of each image; the output is s (our 10 scores, one for each class).  To fit this network would require 40,821,000 parameters; obviously, we want a lot of data to do this - i.e., we would want to use all of CIFAR10.
#SLIDE
Now imagine that you have another database of images that have the same input dimensions (3072 pixels), but represent two different classes not present in CIFAR10: Desks and Bathtubs.  You want to be able to automatically classify if an input image belongs to one of those two classes.  Unfortunately, you only have 1000 examples of each, which is far too few to fit the nearly 41 million parameters in your network.  So, what are you to do?
#SLIDE
In transfer learning, you first freeze all of the earlier layers of your network - in this example, you would save your network and retain all of the weights in the beta, alpha, and gamma layers.  You would then delete the last layer of weights, and re-initialize it with your new case - taking in the same 100 values from earlier layers, but then outputting only 2 cases.  This final layer would have unfrozen weights, in this case a total of 200 paramaters.  You would then fit this net, preventing any changes from occuring in the earlier layers, and seeking to fit the 200 parameters in weights delta to provide the two scores for your particular problem.  If you have a larger amount of data available, you can unfreeze additional layers and train further back into the network.  Transfer learning is very common across a huge range of domains - i.e., ImageNet is very commonly used as a baseline, and then deeper levels of a network trained for a specific case.
#SLIDE
Very practically, if you have less than 1 million images in your dataset, you may want to consider using a larger dataset in a similar domain to train with, and then apply a transfer learning approach.  Most major software packages (Keras, PyTorch, etc.) provide easy mechanisms to load pre-trained weights.